<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zqhxuyuan</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://github.com/zqhxuyuan/"/>
  <updated>2019-05-23T04:42:49.868Z</updated>
  <id>http://github.com/zqhxuyuan/</id>
  
  <author>
    <name>任何忧伤,都抵不过世界的美丽</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>蚂蚁金服中间件招聘广告</title>
    <link href="http://github.com/zqhxuyuan/2020/01/01/2020-1-1-job/"/>
    <id>http://github.com/zqhxuyuan/2020/01/01/2020-1-1-job/</id>
    <published>2019-12-31T16:00:00.000Z</published>
    <updated>2019-05-23T04:42:49.868Z</updated>
    
    <content type="html"><![CDATA[<p>招聘JD如下，简历可以发送到：qihuang.zqh at antfin.com</p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&amp;mid=100000003&amp;idx=1&amp;sn=80ee472c556a7d74723ddb9a5a0bf1d7&amp;chksm=7aa0eed94dd767cf8d191c4e99124420125c8b78da17c94b575e3a4e159f7777817fe31fcec3&amp;mpshare=1&amp;scene=1&amp;srcid=0124PvSmOgqZhK8nRX4WQzFS&amp;pass_ticket=SfKCFUb0WAT2UBkLGfjqoB4pzfyyXj4jwi7fd9gRJXvfI%2FwusG1avSFPTvgAKu7I#rd" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&amp;mid=100000003&amp;idx=1&amp;sn=80ee472c556a7d74723ddb9a5a0bf1d7&amp;chksm=7aa0eed94dd767cf8d191c4e99124420125c8b78da17c94b575e3a4e159f7777817fe31fcec3&amp;mpshare=1&amp;scene=1&amp;srcid=0124PvSmOgqZhK8nRX4WQzFS&amp;pass_ticket=SfKCFUb0WAT2UBkLGfjqoB4pzfyyXj4jwi7fd9gRJXvfI%2FwusG1avSFPTvgAKu7I#rd</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;招聘JD如下，简历可以发送到：qihuang.zqh at antfin.com&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzUzMzU5Mjc1Nw==&amp;amp;mid=100000003&amp;amp;idx=1&amp;am
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="work" scheme="http://github.com/zqhxuyuan/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>Kafka消费者增量拉取</title>
    <link href="http://github.com/zqhxuyuan/2019/03/07/2019-03-07-KafkaConsumer-IncrementalFetch/"/>
    <id>http://github.com/zqhxuyuan/2019/03/07/2019-03-07-KafkaConsumer-IncrementalFetch/</id>
    <published>2019-03-06T16:00:00.000Z</published>
    <updated>2019-03-07T14:38:38.558Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability</a><br><a href="https://www.cnblogs.com/huxi2b/p/9335064.html" target="_blank" rel="noopener">https://www.cnblogs.com/huxi2b/p/9335064.html</a><br><a id="more"></a></p>
<h3 id="简介">简介</h3><p>为了减少客户端每次拉取都要拉取全部的分区，增加了增量拉取分区的概念。</p>
<p>拉取会话（Fetch Session），类似于web中的session是有状态的，客户端的fetch也可以认为是有状态的。<br>这里的状态指的是知道“要拉取哪些分区”，如果第一次拉取了分区1，如果后续分区1没有数据，就不需要拉取分区1了。</p>
<p>FetchSession的数据结构如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">case</span> class <span class="title">FetchSession</span><span class="params">(val id: Int, // session编号是随机<span class="number">32</span>位数字，防止未授权的客户端伪造数据</span></span></span><br><span class="line"><span class="function"><span class="params">                        val privileged: Boolean,</span></span></span><br><span class="line"><span class="function"><span class="params">                        val partitionMap: FetchSession.CACHE_MAP,</span></span></span><br><span class="line"><span class="function"><span class="params">                        val creationMs: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">var</span> lastUsedMs: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">                        <span class="keyword">var</span> epoch: Int)</span> <span class="comment">// 自增</span></span></span><br></pre></td></tr></table></figure>
<p>为了支持增量拉取，FetchSession需要维护每个分区的以下信息：</p>
<ul>
<li>topic，partition Index（来自于TopicParttition）</li>
<li>maxBytes，fetchOffset，fetcherLogStartOffset（来自于最近一次的拉取请求）</li>
<li>highWatermark，localLogStartOffset（来自Leader的本地日志）</li>
</ul>
<blockquote>
<p>因为Follower或者Consumer发送拉取请求都是到Leader，所以FetchSession也是记录在Leader节点上的</p>
</blockquote>
<p>FetchRequest Metadata（客户端的拉取请求元数据）</p>
<table>
<thead>
<tr>
<th>sessionId</th>
<th>epoch</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>-1</td>
<td>全量拉取（没有使用或者创建session时）</td>
</tr>
<tr>
<td>0</td>
<td>0</td>
<td>全量拉取（如果是新的会话，epoch从1开始）</td>
</tr>
<tr>
<td><code>$ID</code></td>
<td>0</td>
<td>关闭标识为<code>$ID</code>的增量拉取会话，并创建一个新的全量拉取</td>
</tr>
<tr>
<td><code>$ID</code></td>
<td><code>$EPOCH</code></td>
<td>创建增量拉取</td>
</tr>
</tbody>
</table>
<p>对于客户端而言，什么时候一个分区会被包含到增量的拉取请求中：</p>
<ul>
<li>客户端通知Broker，分区的maxBytes,fetchOffset,logStartOffset改变了</li>
<li>分区在之前的增量拉取会话中不存在，客户端想要增加这个分区（拉取新的分区）</li>
<li>分区在增量拉取会话中，客户端要移除</li>
</ul>
<p>Fetch Response Metadata（服务端返回给客户端的sessionId）</p>
<table>
<thead>
<tr>
<th>sessionId</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>之前没有创建过拉取回话</td>
</tr>
<tr>
<td><code>$ID</code></td>
<td>下一个请求会是增量的拉取请求，并且sessionId是<code>$ID</code></td>
</tr>
</tbody>
</table>
<p>服务端增加分区包含到增量的拉取响应中：</p>
<ul>
<li>Broker通知客户端分区的hw或者brokerLogStartOffset变化了</li>
<li>分区有新的数据</li>
</ul>
<h3 id="源码解析">源码解析</h3><p>Fetcher.java#sendFetches(): prepareFetchRequests创建FetchSessionHandler.FetchRequestData。<br>构建拉取请求通过FetchSessionHandler.Builder，builder.add(partition, PartitionData)会添加next：<br>即要拉取的分区。构建时调用Builder.build()，针对Full拉取：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// FetchSessionHandler.Builder.build()</span></span><br><span class="line"><span class="keyword">if</span> (nextMetadata.isFull()) &#123; <span class="comment">// epoch=0或者-1</span></span><br><span class="line">    sessionPartitions = next; <span class="comment">// next为之前调动add添加的分区</span></span><br><span class="line">    next = <span class="keyword">null</span>; <span class="comment">// 本地full拉取，下次next=null</span></span><br><span class="line">    Map&lt;TopicPartition, PartitionData&gt; toSend = Collections.unmodifiableMap(<span class="keyword">new</span> LinkedHashMap&lt;&gt;(sessionPartitions));</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> FetchRequestData(toSend, Collections.emptyList(), toSend, nextMetadata);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>收到响应结果后，通过sessionHandler，调用FetchSessionHandler.handleResponse()。<br>假设第一次是Full拉取，响应结果没有出错时，nextMetadata.isFull()仍然为true。<br>假设服务端创建了一个新的session（随机的唯一ID），客户端的Fetch SessionId会设置为服务端返回的sessionId，<br>并且epoch会增加1。这样下次客户端的拉取就不再是Full，而是Increment了（toSend, toForget分别表示要拉取的和不需要拉取的）。<br>同样假设服务端正常处理（这次不会生成新的session），客户端也正常处理响应，则sessionId不会增加，但是epoch会增加1</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">handleResponse</span><span class="params">(FetchResponse&lt;?&gt; response)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (response.error() != Errors.NONE) &#123;</span><br><span class="line">        log.info(<span class="string">"Node &#123;&#125; was unable to process the fetch request with &#123;&#125;: &#123;&#125;."</span>,</span><br><span class="line">            node, nextMetadata, response.error());</span><br><span class="line">        <span class="keyword">if</span> (response.error() == Errors.FETCH_SESSION_ID_NOT_FOUND) &#123;</span><br><span class="line">            nextMetadata = FetchMetadata.INITIAL;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            nextMetadata = nextMetadata.nextCloseExisting();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nextMetadata.isFull()) &#123;</span><br><span class="line">        String problem = verifyFullFetchResponsePartitions(response);</span><br><span class="line">        <span class="keyword">if</span> (problem != <span class="keyword">null</span>) &#123;</span><br><span class="line">            log.info(<span class="string">"Node &#123;&#125; sent an invalid full fetch response with &#123;&#125;"</span>, node, problem);</span><br><span class="line">            nextMetadata = FetchMetadata.INITIAL;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (response.sessionId() == INVALID_SESSION_ID) &#123;</span><br><span class="line">            log.debug(<span class="string">"Node &#123;&#125; sent a full fetch response&#123;&#125;"</span>,</span><br><span class="line">                node, responseDataToLogString(response));</span><br><span class="line">            nextMetadata = FetchMetadata.INITIAL;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// The server created a new incremental fetch session. 客户端正常处理全量拉取的响应</span></span><br><span class="line">            log.debug(<span class="string">"Node &#123;&#125; sent a full fetch response that created a new incremental "</span> +</span><br><span class="line">                <span class="string">"fetch session &#123;&#125;&#123;&#125;"</span>, node, response.sessionId(), responseDataToLogString(response));</span><br><span class="line">            nextMetadata = FetchMetadata.newIncremental(response.sessionId());</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        String problem = verifyIncrementalFetchResponsePartitions(response);</span><br><span class="line">        <span class="keyword">if</span> (problem != <span class="keyword">null</span>) &#123;</span><br><span class="line">            log.info(<span class="string">"Node &#123;&#125; sent an invalid incremental fetch response with &#123;&#125;"</span>, node, problem);</span><br><span class="line">            nextMetadata = nextMetadata.nextCloseExisting();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (response.sessionId() == INVALID_SESSION_ID) &#123;</span><br><span class="line">            <span class="comment">// The incremental fetch session was closed by the server.</span></span><br><span class="line">            log.debug(<span class="string">"Node &#123;&#125; sent an incremental fetch response closing session &#123;&#125;&#123;&#125;"</span>,</span><br><span class="line">                node, nextMetadata.sessionId(), responseDataToLogString(response));</span><br><span class="line">            nextMetadata = FetchMetadata.INITIAL;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// The incremental fetch session was continued by the server. 客户端正常处理增量拉取的响应结果</span></span><br><span class="line">            log.debug(<span class="string">"Node &#123;&#125; sent an incremental fetch response for session &#123;&#125;&#123;&#125;"</span>,</span><br><span class="line">                node, response.sessionId(), responseDataToLogString(response));</span><br><span class="line">            nextMetadata = nextMetadata.nextIncremental();</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端处理拉取请求时，会创建不同类型的FetchContext：</p>
<ul>
<li>SessionErrorContext：拉取会话错误（比如epoch不相等）</li>
<li>SessionlessFetchContext：不需要拉取会话（旧版本）</li>
<li>IncrementalFetchContext：增量拉取</li>
<li>FullFetchContext：全量拉取</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// KafkaApis.handleFetchRequest</span></span><br><span class="line">    val fetchContext = fetchManager.newContext(</span><br><span class="line">      fetchRequest.metadata,</span><br><span class="line">      fetchRequest.fetchData,</span><br><span class="line">      fetchRequest.toForget,</span><br><span class="line">      fetchRequest.isFromFollower)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 针对不同的拉取上下文，分别更新并生成响应数据</span></span><br><span class="line">    unconvertedFetchResponse = fetchContext.updateAndGenerateResponseData(partitions)</span><br></pre></td></tr></table></figure>
<p>服务端的FetchManager创建Context时，如果FetchMetadata.isFull，再判断epoch=-1时，类型为SessionlessFetchContext，<br>否则（epoch=0）时，类型为FullFetchContext。如果!isFull()，必须保证session.epoch = FetchMetadata.epoch，否则类型为SessionErrorContext。<br>当!isFull且epoch相等时，先增加session.epoch（服务端的epoch，即为客户端下次拉取的epoch），然后返回类型为IncrementalFetchContext。</p>
<p>FullFetchContext更新响应数据，对于全量拉取，一般是新会话，所以需要更新缓存</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">override def <span class="title">updateAndGenerateResponseData</span><span class="params">(updates: FetchSession.RESP_MAP)</span>: FetchResponse[Records] </span>= &#123;</span><br><span class="line">  def createNewSession: FetchSession.CACHE_MAP = &#123;</span><br><span class="line">    val cachedPartitions = <span class="keyword">new</span> FetchSession.CACHE_MAP(updates.size)</span><br><span class="line">    updates.entrySet.asScala.foreach(entry =&gt; &#123;</span><br><span class="line">      val part = entry.getKey</span><br><span class="line">      val respData = entry.getValue</span><br><span class="line">      val reqData = fetchData.get(part)</span><br><span class="line">      cachedPartitions.mustAdd(<span class="keyword">new</span> CachedPartition(part, reqData, respData))</span><br><span class="line">    &#125;)</span><br><span class="line">    cachedPartitions</span><br><span class="line">  &#125;</span><br><span class="line">  val responseSessionId = cache.maybeCreateSession(time.milliseconds(), isFromFollower,</span><br><span class="line">      updates.size, () =&gt; createNewSession)</span><br><span class="line">  debug(s<span class="string">"Full fetch context with session id $responseSessionId returning "</span> +</span><br><span class="line">    s<span class="string">"$&#123;partitionsToLogString(updates.keySet)&#125;"</span>)</span><br><span class="line">  <span class="keyword">new</span> FetchResponse(Errors.NONE, updates, <span class="number">0</span>, responseSessionId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">def <span class="title">maybeCreateSession</span><span class="params">(now: Long,</span></span></span><br><span class="line"><span class="function"><span class="params">                       privileged: Boolean,</span></span></span><br><span class="line"><span class="function"><span class="params">                       size: Int,</span></span></span><br><span class="line"><span class="function"><span class="params">                       createPartitions: ()</span> </span>=&gt; FetchSession.CACHE_MAP): Int =</span><br><span class="line"><span class="keyword">synchronized</span> &#123;</span><br><span class="line">  <span class="comment">// If there is room, create a new session entry.</span></span><br><span class="line">  <span class="keyword">if</span> ((sessions.size &lt; maxEntries) ||</span><br><span class="line">      tryEvict(privileged, EvictableKey(privileged, size, <span class="number">0</span>), now)) &#123;</span><br><span class="line">    val partitionMap = createPartitions()</span><br><span class="line">    <span class="comment">// 这里创建一个新的session时，同时也会增加epoch，从0到1</span></span><br><span class="line">    val session = <span class="keyword">new</span> FetchSession(newSessionId(), privileged, partitionMap,</span><br><span class="line">        now, now, JFetchMetadata.nextEpoch(INITIAL_EPOCH))</span><br><span class="line">    debug(s<span class="string">"Created fetch session $&#123;session.toString&#125;"</span>)</span><br><span class="line">    sessions.put(session.id, session)</span><br><span class="line">    touch(session, now)</span><br><span class="line">    session.id</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    debug(s<span class="string">"No fetch session created for privileged=$privileged, size=$size."</span>)</span><br><span class="line">    INVALID_SESSION_ID</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结下客户端和服务端的Full拉取过程：</p>
<p>1.客户端创建的拉取请求FetchMetadata.isFull()，初始时epoch=0<br>2.服务端创建的FetchContext类型为FullFetchContext<br>3.服务端创建新的Session(xxx)，以及初始化epoch=1(0+1=1)，并缓存<br>4.客户端收到服务端的FetchResponse，设置FetchMetadata.sessionId为response中的sessionId(xxx)，并增加epoch=1(从步骤1的0+1=1)<br>5.客户端继续拉取，isFull=false，sessionId=xxx, epoch=1<br>6.服务端创建的FetchContext类型为IncrementalFetchContext（满足session.epoch=reqMetadata.epoch=1, isFull=false）<br>7.服务端增加epoch，设置session.epoch=2，为下次的拉取（对比epoch）做准备<br>8.对reqMetadata.epoch加1(=2)然后对比session.epoch(2)，如果不等，返回错误码INVALID_FETCH_SESSION_EPOCH，相等返回NONE<br>9.客户端收到服务端的FetchResponse，设置epoch增加1（sessionId没有变化时，不需要更新sessionId，实际上设置的是nextMetadata对象）</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://cwiki.apache.org/confluence/display/KAFKA/KIP-227%3A+Introduce+Incremental+FetchRequests+to+Increase+Partition+Scalability&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.cnblogs.com/huxi2b/p/9335064.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/huxi2b/p/9335064.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Source" scheme="http://github.com/zqhxuyuan/categories/Source/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka技术内幕</title>
    <link href="http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/"/>
    <id>http://github.com/zqhxuyuan/2018/01/01/Kafka-Code-Index/</id>
    <published>2017-12-31T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.339Z</updated>
    
    <content type="html"><![CDATA[<p>《Kafka技术内幕》<br><a href="http://www.ituring.com.cn/book/1927" target="_blank" rel="noopener">图灵社区主页</a> |<br><a href="http://product.china-pub.com/6817109" target="_blank" rel="noopener">ChinaPub购买链接</a> |<br><a href="https://item.jd.com/12234113.html?dist=jd" target="_blank" rel="noopener">京东购买链接</a> |<br><a id="more"></a></p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019131423727" alt="book"></p>
<p>Update:</p>
<p>2017-11-22: 公司内部做的一个分享：<a href="https://pan.baidu.com/s/1o85h2K6" target="_blank" rel="noopener">Kafka构建流式数据处理平台</a></p>
<h2 id="本书介绍：">本书介绍：</h2><p>本书主要以0.10版本的Kafka源码为基础，并通过图文详解的方式分析Kafka内部组件的实现细节，全书原创的图片有近400幅。对于Kafka流处理的一些新特性，也会分析0.11版本的相关源码。本书各个章节的主要内容如下。</p>
<ul>
<li>第一章首先介绍了Kafka作为流式数据平台的三个组成，包括消息系统、存储系统、流处理系统。Kafka基本概念的三种模型，包括分区模型、消费模型、分布式模型。然后介绍了Kafka几个比较重要的设计思路，最后介绍了如何在一台机器上模拟单机模式与分布式模式，以及如何搭建源码开发环境。</li>
<li>第二章从一个生产者的示例开始，引出了新版本生产者的两种消息发送方式。生产者客户端利用记录收集器、发送线程，对消息集进行分组和缓存，并为目标节点创建生产请求，发送到不同的代理节点。接着介绍了与网络相关的Kafka通道、选择器、轮询等NIO操作。另外还介绍了Scala版本的旧生产者，它使用阻塞通道的方式发送请求。最后介绍了服务端采用<code>Reactor</code>模式处理客户端的请求。</li>
<li>第三章首先介绍了消费者相关的基础概念，然后从一个消费者的示例开始，引出了基于ZooKeeper的高级消费者API，理解高级API主要是要理解消费线程的模型以及变量的传递方式。接着介绍了消费者提交分区偏移量的两种方式。最后，我们举了一个低级API的示例，开发者需要自己实现一些比较复杂的逻辑处理才能保证消费程序的健壮性和稳定性。</li>
<li>第四章介绍了新版本的消费者，不同于旧版本的消费者，新版本去除了ZooKeeper的依赖，统一了旧版本的高级API和低级API，并提供了两种消费方式：订阅和分配。新版本引入订阅状态来管理消费者的订阅信息、并使用拉取器拉取消息。新版本的消费者没有使用拉取线程，而是采用轮询的方式拉取消息，它的性能比旧版本的消费者更好。另外还介绍了消费者采用回调器、处理器、监听器、适配器、组合模式、链式调用等实现不同类型的异步请求。最后，我们介绍了新消费者的心跳任务、提交偏移量以及三种消息处理语义的使用方式。</li>
<li>第五章介绍了新版本消费者相关的协调者实现，主要包括“加入组”与“同步组”。每个消费者都有一个客户端的协调者，服务端也有一个消费组级别的协调者负责处理所有消费者客户端的请求。当消费组触发再平衡操作，服务端的协调者会记录消费组元数据的变化，并通过状态机保证消费组状态的正常转换。本章会通过很多不同的示例场景来帮助读者理解消费组相关的实现。</li>
<li>第六章介绍了Kafka的存储层实现，包括日志的读写、日志的管理、日志的压缩等一些常用的日志操作。服务端通过副本管理器处理客户端的生产请求和拉取请求。接着介绍了副本机制相关的分区、副本、最高水位、复制点等一些概念。最后介绍了延迟操作接口与延迟缓存。服务端如果不能立即返回响应结果给客户端，会先将延迟操作缓存起来，直到请求处理完成或超时。</li>
<li>第七章介绍了作为服务端核心的Kafka控制器，它主要负责管理分区状态机和副本状态机，以及多种类型的监听器，比如代理节点上线和下线、删除主题、重新分配分区等。控制器的一个重要职责是选举分区的主副本。不同代理节点根据控制器下发的请求，决定成为分区的主副本还是从副本。另外，我们还分析了本地副本与远程副本的区别，以及元数据缓存的作用。</li>
<li>第八章首先介绍了两种集群的同步工具：Kafka内置的MirrorMaker和Uber开源的uReplicator。接着介绍了新版本Kafka提供的连接器框架，以及如何开发一个自定义的连接器。然后介绍了连接器的架构模型的具体实现，主要包括数据模型、Connector模型、Worker模型。</li>
<li>第九章介绍了Kafka流处理的两种API：低级Processor API和高级DSL。本章我们重点介绍了流处理的线程模型，主要包括流实例、流线程、流任务。我们还介绍了流处理的本地状态存储，它主要用来作为备份任务的数据恢复。高级DSL包括两个组件：<code>KStream</code>与<code>KTable</code>，它们都定义了一些常用的流处理算子操作，比如无状态的操作（过滤、映射等）、有状态的操作（连接、窗口等）。</li>
<li>第十章介绍了Kafka的一些高级特性，比如客户端的配额、新的消息格式、事务特性。</li>
</ul>
<p>本书相关的示例代码在笔者的Github主页<a href="https://github.com/zqhxuyuan/kafka-book">https://github.com/zqhxuyuan/kafka-book</a>上，另外，限于篇幅，本书的附录部分会放在个人博客上。由于个人能力有限，文中的错误在所难免，读者在阅读本书的过程中，发现不妥之处，可以私信笔者的微博：<a href="http://weibo.com/xuyuantree" target="_blank" rel="noopener">http://weibo.com/xuyuantree</a>，笔者会定期将勘误表更新到个人博客上。</p>
<h2 id="English_Introduce">English Introduce</h2><p>《Apache Kafka Internal》</p>
<p>This book mostly based on Kafka-0.10, and some part of 0.11 for streaming. It has nearly 400 pictures to analysis Kafka internal implementation. The book written from client to coordinator, from storage to controller, and also including Kafka Connect and Kafka Streams. Here is content introduction of each chapter:</p>
<p>Chapter 1: Being a streaming platform, kafka composed of message system, storage and streaming processing. There are three model of Kafka basic concepts: Partition model, Consumer model and Distributed model. We also introduce some important design ideas of kafka, such as file system persistent, data transformation, producer and consumer, replication and HA.</p>
<p>Chapter 2: From a producer example into how client send message. The whole workflow include record accumulator, sender thread, grouping message, create request and at last send to different target broker. Then we introduce Kafka channel, selector and also how server use NIO reactor to handle client request.</p>
<p>Chapter 3: From a old high-level consumer example into zookeeper based api. The most important of high-level consuemr is consumer thread model. Then we introduce two approach to commit consumer offset which is zookeeper or internal topic. After that, we illustrate how to write low-level consumer to ensure processing messages stability and robust.</p>
<p>Chapter 4: New version consumer client use subscription state and polling fetch instead of fetcher thread. We also introduce how consumer use callback, handler, listener, adapter, chain to implement different asynchronous request mode. Last we introduce heartbeat, offset commit and three consumer processing semantic: at-most-once,at-least-once,exactly-once.</p>
<p>Chapter 5: New consumer communicate with server coordinator by ConsumerCoordinator, there’re mainly two request/response involved: Join-group and Sync-group. This process also called consumer group rebalance. We also discussed how server coordinator use state machine to ensure group state transformation, such as PreparingRebalance,AwaitingSync,Stable. This chapter also give some different scene to help reader understand how consumer group worked in production environment.</p>
<p>Chapter 6: Kafka’s storage layer process include log read/write, log manager, log compaction. In server side, ReplicationManager is responsible for client’s request. Then we introduce Replication mechanism concepts, such as Partition, Replication, HW, LEO. Last we introduce delayed operation and delayed purgatory. If server can’t response immediately to client, they have to cache request and send response to client some times later.</p>
<p>Chapter 7: Kafka Controller component is in charge of managing PartitionState, ReplicationState, and some listeners, such as broker up/down, topic deletion, partition reassign. The main duty of controller is selecting partition’s leader and sent LeaderAndIsr request down to brokers. Target brokers receiving request will decide to be partition leader or follower. Furthermore, we introduce the different between local replication and remote replication, also the function of metadata cache.</p>
<p>Chapter 8: First we introduce two kind of cluster synchronization: Kafka internal MirrorMaker and Uber open sourced uReplicator, we also show how apache helix build replicated uReplicator. Next we introduce new build-in kafka connect framework and how to develop a custom connector plugin. Then we deep into connector’s architecture, mainly concentrate on data model, connector model, worker model.</p>
<p>Chapter 9: Introduce Kafka Streams two api: low-level processor and high-level DSL. This chapter focus on streaming thread model, including stream instance, thread and task. We also introduce local state store used by standby task for recovery. After that, we introduce two abstract components in High-level DSL: KStream and KTable, they both based on low-level processor, support common operator and advance function, such as window, join and so on.</p>
<p>Chapter 10: Introduce some advanced features. such as client quota, new message format in 0.11 and also  transaction support.</p>
<h2 id="目录">目录</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019132244310" alt="toc1"></p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019132056607" alt="toc"></p>
<h2 id="勘误表">勘误表</h2><p>TODO</p>
<hr>
<p>下面是以前写的一些博客，当然实际的书籍已经改动很大了，下面的一些博文仅供参考。</p>
<h2 id="Introduce">Introduce</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/05/2016-01-05-Kafka-Unix/" target="_blank" rel="noopener">使用Unix管道解释Kafka</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/13/2016-01-13-Kafka-Picture/" target="_blank" rel="noopener">Kafka图文理解</a></li>
</ul>
<h2 id="源码分析汇总">源码分析汇总</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/01/06/2016-01-06-Kafka_Producer/" target="_blank" rel="noopener">生产者(java)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/07/2016-01-07-Kafka_Producer-scala/" target="_blank" rel="noopener">生产者(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/08/2016-01-08-Kafka_SocketServer/" target="_blank" rel="noopener">网络层SocketServer</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/10/2016-01-10-Kafka_LogAppend/" target="_blank" rel="noopener">消息存储到日志文件中</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/14/2016-01-14-Kafka-ISR/" target="_blank" rel="noopener">Partition的ISR工作机制</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/19/2016-01-19-Kafka-Consumer-scala/" target="_blank" rel="noopener">消费者初始化(scala)</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/01/20/2016-01-20-Kafka-Consumer-fetcher/" target="_blank" rel="noopener">消费者抓取流程</a></li>
<li>☆☆☆<a href="http://zqhxuyuan.github.io/2016/05/26/2016-05-13-Kafka-Book-Sample/" target="_blank" rel="noopener">旧的样章</a>  </li>
</ul>
<h2 id="新消费者">新消费者</h2><ul>
<li>☆☆☆☆☆<a href="http://zqhxuyuan.github.io/2016/02/20/Kafka-Consumer-New/" target="_blank" rel="noopener">译：使用新的Kafka消费者客户端</a></li>
</ul>
<h2 id="Kafka_Connect">Kafka Connect</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-Connect/" target="_blank" rel="noopener">使用Kafka Connect构建一个可扩展的ETL管道</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/02/21/Kafka-connect-pipeline/" target="_blank" rel="noopener">使用Kafka Connect构建大规模低延迟的数据管道</a></li>
</ul>
<h2 id="Kafka_Streams">Kafka Streams</h2><ul>
<li><a href="http://zqhxuyuan.github.io/2016/11/18/Kafka-CQRS-Streams/" target="_blank" rel="noopener">译：Kafka事件驱动和流处理</a></li>
<li>☆☆☆☆☆<a href="http://zqhxuyuan.github.io/2016/11/02/Kafka-Streams-cn/" target="_blank" rel="noopener">Kafka Streams中文翻译</a></li>
<li><a href="http://zqhxuyuan.github.io/2016/10/29/Kafka-Interactive-Query/" target="_blank" rel="noopener">译：Kafka交互式查询和流处理的统一</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;《Kafka技术内幕》&lt;br&gt;&lt;a href=&quot;http://www.ituring.com.cn/book/1927&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;图灵社区主页&lt;/a&gt; |&lt;br&gt;&lt;a href=&quot;http://product.china-pub.com/6817109&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChinaPub购买链接&lt;/a&gt; |&lt;br&gt;&lt;a href=&quot;https://item.jd.com/12234113.html?dist=jd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;京东购买链接&lt;/a&gt; |&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka技术内幕拾遗</title>
    <link href="http://github.com/zqhxuyuan/2017/12/31/Kafka-Book-Resources/"/>
    <id>http://github.com/zqhxuyuan/2017/12/31/Kafka-Book-Resources/</id>
    <published>2017-12-30T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.337Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka技术内幕拾遗</p>
<ul>
<li>✅ 客户端元数据（Metadata）</li>
<li>✅ 客户端线程模型（ThreadModel）</li>
<li>即席查询（Interactive Query）</li>
<li>EOS事务（Transaction）<a id="more"></a>
</li>
</ul>
<h3 id="客户端的元数据对象">客户端的元数据对象</h3><p>客户端的连接对象（<code>NetworkClient</code>）在轮询时会判断是否需要更新元数据。客户端调用元数据更新器的<code>maybeUpdate()</code>方法，并不一定每次都需要更新元数据。只有当元数据的超时时间（<code>metadataTimeout</code>）等于<code>0</code>时，客户端才会发送元数据请求。</p>
<h4 id="1-_客户端轮询与元数据更新器">1. 客户端轮询与元数据更新器</h4><p>客户端调用选择器的轮询方法，最长的阻塞时间会在“轮询时间（<code>pollTimeout</code>）、元数据的更新时间（<code>metadataTimeout</code>）、请求的超时时间（<code>requestTimeoutMs</code>）”三者中选取最小值。如果元数据的更新时间等于0，表示客户端会立即发送元数据请求，不会阻塞。下面解释这几个时间变量的数据来源，以及它们在发送请求过程中所代表的含义。</p>
<ul>
<li>生产者的<code>requestTimeoutMs</code>变量，对应的配置项是<code>request.timeout.ms</code>，默认值<code>30</code>秒。该配置表示生产者等待收到响应结果的最长时间。如果生产者在这个时间超时后没有收到响应结果，就会认为生产请求失败，它可以重新发送生产请求。</li>
<li>生产者的<code>retryBackoffMs</code>变量，对应的配置项是<code>retry.backoff.ms</code>，默认值<code>100</code>毫秒。该配置表示客户端发送请求失败时，为了避免在短时间内客户端重复地发送请求导致重试次数用光，客户端必须要等待一小会儿才允许发送新的请求。这个配置项可用于元数据请求、生产请求和拉取请求，但只有在发送失败时才会用到。该配置会传给元数据对象（元数据请求）、记录收集器（生产请求）。</li>
<li>生产者的<code>lingerMs</code>变量，对应的配置项是<code>linger.ms</code>，默认值为<code>0</code>毫秒。该配置表示生产者在发送请求之前是否会延迟等待一段时间收集更多的消息。如果等于0，表示生产者会立即发送请求。</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 客户端的网络连接对象在每次轮询之前，都会判断是否需要更新元数据</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NetworkClient</span> <span class="keyword">implements</span> <span class="title">KafkaClient</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> MetadataUpdater metadataUpdater; <span class="comment">// 元数据的更新器</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 生产者会由发送线程调用该方法，消费者会由ConsumerNetworkClient调用该方法</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> List&lt;ClientResponse&gt; <span class="title">poll</span><span class="params">(<span class="keyword">long</span> pollTimeout, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> metadataTimeout = metadataUpdater.maybeUpdate(now);</span><br><span class="line">    selector.poll(Utils.min(pollTimeout,metadataTimeout,requestTimeoutMs));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">DefaultMetadataUpdater</span> <span class="keyword">implements</span> <span class="title">MetadataUpdater</span> </span>&#123;</span><br><span class="line">    Metadata metadata;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">maybeUpdate</span><span class="params">(<span class="keyword">long</span> now, Node node)</span> </span>&#123;</span><br><span class="line">      <span class="comment">// 这里简化了其他一些判断条件，实际的超时时间计算方式比较复杂</span></span><br><span class="line">      <span class="keyword">long</span> metadataTimeout = metadata.timeToNextUpdate(now);</span><br><span class="line">      <span class="keyword">if</span>(metadataTimeout == <span class="number">0</span>) <span class="comment">// 准备发送“获取元数据”的请求</span></span><br><span class="line">        doSend(<span class="keyword">new</span> MetadataRequest(metadata.topics()), now);</span><br><span class="line">      <span class="keyword">return</span> metadataTimeout; </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 处理“获取元数据请求”的响应</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">handleResponse</span><span class="params">(RequestHeader header, Struct body, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">      Cluster cluster = <span class="keyword">new</span> MetadataResponse(body).cluster();</span><br><span class="line">      <span class="keyword">this</span>.metadata.update(cluster, now); <span class="comment">// 更新元数据的具体逻辑</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>客户端每次轮询收到元数据请求的响应结果后，会解析成<code>Cluster</code>对象，然后更新元数据对象。</p>
<h4 id="2-_元数据对象">2. 元数据对象</h4><p>元数据对象有多个用于控制元数据更新策略的变量，相关的时间配置项主要有下面几个。</p>
<ul>
<li><code>metadata.fetch.timeout.ms</code>（生产者的<code>maxBlockTimeMs</code>变量，默认值为<code>60</code>秒）：生产者第一次发送消息，如果主题没有分区，它等待元数据更新的最长阻塞时间（第7.3.2节第三小节）。</li>
<li><code>metadata.max.age.ms</code>（元数据的<code>metadataExpireMs</code>变量，默认值为五分钟）：即使不需要更新元数据，客户端也需要间隔一段时间更新一次元数据。</li>
<li><code>retry.backoff.ms</code>（元数据的<code>refreshBackoffMs</code>变量，默认值为<code>100</code>毫秒）：客户端多次发送元数据请求，需要等待一小段时间再发送元数据请求。</li>
</ul>
<p>元数据的更新时间主要与后两项配置有关。<code>refreshBackoffMs</code>变量用来计算允许更新的时间（<code>timeToAllowUpdate</code>），<code>metadataExpireMs</code>变量用来计算失效的时间（<code>timeToExpire</code>）。默认情况下，<code>retry.backoff.ms</code>等于<code>100</code>毫秒时，允许更新的时间一般小于<code>0</code>。<code>timeToNextUpdate()</code>方法主要取决于失效的时间，下面列举了几种不同的场景。</p>
<ul>
<li>需要更新元数据时，失效时间等于<code>0</code>，表示需要立即更新元数据。</li>
<li>当前时间在失效阈值的范围内，即上次更新时间加上失效阈值大于当前时间，失效时间等于上次更新时间加上失效阈值，再减去当前时间，结果会大于0，表示再过指定的失效时间才需要更新元数据。</li>
<li>当前时间超过失效阈值的范围，即当前时间大于上次更新时间加上失效阈值，失效时间也设置为0。</li>
</ul>
<blockquote>
<p><strong>注意：</strong>元数据对象的<code>metadataExpireMs</code>和<code>refreshBackoffMs</code>都是固定的值，<code>timeToNextUpdate()</code>方法依赖<code>needUpdate</code>和上次的更新时间，来计算下次更新元数据的时间。当调用元数据对象的<code>requestUpdate()</code>方法和<code>update()</code>方法时，才会分别更新<code>needUpdate</code>和上次的更新时间。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Metadata</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> refreshBackoffMs; <span class="comment">// 更新失败时，下一次更新的补偿时间</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> metadataExpireMs; <span class="comment">// 每隔多久，更细一次元数据</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">int</span> version; <span class="comment">// 版本号，当更新一次元数据，版本号加一</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> lastRefreshMs; <span class="comment">// 上一次更新的时间，更新失败也会更新这个值</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">long</span> lastSuccessfulRefreshMs; <span class="comment">// 上一次成功更新的时间</span></span><br><span class="line">  <span class="keyword">private</span> Cluster cluster; <span class="comment">// 集群的配置信息</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">boolean</span> needUpdate; <span class="comment">// 是否需要更新元数据</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">int</span> <span class="title">requestUpdate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.needUpdate = <span class="keyword">true</span>; <span class="comment">// 需要更新元数据</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">this</span>.version; <span class="comment">// 返回当前的版本号，这个版本号是旧的</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">boolean</span> <span class="title">updateRequested</span><span class="params">()</span></span>&#123;<span class="keyword">return</span> <span class="keyword">this</span>.needUpdate;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">long</span> <span class="title">timeToNextUpdate</span><span class="params">(<span class="keyword">long</span> nowMs)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">long</span> timeToExpire = needUpdate ? <span class="number">0</span> : Math.max(</span><br><span class="line">      <span class="keyword">this</span>.lastSuccessfulRefreshMs + <span class="keyword">this</span>.metadataExpireMs - nowMs, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">long</span> timeToAllowUpdate=<span class="keyword">this</span>.lastRefreshMs+<span class="keyword">this</span>.refreshBackoffMs-nowMs;</span><br><span class="line">    <span class="keyword">return</span> Math.max(timeToExpire, timeToAllowUpdate);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">awaitUpdate</span><span class="params">(<span class="keyword">int</span> lastVersion,<span class="keyword">long</span> maxWaitMs)</span></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> begin = System.currentTimeMillis();</span><br><span class="line">    <span class="keyword">long</span> remainingWaitMs = maxWaitMs;</span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">this</span>.version &lt;= lastVersion) &#123;</span><br><span class="line">      <span class="keyword">if</span> (remainingWaitMs != <span class="number">0</span>) wait(remainingWaitMs); <span class="comment">// 等待</span></span><br><span class="line">      <span class="keyword">long</span> elapsed = System.currentTimeMillis() - begin;</span><br><span class="line">      <span class="keyword">if</span> (elapsed &gt;= maxWaitMs) <span class="keyword">throw</span> <span class="keyword">new</span> TimeoutException(<span class="string">"failed"</span>)</span><br><span class="line">      remainingWaitMs = maxWaitMs - elapsed;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">(Cluster cluster, <span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.needUpdate = <span class="keyword">false</span>;</span><br><span class="line">    <span class="keyword">this</span>.version += <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">this</span>.lastRefreshMs = now;</span><br><span class="line">    <span class="keyword">this</span>.lastSuccessfulRefreshMs = now;</span><br><span class="line">    <span class="keyword">for</span>(Listener listener:listeners) listener.onMetadataUpdate(cluster);</span><br><span class="line">    <span class="keyword">this</span>.cluster = cluster;</span><br><span class="line">    notifyAll(); <span class="comment">// 通知</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>元数据对象的每个方法都加上了<code>synchronized</code>关键字，即使有多个客户端线程（用户线程）使用同一个生产者示例，并且访问相同的元数据对象，也是线程安全的。<code>awaitUpdate()</code>方法只会被生产者在的<code>waitOnMetadata()</code>方法调用。如果元数据的版本号（<code>this.version</code>）小于上一次的版本号（<code>lastVersion</code>），用户线程会通过<code>wait()</code>进入阻塞状态。调用元数据对象的<code>update()</code>方法，更新版本号，并通知用户线程退出<code>awaitUpdate()</code>方法。</p>
<p>元数据对象除了会更新元数据内容，还有一个保存集群配置的<code>Cluster</code>对象。<code>Cluster</code>保存了分区信息相关的变量，分区信息包括分区的主副本、<code>ISR</code>、<code>AR</code>等内容。第二章生产者客户端发送消息时，利用“分区信息”为消息指定分区编号。本章从控制器、<code>LeaderAndIsr</code>请求，最后到<code>Metadata</code>请求，与第二章的“分区信息”互相呼应，算是画上了一个圆满的句号。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Cluster</span> </span>&#123; <span class="comment">// 集群配置</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> List&lt;Node&gt; nodes;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Set&lt;String&gt; unauthorizedTopics;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;TopicPartition, PartitionInfo&gt; partitionsByTopicPartition;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; partitionsByTopic;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;String, List&lt;PartitionInfo&gt;&gt; availablePartitionsByTopic;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, List&lt;PartitionInfo&gt;&gt; partitionsByNode;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Map&lt;Integer, Node&gt; nodesById;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionInfo</span> </span>&#123; <span class="comment">// 分区信息</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Node leader;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Node[] replicas;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">final</span> Node[] inSyncReplicas;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="3-_元数据更新的日志与实例">3. 元数据更新的日志与实例</h4><p>下面举例了生产者发送两条消息，为了模拟发送第一条消息时，生产者必须要等待元数据更新完成。下面的代码会在第一条消息发送完成后等待一秒钟才发送第二条消息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 生产者发送消息的示例</span></span><br><span class="line">KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">log.info(<span class="string">"start producer client app"</span>);</span><br><span class="line">Thread.sleep(<span class="number">1000</span>*<span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">log.info(<span class="string">"start send #1 message..."</span>);</span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"test"</span>, <span class="string">"m1"</span>));</span><br><span class="line">log.info(<span class="string">"sending #1 message end.."</span>);</span><br><span class="line">Thread.sleep(<span class="number">1000</span>); <span class="comment">// 等待一秒才发送第二条消息</span></span><br><span class="line">log.info(<span class="string">"start send #2 message..."</span>);</span><br><span class="line">producer.send(<span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"test"</span>, <span class="string">"m2"</span>));</span><br><span class="line">log.info(<span class="string">"sending #2 message end.."</span>);</span><br></pre></td></tr></table></figure>
<p>为了更清晰地理解元数据、<code>NetworkClient</code>一些变量的含义，在必要的地方加上了日志（比如<code>needUpdate</code>、<code>metadataTimeout</code>等）。将日志级别调成<code>TRACE</code>后，更详细的日志如下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line">[18:00:04,596] TRACE Starting the Kafka producer</span><br><span class="line">[18:00:04,939] DEBUG Updated cluster metadata version 1 to Cluster(</span><br><span class="line">  nodes = [localhost:9092 (id: -1 rack: null)], partitions = [])</span><br><span class="line">[18:00:05,077] DEBUG Starting Kafka producer I/O thread.</span><br><span class="line">[18:00:05,079] INFO [NetworkClient] select timeout:30000</span><br><span class="line">[18:00:05,094] DEBUG Kafka producer started</span><br><span class="line">[18:00:05,094] INFO start producer client app (kafka.examples.Producer)</span><br><span class="line"></span><br><span class="line">[18:00:15,103] INFO start send #1 message... (kafka.examples.Producer)</span><br><span class="line">[18:00:15,109] TRACE Requesting metadata update for topic test. 【1】</span><br><span class="line">[18:00:15,109] TRACE Waking up Sender thread for metadata update.</span><br><span class="line">[18:00:15,111] INFO [Metadata] awaitUpdate begin...</span><br><span class="line">[18:00:15,117] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,117] INFO [Metadata] needUpdate: true</span><br><span class="line">[18:00:15,118] INFO [MetadataUpdater] metadataTimeout: 0</span><br><span class="line">[18:00:15,118] DEBUG Initialize connection to node1 for send metadata request</span><br><span class="line">[18:00:15,118] DEBUG Initiating connection to node1 at localhost:9092. 【2】</span><br><span class="line">[18:00:15,241] INFO [NetworkClient] metadataTimeout:0</span><br><span class="line">[18:00:15,241] INFO [NetworkClient] select timeout:0</span><br><span class="line">[18:00:15,246] DEBUG Completed connection to node -1</span><br><span class="line"></span><br><span class="line">[18:00:15,246] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,246] INFO [Metadata] needUpdate: true</span><br><span class="line">[18:00:15,247] INFO [MetadataUpdater] metadataTimeout: 0</span><br><span class="line">[18:00:15,443] DEBUG Sending metadata request &#123;topics=[test]&#125; to node -1 【3】</span><br><span class="line">[18:00:15,444] INFO [NetworkClient] metadataTimeout:0</span><br><span class="line">[18:00:15,444] INFO [NetworkClient] select timeout:0</span><br><span class="line">[18:00:15,448] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,448] INFO [Metadata] needUpdate: true</span><br><span class="line">[18:00:15,449] INFO [NetworkClient] metadataTimeout:2147483647</span><br><span class="line">[18:00:15,449] INFO [NetworkClient] select timeout:30000</span><br><span class="line"></span><br><span class="line">[18:00:15,628] DEBUG Updated cluster metadata version 2 to Cluster( 【4】</span><br><span class="line">  nodes = [192.168.199.101:9092 (id: 0 rack: null)], partitions = [</span><br><span class="line">   Partition(topic=test,partition=1,leader=0,replicas=[0,],isr=[0,], </span><br><span class="line">   Partition(topic=test,partition=0,leader=0,replicas=[0,],isr=[0,], </span><br><span class="line">   Partition(topi =test,partition=2,leader=0,replicas=[0,],isr=[0,]])</span><br><span class="line">[18:00:15,628] INFO [Metadata] awaitUpdate end...</span><br><span class="line"></span><br><span class="line">[18:00:15,628] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,628] INFO [Metadata] needUpdate: false</span><br><span class="line">[18:00:15,629] INFO [NetworkClient] metadataTimeout:299839</span><br><span class="line">[18:00:15,629] INFO [NetworkClient] select timeout:30000</span><br><span class="line"></span><br><span class="line">[18:00:15,636] TRACE Sending record ProducerRecord(topic=test, partition=null,</span><br><span class="line">  key=null, value=m1, timestamp=null) with callback null to topic test_0 【5】</span><br><span class="line">[18:00:15,636] TRACE Allocating a new 16384 byte message buffer for test_0</span><br><span class="line">[18:00:15,700] TRACE Waking up the sender, test_0 is full or a new batch 【6】</span><br><span class="line">[18:00:15,700] INFO sending #1 message end.. (kafka.examples.Producer)</span><br><span class="line"></span><br><span class="line">[18:00:15,702] INFO [accumulator] batch: test-0</span><br><span class="line">[18:00:15,702] INFO [accumulator] ready expired: true</span><br><span class="line">[18:00:15,702] INFO [Metadata] needUpdate: false</span><br><span class="line">[18:00:15,703] DEBUG Initiating connection to node 0 at localhost:9092. 【7】</span><br><span class="line">[18:00:15,704] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,705] INFO [NetworkClient] metadataTimeout:299767</span><br><span class="line">[18:00:15,705] INFO [NetworkClient] select timeout:30000</span><br><span class="line">[18:00:15,706] DEBUG Completed connection to node 0</span><br><span class="line"></span><br><span class="line">[18:00:15,706] INFO [accumulator] batch: test-0</span><br><span class="line">[18:00:15,707] INFO [accumulator] ready expired: true</span><br><span class="line">[18:00:15,707] INFO [Metadata] needUpdate: false</span><br><span class="line">[18:00:15,707] INFO [accumulator] drained batch: test-0</span><br><span class="line">[18:00:15,718] TRACE Nodes with data ready to send: [localhost:9092]</span><br><span class="line">[18:00:15,719] TRACE Created 1 produce requests: [ClientRequest( 【8】</span><br><span class="line">  expectResponse=true,callback=o.a.k.c.p.internals.Sender$1@6008d3ea, </span><br><span class="line">  request=RequestSend(header=&#123;.&#125;, body=&#123;acks=1,timeout=30000,</span><br><span class="line">    topic_data=[&#123;topic=test,data=[&#123;partition=0,</span><br><span class="line">      record_set=HeapByteBuffer[pos=0 lim=36 cap=16384]</span><br><span class="line">  &#125;]&#125;]&#125;), createdTimeMs=1494151215706, sendTimeMs=0)]</span><br><span class="line">[18:00:15,719] INFO [Sender] readyNodes:1</span><br><span class="line">[18:00:15,720] INFO [NetworkClient] poll timeout:0</span><br><span class="line">[18:00:15,720] INFO [NetworkClient] metadataTimeout:299761</span><br><span class="line">[18:00:15,720] INFO [NetworkClient] select timeout:0</span><br><span class="line">[18:00:15,720] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,721] INFO [Metadata] needUpdate: false</span><br><span class="line">[18:00:15,721] INFO [NetworkClient] metadataTimeout:299747</span><br><span class="line">[18:00:15,721] INFO [NetworkClient] select timeout:30000</span><br><span class="line"></span><br><span class="line">[18:00:15,737] TRACE Received produce response from node 0 【9】</span><br><span class="line">[18:00:15,740] TRACE Produced messages to test-0 with base offset offset 11.</span><br><span class="line">[18:00:15,741] INFO [Sender] readyNodes:0</span><br><span class="line">[18:00:15,741] INFO [Metadata] needUpdate: false</span><br><span class="line">[18:00:15,741] INFO [NetworkClient] metadataTimeout:299726</span><br><span class="line">[18:00:15,741] INFO [NetworkClient] select timeout:30000</span><br><span class="line"></span><br><span class="line">[18:00:16,705] INFO start send #2 message... (kafka.examples.Producer)</span><br><span class="line">[18:00:16,706] TRACE [KafkaProducer] waitedOnMetadataMs: 0</span><br><span class="line">[18:00:16,706] TRACE Sending record ProducerRecord(topic=test, partition=null,</span><br><span class="line">  key=null, value=m2, timestamp=null) with callback null to test_2</span><br><span class="line">[18:00:16,706] TRACE Allocating a new 16384 byte message buffer for test_2</span><br><span class="line">[18:00:16,706] TRACE Waking up the sender, test_2 is full or a new batch</span><br><span class="line">[18:00:16,706] INFO sending #2 message end.. (kafka.examples.Producer)</span><br></pre></td></tr></table></figure>
<p>如图1所示，将上面日志中一些重要的时间点与事件抽取出来，具体步骤如下。</p>
<ol>
<li>第一次发送消息，唤醒发送线程，等待元数据更新完成；</li>
<li>初始化网络连接，为发送元数据请求做准备；</li>
<li>生产者发送元数据请求；</li>
<li>收到元数据响应，更新元数据对象，步骤(1)等待元数据更新完成正式结束；</li>
<li>生产者发送消息的流程接着执行，为消息指定分区，追加消息到记录收集器；</li>
<li>创建新的批记录（RecordBatch），再次唤醒发送线程；</li>
<li>从记录收集器中获取准备好的目标代理节点，并初始化网络连接，准备发送生产请求；</li>
<li>从记录收集器中再次获取准备好的节点，并获取需要发送的数据，创建生产请求；</li>
<li>发送生产请求，并等待响应结果，一批记录（实际上只有一条记录）的发送流程结束。</li>
</ol>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170507223930619" alt="7"></p>
<p>图1 生产者发送消息与更新元数据的过程</p>
<h3 id="客户端线程模型（Thread_Model）">客户端线程模型（Thread Model）</h3><p>Kafka作为一个流式数据平台，对开发者提供了三种客户端：生产者/消费者、连接器、流处理。本文着重分析这三种客户端的线程模型。</p>
<h4 id="消费者的线程模型">消费者的线程模型</h4><p>0.8版本以前的消费者客户端会创建一个基于ZK的消费者连接器，一个消费者客户端是一个Java进程，消费者可以订阅多个主题，每个主题也可以多个线程。为了让消息在多个节点被分布式地消费，提高消息处理的吞吐量，Kafka允许多个消费者订阅同一个主题，这些消费者需要满足“一个分区只能被一个消费者中的一个线程处理”的限制条件。通常，我们会将同一份相同业务处理逻辑的应用程序部署在不同机器上，并且指定一个消费组编号。当不同机器上的消费者进程启动后，所有这些消费者进程就组成了一个逻辑意义上的消费组。</p>
<p>消费组中的消费者数量是动态变化的，当有新消费者加入消费组，或者旧消费者离开消费组，都会触发基于ZK的消费组“再平衡”操作。当“再平衡”操作发生时，每个消费者都会在客户端执行分区分配算法，然后从全局的分配结果中获取属于自己的分区。它的缺点是消费者会和ZK产生频繁的交互，造成ZK集群的压力过大，并且容易产生羊群效应和脑裂等问题。</p>
<p>在0.8版本以后，Kafka重新设计了客户端，并且引入了“协调者”和“消费组管理协议”。新的消费者将“消费组管理协议”和“分区分配策略”进行了分离。协调者负责消费组的管理，而分区分配则会在消费组的一个主消费者中完成。采用这种方式，每个消费者都需要发送下面两种请求给协调者。</p>
<ul>
<li><strong>加入组请求：</strong>协调者收集消费组的所有消费者，并选举一个主消费者执行分区分配工作。</li>
<li><strong>同步组请求：</strong>主消费者完成分区分配，由协调者将分区的分配结果传播给每个消费者。</li>
</ul>
<p>新版本的消费者客户端引入了一个客户端协调者的抽象类，它的实现除了消费者的协调者，还有一个连接器的实现。</p>
<h4 id="连接器的线程模型">连接器的线程模型</h4><p>Kafka连接器的出现标准化了Kafka与各种外部存储系统的数据同步。用户开发和使用连接器就变得非常简单，只需要在配置文件中定义连接器，就可以将外部系统的数据导入Kafka或将Kafka数据导出到外部系统。如图1所示，中间部分都是Kafka连接器的内部组件，包括源连接器（Source Connector）和目标连接器（Sink Connector）。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170514212056357" alt="1"></p>
<p><strong>图1 Kafka连接器的源连接器与目标连接器</strong></p>
<p>Kafka连接器的单机模式会在一个进程内启动一个Worker以及所有的连接器和任务。分布式模式的每个进程都有一个Worker，而连接器和任务则分别运行在各个节点上。图2列举了连接器和任务在不同Worker上的四种分布方式：</p>
<ol>
<li>一个Worker，一个源任务、一个目标任务</li>
<li>一个Worker，两个源任务、两个目标任务</li>
<li>两个Worker，两个源任务、两个目标任务</li>
<li>三个Worker，两个源任务、两个目标任务</li>
</ol>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170515130348447" alt="2"></p>
<p><strong>图2 分布式模式的Kafka连接器集群</strong></p>
<p>分布式模式下，不同Worker进程之间的协调工作类似于消费者的协调。消费者通过协调者获取分配的分区，Worker也会通过协调者获取分配的连接器与任务。如图3所示，消费者客户端和Worker客户端为了加入到组管理中，分别通过客户端的协调者对象来和服务端的消费组协调者（GroupCoordinator）通信。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170526103919908" alt="8"></p>
<p><strong>图3 消费者和Worker的工作都是通过协调者分配的</strong></p>
<h4 id="流处理的线程模型">流处理的线程模型</h4><p>Kafka流处理的工作流程简单来看分成三个步骤：消费者读取输入分区的数据、流式地处理每条数据、生产者将处理结果写入输出分区，这里面步骤1也充分利用了“消费组管理协议”。Kafka流处理的输入数据源基于具有分布式分区模型的Kafka主题，它的线程模型主要由下面三个类组成：</p>
<ul>
<li><strong>流实例（KafkaStreams）：</strong>通常一个节点（一台机器）只运行一个流实例。</li>
<li><strong>流线程（StreamThread）：</strong>一个流实例可以配置多个流线程。</li>
<li><strong>流任务（StreamTask）：</strong>一个流线程可以运行多个流任务，根据输入主题的分区数确定任务数。</li>
</ul>
<p>如图4所示，输入主题有六个分区，Kafka流处理总共就会产生六个流任务。流实例可以动态扩展，流线程的个数也可以动态配置。图中一共有三个流线程，则每个流线程会有两个流任务，每个流任务都对应输入主题的一个分区。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170614142554625" alt="4"></p>
<p><strong>图4 Kafka流处理的线程模型</strong> </p>
<p>Kafka的流处理框架使用并行的线程模型处理输入主题的数据集，这种设计思路和Kafka的消费者线程模型非常类似。消费者分配到订阅主题的不同分区，流处理框架的流任务也分配到输入主题的不同分区。如图5所示，输入主题1的分区P1和输入主题2的分区P1分配给流线程1的流任务，输入主题1的分区P2和输入主题2的分区P2分配给流线程2的流任务。流处理相比消费者，还会将拓扑的计算结果写到输出主题。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170530171408924" alt="5"></p>
<p><strong>图5 消费者模型与流处理的线程模型</strong> </p>
<p>消费者和流处理的故障容错机制也是类似的。如图6所示，假设消费者2进程挂掉，它所持有的分区会被分配给同一个消费组中的消费者1，这样消费者1会分配到订阅主题的所有分区。对于流处理而言，如果流线程2挂掉了，流线程2中的流任务会分配给流线程1。即流线程1会运行两个流任务，每个流任务分配的分区仍然保持不变。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170530171420377" alt="6">·</p>
<p><strong>图6 消费者与流处理的故障容错机制</strong> </p>
<h4 id="小结">小结</h4><p>Kafka客户端抽象出来的的“组管理协议”充分运用在消费者、连接器、流处理三个使用场景中。客户端中的消费者、连接器中的工作者、流处理中的流进程都可以看做“组”的一个成员。当增加或减少组成员时，在这个协议的约束下，每个组成员都可以获取到最新的任务，从而做到无缝的任务迁移。一旦理解了“组管理协议”，对于理解Kafka的架构设计是很有帮助的。</p>
<h3 id="即席查询（Interactive_Query）">即席查询（Interactive Query）</h3><h3 id="EOS事务（Transaction）">EOS事务（Transaction）</h3><p>参考文档</p>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/Transactional+Messaging+in+Kafka</a></li>
</ul>
<p><img src alt></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka技术内幕拾遗&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;✅ 客户端元数据（Metadata）&lt;/li&gt;
&lt;li&gt;✅ 客户端线程模型（ThreadModel）&lt;/li&gt;
&lt;li&gt;即席查询（Interactive Query）&lt;/li&gt;
&lt;li&gt;EOS事务（Transaction）
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>Kafka技术内幕附录</title>
    <link href="http://github.com/zqhxuyuan/2017/12/31/Kafka-Book-Appendix/"/>
    <id>http://github.com/zqhxuyuan/2017/12/31/Kafka-Book-Appendix/</id>
    <published>2017-12-30T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.336Z</updated>
    
    <content type="html"><![CDATA[<p>Kafka技术内幕附录<br><a id="more"></a></p>
<h1 id="第11章：附录">第11章：附录</h1><h2 id="11-1_Kafka基本操作">11.1 Kafka基本操作</h2><h3 id="11-1-1_创建、修改、删除、查看主题">11.1.1 创建、修改、删除、查看主题</h3><p>我们可以手动创建主题或者让Kafka自动创建主题，手动创建主题必须指定分区数和副本因子。如果服务端开启了自动创建主题，新数据写入一个不存在的主题，服务端会自动创建这个主题。自动模式下主题的配置信息在server.properties文件中，比如分区数默认只有一个。因为分区是Kafka的最小并行单位，所以我们一般会根据集群规模设置合理的分区数，来达到客户端和服务端的负载均衡。副本因子（<code>replication-factor</code>）是分区的副本数量，每条消息会复制到多个节点上，一般设置为3个副本。假设副本数为<em>N</em>，则最多允许<em>N</em> - 1个节点宕机。 下面的实验在本机安装Kafka，假设ZK的端口为2181，Kafka的端口为9092。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 创建主题</span><br><span class="line">$ bin/kafka-topics.sh --zookeeper localhost:2181 --create \</span><br><span class="line">    --topic test --partitions 1 --replication-factor 3</span><br><span class="line"># 修改主题的分区数</span><br><span class="line">$ bin/kafka-topics.sh --zookeeper localhost:2181 --alter \</span><br><span class="line">    --topic test --partitions 2</span><br><span class="line"># 列出所有的主题</span><br><span class="line">$ bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">test</span><br><span class="line"># 查看某个主题的详细信息</span><br><span class="line">$ bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test</span><br><span class="line">Topic:test  PartitionCount:1    ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: test Partition: 0    主副本: 0   Replicas: 0 Isr: 0</span><br></pre></td></tr></table></figure>
<p>在0.8.2版本之后，Kafka提供了删除主题的功能，但是默认并不会直接将Topic数据物理删除。如果要启用物理删除（即删除主题后，日志文件也会一同删除），需要在server.properties中设置<code>delete.topic.enable=true</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test</span><br><span class="line">Topic test is marked for deletion.</span><br><span class="line">Note: This will have no impact if delete.topic.enable is not set to true.</span><br><span class="line"></span><br><span class="line">$ bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br><span class="line">test - marked for deletion</span><br></pre></td></tr></table></figure>
<p>管理员创建好主题后，主题会被生产者和消费者使用。注意下面的实验中，新版本的生产者和消费者都是使用Broker地址连接Kafka集群，旧版本的消费者则使用ZK地址连接Kafka集群。</p>
<h3 id="11-1-2_生产者和消费者">11.1.2 生产者和消费者</h3><p>在终端控制台模拟生产消息和消费消息，每个控制台的消费者都会被分配唯一的消费组：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 生产者</span><br><span class="line">$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"># 旧消费者（控制台）</span><br><span class="line">$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line"># 新消费者（控制台）</span><br><span class="line">$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \</span><br><span class="line">  --new-consumer --topic test --from-beginning</span><br></pre></td></tr></table></figure>
<p>执行查看消费组列表的操作，可以列出当前活动的消费组，默认控制台的消费组是<code>console-consumer</code>加上一个随机数。上面由于分别启动了两个版本的消费者，所以对应了两个消费组。当然，也可以在控制台通过其他参数来指定消费组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 查看使用旧消费者的消费组列表</span><br><span class="line">$ bin/kafka-consumer-groups.sh --list --zookeeper localhost:2181</span><br><span class="line">console-consumer-36296</span><br><span class="line"># 查询使用新消费者的消费组列表</span><br><span class="line">$ bin/kafka-consumer-groups.sh --list --bootstrap-server localhost:9092</span><br><span class="line">console-consumer-89231</span><br></pre></td></tr></table></figure>
<p>查看消费组对某个主题的消费状态，需要指定主题和消费组，这会打印出主题的所有分区、日志的大小、所属的消费者等。<br>采用新消费者方式的<code>Owner</code>为<code>none</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-consumer-offset-checker.sh --zookeeper localhost:2181 \</span><br><span class="line">  --topic test --group console-consumer-36296</span><br><span class="line">Group         Topic Pid Offset logSize Lag Owner</span><br><span class="line">console-36296 test  0   2      2       0   dp0652-f94edaea-0</span><br><span class="line">console-36296 test  1   1      1       0   dp0652-f94edaea-0</span><br><span class="line">console-36296 test  2   2      2       0   dp0652-f94edaea-0</span><br><span class="line"></span><br><span class="line">$ bin/kafka-consumer-offset-checker.sh --zookeeper localhost:2181 \</span><br><span class="line">  --topic test --group console-consumer-89231</span><br><span class="line">Group         Topic Pid Offset logSize Lag Owner</span><br><span class="line">console-89231 test  0   2      2       0   none</span><br><span class="line">console-89231 test  1   1      1       0   none</span><br><span class="line">console-89231 test  2   2      2       0   none</span><br></pre></td></tr></table></figure>
<h3 id="11-1-3_扩展集群">11.1.3 扩展集群</h3><p>要向已有的Kafka集群添加新节点，我们只需要保证<code>broker.id</code>编号是唯一的，即可启动Kafka服务。但是新节点不会自动地分配到分区，除非在新加节点之后，新创建了主题。因此，通常我们希望在新添加节点后，能够将旧节点上的分区迁移一部分到新节点上，从而达到负载均衡的目的。迁移分区，实际上是将新节点作为分区的备份副本，当新节点完全复制了一个分区的所有数据，并且加入分区的ISR集合后，旧节点已有的一个副本就会被删除。在整个迁移过程中，分区的副本数保持不变，只不过分区的所属节点从旧节点迁移到了新节点。Kafka提供了分区重新分配（<code>partition reassignment tool</code>）的工具来在不同节点之间移动分区，但该工具并不会自动学习Kafka集群的数据分布来移动分区达到数据的均匀分布，管理员需要手动指定哪些主题或分区需要移动。使用该工具需要执行下面的3个步骤。</p>
<ol>
<li><code>--generate</code>：给定主题和需要移动到的目标节点，生成候选的分区分配计划。</li>
<li><code>--execute</code>：根据上一步的分区分配计划或者手动定义的计划执行数据迁移的任务。</li>
<li><code>--verify</code>：验证上一步执行任务涉及的所有分区的分配状态是否已经完成。</li>
</ol>
<p>下面的示例会将<code>foo1</code>和<code>foo2</code>主题的所有分区全部移动到新的节点5、6上，最后这两个主题的所有分区都只在5、6节点上。第一步生成计划时，会列举出当前主题所有分区目前所在的节点，如果执行失败，管理员还可以进行回滚操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># [1] 生成分区分配计划，指定需要移动的主题和需要移动到的目标节点</span><br><span class="line">$ cat topics-to-move.json</span><br><span class="line">&#123;&quot;topics&quot;: [&#123;&quot;topic&quot;: &quot;foo1&quot;&#125;, &#123;&quot;topic&quot;: &quot;foo2&quot;&#125;], &quot;version&quot;:1&#125;</span><br><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 </span><br><span class="line">    --topics-to-move-json-file topics-to-move.json \</span><br><span class="line">    --broker-list &quot;5,6&quot; --generate</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,4]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[5,6]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># [2] 执行分区重新分配的任务</span><br><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 \</span><br><span class="line">  --reassignment-json-file expand-cluster-reassignment.json --execute</span><br><span class="line"></span><br><span class="line"># [3] 验证分区重新分配的进度</span><br><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 \</span><br><span class="line">    --reassignment-json-file expand-cluster-reassignment.json --verify</span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [foo1,0] completed successfully</span><br><span class="line">Reassignment of partition [foo1,1] is in progress</span><br><span class="line">Reassignment of partition [foo1,2] is in progress</span><br><span class="line">Reassignment of partition [foo2,0] completed successfully</span><br><span class="line">Reassignment of partition [foo2,1] completed successfully</span><br><span class="line">Reassignment of partition [foo2,2] completed successfully</span><br></pre></td></tr></table></figure>
<p>除了给定主题，由工具生成所有分区的执行计划，我们也可以直接指定主题需要迁移的分区（当然在<code>execute</code>阶段，工具还是会列出指定主题分区当前所在的节点）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ cat custom-reassignment.json</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;</span><br><span class="line">]&#125;</span><br><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 \</span><br><span class="line">  --reassignment-json-file custom-reassignment.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2]&#125;,</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,4]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo1&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6]&#125;,</span><br><span class="line">  &#123;&quot;topic&quot;:&quot;foo2&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>除此之外，迁移工具还适用于给分区增加副本数。增加副本数是复制（而不是移动）已有的分区到其他节点，不管使用手动还是自动生成的分配计划，都要包含分区之前所在的节点。下面的示例中，<code>foo</code>主题的分区0只有一个副本是存在节点5上，增加到3个副本后，存在的节点有5、6、7这3个节点。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ cat increase-replication-factor.json</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6,7]&#125;]</span><br><span class="line">&#125;</span><br><span class="line">$ bin/kafka-reassign-partitions.sh --zookeeper localhost:2181 \</span><br><span class="line">    --reassignment-json-file increase-replication-factor.json --execute</span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[&#123;</span><br><span class="line">    &quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Save this to use as the --reassignment-json-file option during rollback</span><br><span class="line">Successfully started reassignment of partitions</span><br><span class="line">&#123;&quot;version&quot;:1, &quot;partitions&quot;:[</span><br><span class="line">    &#123;&quot;topic&quot;:&quot;foo&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[5,6,7]&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 副本数为一个时的主题信息</span><br><span class="line">$ bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe</span><br><span class="line">Topic:foo PartitionCount:1  ReplicationFactor:1 Configs:</span><br><span class="line">    Topic: foo  Partition: 0  主副本: 5 Replicas: 5 Isr: 5</span><br><span class="line"></span><br><span class="line"># 增加副本数后的主题信息</span><br><span class="line">$ bin/kafka-topics.sh --zookeeper localhost:2181 --topic foo --describe</span><br><span class="line">Topic:foo PartitionCount:1  ReplicationFactor:3 Configs:</span><br><span class="line">    Topic: foo  Partition: 0  主副本: 5 Replicas: 5,6,7 Isr: 5,6,7</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意：</strong>修改主题的分区数可以直接采用修改主题的方式，但是修改分区的副本数涉及数据的复制，需要用到上面的分区迁移工具。</p>
</blockquote>
<h2 id="11-2_安全机制（Security）">11.2 安全机制（<code>Security</code>）</h2><p>Kafka的安全机制主要分为下面两个部分：</p>
<ul>
<li>身份认证（<code>Authentication</code>）：对客户端与服务器的连接进行身份认证。Kafka目前支持<code>SSL</code>、<code>SASL/Kerberos</code>、<code>SASL/PLAIN</code>三种认证机制。</li>
<li>权限控制（<code>Authorization</code>）：对消息级别的访问控制列表（ACL）权限控制。</li>
</ul>
<p>下面以<code>SASL/PLAIN</code>的身份认证为例，服务端需要先修改下面三个配置文件，然后启动服务端：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ vi config/server.properties</span><br><span class="line">listeners=SASL_PLAINTEXT://localhost:9092</span><br><span class="line">security.inter.broker.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism.inter.broker.protocol=PLAIN</span><br><span class="line">sasl.enabled.mechanisms=PLAIN</span><br><span class="line"></span><br><span class="line">$ vi config/jaas.conf</span><br><span class="line">KafkaServer &#123;</span><br><span class="line">  org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">  username=&quot;admin&quot;</span><br><span class="line">  password=&quot;admin&quot;</span><br><span class="line">  user_admin=&quot;admin&quot;</span><br><span class="line">&#125;;</span><br><span class="line">KafkaClient &#123;</span><br><span class="line">  org.apache.kafka.common.security.plain.PlainLoginModule required</span><br><span class="line">  username=&quot;admin&quot;</span><br><span class="line">  password=&quot;admin&quot;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">$ vi bin/kafka-run-class.sh</span><br><span class="line">KAFKA_SASL_OPTS=&quot;-Djava.security.auth.login.config=../config/jaas.conf&quot;</span><br><span class="line">KAFKA_OPTS=&quot;$KAFKA_SASL_OPTS $KAFKA_OPTS&quot;</span><br></pre></td></tr></table></figure>
<p>客户端也需要添加两个配置项，下面以控制台的生产者和消费者为例，说明客户端的身份认证：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ vi config/producer.properties</span><br><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism=PLAIN</span><br><span class="line"></span><br><span class="line">$ vi config/consumer.properties</span><br><span class="line">security.protocol=SASL_PLAINTEXT</span><br><span class="line">sasl.mechanism=PLAIN</span><br><span class="line"></span><br><span class="line">$ bin/kafka-console-producer.sh --broker-list localhost:9092 \</span><br><span class="line">  --topic test-security --producer.config config/producer.properties</span><br><span class="line">hello</span><br><span class="line"></span><br><span class="line">$ bin/kafka-console-consumer.sh --new-consumer \</span><br><span class="line">  --bootstrap-server localhost:9092 --topic test-security \</span><br><span class="line">  --from-beginning --consumer.config config/consumer.properties</span><br><span class="line">hello</span><br></pre></td></tr></table></figure>
<p>如果使用代码，还需要设置<code>java.security.auth.login.config</code>为系统的环境变量配置。下面是生产者使用身份认证的示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 设置客户端登陆的身份认证机制，指定配置文件</span></span><br><span class="line">    System.setProperty(<span class="string">"java.security.auth.login.config"</span>, </span><br><span class="line">      <span class="string">"/Users/zhengqh/.../resources/kafka_client_jaas.conf"</span>);</span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">    props.put(<span class="string">"client.id"</span>, <span class="string">"DemoProducer"</span>);</span><br><span class="line">    props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.IntegerSerializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.serializer"</span>,  <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">    props.put(<span class="string">"security.protocol"</span>, <span class="string">"SASL_PLAINTEXT"</span>); <span class="comment">// 安全协议类型</span></span><br><span class="line">    props.put(<span class="string">"sasl.mechanism"</span>, <span class="string">"PLAIN"</span>); <span class="comment">// 安全机制</span></span><br><span class="line">    KafkaProducer&lt;Integer, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">    ProducerRecord&lt;Integer, String&gt; record1 = <span class="keyword">new</span> ProducerRecord&lt;Integer, String&gt;(<span class="string">"test-security"</span>, <span class="number">1</span>, <span class="string">"one"</span>);</span><br><span class="line">    producer.send(record1, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata,Exception e)</span></span>&#123;</span><br><span class="line">        System.out.println(recordMetadata);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    producer.flush();</span><br><span class="line">    producer.close();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面我们只分析了<code>SASL_PLAINTEXT</code>安全协议的例子，Kafka支持的其他安全协议以及权限认证可以参考官方的文档。另外，服务端与ZooKeeper以及服务端之间也都有安全机制和身份认证机制，这里就不再深入分析。</p>
<h2 id="11-3_Kafka配置">11.3 Kafka配置</h2><p>Kafka官方文档中针对服务端（代理节点）、主题、生产者、消费者都有完整的配置说明，下面列举了比较重要的一些配置项。</p>
<h3 id="11-3-1_服务端的配置项">11.3.1 服务端的配置项</h3><p>服务端的配置项参见表1。</p>
<p>表1 服务端配置信息</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>broker.id</code></td>
<td>Kafka服务器的编号，同一个集群不同节点的编号应该唯一</td>
</tr>
<tr>
<td><code>zookeeper.connect</code></td>
<td>连接ZooKeeper的地址，不同Kafka集群如果连接到同一个ZooKeeper，应该使用不同的chroot路径</td>
</tr>
<tr>
<td><code>auto.create.topics.enable</code></td>
<td>自动创建主题，默认为<code>true</code></td>
</tr>
<tr>
<td><code>auto.leader.rebalance.enable</code></td>
<td>开启主副本自动平衡，当节点宕机后，会影响这个节点上的主副本转移到其他节点，宕机的节点重启后只能作为备份副本，如果开启平衡，则会将主副本转移到原节点</td>
</tr>
<tr>
<td><code>delete.topic.enable</code></td>
<td>自动删除主题，默认为<code>false</code>，通过<code>delete</code>命令删除主题，并不会物理删除，只有开启该选项才会真正删除主题的日志文件</td>
</tr>
<tr>
<td><code>log.dirs</code></td>
<td>日志文件的目录，可以指定多个目录。默认是/tmp/kafka-logs</td>
</tr>
<tr>
<td><code>log.flush.interval.messages</code></td>
<td>在消息集刷写到磁盘之前需要收集的消息数量，默认值为<code>Long.MAX</code></td>
</tr>
<tr>
<td><code>log.flush.scheduler.interval.ms</code></td>
<td>日志刷新线程过久，检查一次是否有日志文件需要刷写到磁盘，默认值为<code>Long.MAX</code>。</td>
</tr>
<tr>
<td><code>log.retention.bytes</code></td>
<td>日志文件超过最大大小时删除旧数据，默认值为<code>-1</code>，即永不会删除</td>
</tr>
<tr>
<td><code>log.retention.hours</code></td>
<td>日志文件保留的时间，默认为168小时，即7天</td>
</tr>
<tr>
<td><code>log.segment.bytes</code></td>
<td>单个日志文件片段的最大值，默认为1 GB，日志超过1 GB后会刷写到磁盘</td>
</tr>
<tr>
<td><code>message.max.bytes</code></td>
<td>服务端接收的消息最大值，默认为1 MB，即一批消息最大不能超过1 MB</td>
</tr>
<tr>
<td><code>min.insync.replicas</code></td>
<td>当生产者的应答策略设置为<code>all</code>时，写操作的数量必须满足该值才算成功。默认值为<code>1</code>，表示只要写到一个节点就算成功</td>
</tr>
<tr>
<td><code>offsets.commit.required.acks</code></td>
<td>消费者提交偏移量和生产者写消息的行为类似，用应答来表示写操作是否成功，默认值为<code>-1</code></td>
</tr>
<tr>
<td><code>offsets.commit.timeout.ms</code></td>
<td>类似于生产者的请求超时时间，写请求会被延迟，默认5秒</td>
</tr>
<tr>
<td><code>offsets.topic.num.partitions</code></td>
<td>消费者提交偏移量内部主题的分区数量，默认为50个</td>
</tr>
<tr>
<td><code>offsets.topic.replication.factor</code></td>
<td>消费者提交偏移量内部主题的副本数量，默认为3个</td>
</tr>
<tr>
<td><code>replica.fetch.min.bytes</code></td>
<td>每个拉取请求最少要拉取的字节数量，默认为1byte。</td>
</tr>
<tr>
<td><code>replica.fetch.wait.max.ms</code></td>
<td>每个拉取请求的最大等待时间，默认为500毫秒</td>
</tr>
<tr>
<td><code>replica.lag.time.max.ms</code></td>
<td>备份副本在指定时间内都没有发送拉取请求，或者在这个时间内仍然没有赶上主副本，它将会被从ISR中移除，默认10秒</td>
</tr>
<tr>
<td><code>request.timeout.ms</code></td>
<td>客户端从发送请求到接收响应的超时时间，默认30秒</td>
</tr>
<tr>
<td><code>zookeeper.session.timeout.ms</code></td>
<td>ZooKeeper会话的超时时间，默认6秒</td>
</tr>
<tr>
<td><code>default.replication.factor</code></td>
<td>自动创建的主题的副本数，默认为1个</td>
</tr>
<tr>
<td><code>log.cleaner.delete.retention.ms</code></td>
<td>被删除的记录保存的时间，默认为1天</td>
</tr>
<tr>
<td><code>log.cleaner.enable</code></td>
<td>是否开启日志清理线程，当清理策略为<code>compact</code>时，建议开启</td>
</tr>
<tr>
<td><code>log.index.interval.bytes</code></td>
<td>添加1条索引到日志文件的间隔，默认为4096条</td>
</tr>
<tr>
<td><code>log.index.size.max.bytes</code></td>
<td>索引文件的最大大小，默认为10 MB</td>
</tr>
<tr>
<td><code>num.partitions</code></td>
<td>每个主题的分区数量，默认为1个</td>
</tr>
<tr>
<td><code>replica.fetch.max.bytes</code></td>
<td>拉取请求中每个分区的消息最大值，默认为1 MB</td>
</tr>
<tr>
<td><code>replica.fetch.response.max.bytes</code></td>
<td>整个拉取请求的消息最大值，默认为10 MB</td>
</tr>
</tbody>
</table>
<p>主题级别的一些配置和服务端级别的设置类似，比如<code>flush.messages</code>类似<code>log.flush.interval.messages</code>，表示刷写到磁盘的消息数量；<code>flush.ms</code>类似<code>log.flush.scheduler.interval.ms</code>，表示刷写到磁盘的时间间隔；<code>max.message.bytes</code>类似<code>message.max.bytes</code>，表示服务端接收的单条消息大小。</p>
<h3 id="11-3-2_生产者的配置项">11.3.2 生产者的配置项</h3><p>生产者配置信息参见表2。</p>
<p>表2 生产者配置信息</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>bootstrap.servers</code></td>
<td>生产者客户端连接Kafka集群的地址和端口，多个节点用逗号分隔</td>
</tr>
<tr>
<td><code>acks</code></td>
<td>生产者请求要求主副本收到的应答数量满足后，写请求才算成功。<code>0</code>表示记录添加到网络缓冲区后就认为已经发送，生产者不会等待服务端的任何应答；<code>1</code>表示主副本会将记录到本地日志文件，但不会等待任何备份副本的应答；<code>-1</code>或<code>all</code>表示主副本必须等待ISR中所有副本都返回应答给它</td>
</tr>
<tr>
<td><code>retries</code></td>
<td>发送时出现短暂的错误或者收到错误码，客户端会重新发送记录。如果<code>max.in.flight.requests.per.connection</code>没有设置为<code>1</code>，在异常重试时，服务端收到的记录可能是乱序的</td>
</tr>
<tr>
<td><code>buffer.memory</code></td>
<td>生产者发送记录给服务端在客户端的缓冲区，默认为32 MB</td>
</tr>
<tr>
<td><code>batch.size</code></td>
<td>当多条记录发送到同一个分区，生产者会尝试将一批记录分成更少的请求，来提高客户端和服务端的性能，默认每一个Batch的大小为16 KB。如果一条记录就超过了16 KB，则这条记录不会和其他记录组成Batch。Batch太小会减小吞吐量，Batch太大会占用太多的内存</td>
</tr>
<tr>
<td><code>max.request.size</code></td>
<td>一个请求的最大值，实际上也是记录的最大值。注意服务端关于记录的最大值（Broker的<code>message.max.bytes</code>，或者Topic的<code>max.message.bytes</code>）可能和它不同（实际上默认值都是1 MB）。这个配置项会限制生产者一个请求中Batch的记录数，防止发送过大的请求</td>
</tr>
<tr>
<td><code>partitioner.class</code></td>
<td>消息的分区语义，对消息进行路由到指定的分区，实现分区接口</td>
</tr>
<tr>
<td><code>request.timeout.ms</code></td>
<td>客户端等待一个请求的响应的最长时间，超时后客户端会重新发送或失败</td>
</tr>
<tr>
<td><code>timeout.ms</code></td>
<td>服务端等待备份的应答来达到生产者设置的<code>ack</code>的最长时间，超时后不满足失败</td>
</tr>
</tbody>
</table>
<h3 id="11-3-3_新消费者的配置项">11.3.3 新消费者的配置项</h3><p>新消费者的配置信息参见表3。</p>
<p>表3 新消费者的配置信息</p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>fetch.min.bytes</code></td>
<td>拉取请求要求服务端返回的数据最小值，如果服务端的数据量还不够，客户端的请求会一直等待，直到服务端收集到足够的数据才会返回响应给客户端。默认值为1个字节，表示服务端处理的拉取请求数据量只要达到1个字节就立即收到响应，或者因为在等待数据的到达一直没有满足最小值时而超时后，拉取请求也会结束。将该值设置大一点，可以牺牲一些延迟来获取服务端更高的吞吐量</td>
</tr>
<tr>
<td><code>fetch.max.bytes</code></td>
<td>服务端对一个拉取请求返回数据的最大值，默认值为50 MB</td>
</tr>
<tr>
<td><code>fetch.max.wait.ms</code></td>
<td>在没有收集到满足<code>fetch.min.bytes</code>大小的数据之前，服务端对拉取请求的响应会阻塞直到超时，默认500毫秒</td>
</tr>
<tr>
<td><code>group.id</code></td>
<td>消费者所述的唯一消费组名称，在使用基于Kafka的偏移量管理策略，或者使用消费组管理协议的订阅方法时，必须指定消费组名称</td>
</tr>
<tr>
<td><code>heartbeat.interval.ms</code></td>
<td>使用消费组管理协议时消费者和协调者的心跳间隔，心跳用来确保消费者的会话保持活动的状态，以及当有新消费者加入或消费者离开时可以更容易地进行平衡，该选项必须比<code>session.timeout.ms</code>小，通常设置为不大于它的1/3。默认值为3秒，我们可以将心跳值设置得更低，来更好地控制平衡：需要平衡时，心跳间隔越短就能越快地感知到</td>
</tr>
<tr>
<td><code>max.partition.fetch.bytes</code></td>
<td>服务端返回的数据中每个分区的最大值，默认值为1 MB</td>
</tr>
<tr>
<td><code>session.timeout.ms</code></td>
<td>使用消费组管理协议检测到消费者失败的最大时间，消费者定时地向Broker发送心跳表示处于存活状态。服务端的Broker会记录消费者的心跳时间，如果在指定的会话时间内都没有收到消费者的心跳，Broker会将其从消费组中移除并启动一次平衡</td>
</tr>
<tr>
<td><code>auto.offset.reset</code></td>
<td>Kafka中没有分区的初始偏移量，消费者任何定位分区位置。<code>earliest</code>表示重置到最旧的位置；<code>latest</code>表示重置到最新的位置，默认值为<code>latest</code></td>
</tr>
<tr>
<td><code>enable.auto.commit</code></td>
<td>消费者的偏移量是否会在后台定时地提交，默认值为<code>true</code></td>
</tr>
<tr>
<td><code>auto.commit.interval.ms</code></td>
<td>消费者自动提交偏移量的时间间隔，默认值为5秒</td>
</tr>
<tr>
<td><code>max.poll.interval.ms</code></td>
<td>使用消费组管理协议时，在调用<code>poll()</code>之间的最大延迟，它设置了消费者在下一次拉取更多记录之前允许的最长停顿时间。如果超时后消费者仍然没有调用<code>poll()</code>，那么消费者就会被认为失败了，就会启动消费组的平衡，默认值为5秒</td>
</tr>
<tr>
<td><code>max.poll.records</code></td>
<td>在一次<code>poll()</code>调用中允许返回的最大记录数，默认值为500条</td>
</tr>
<tr>
<td><code>partition.assignment.strategy</code></td>
<td>使用消费者管理协议时，消费者实例之间用来进行分区分配的策略，默认值为<code>RangeAssignor</code></td>
</tr>
</tbody>
</table>
<h2 id="11-4_Kafka其他操作实验">11.4 Kafka其他操作实验</h2><h3 id="11-4-1_ZooKeeper连接配置">11.4.1 ZooKeeper连接配置</h3><p>Kafka的ZooKeeper配置和命令行的ZooKeeper地址不一致导致连接不上ZooKeeper，下面是server.properties的ZooKeeper连接配置，指定了Kafka在ZooKeeper中的根节点是<code>/kafka</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0</span><br><span class="line">#listeners=PLAINTEXT://:9092</span><br><span class="line">zookeeper.connect=localhost:2181/kafka</span><br><span class="line">log.dirs=/tmp/kafka-logs</span><br></pre></td></tr></table></figure>
<p>如果命令行中连接的ZooKeeper地址没有加上<code>/kafka</code>，创建主题会报错可用的节点为0，加上<code>/kafka</code>后可以成功创建主题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --create --zookeeper localhost:2181 \</span><br><span class="line">--replication-factor 1 --partitions 1 --topic test</span><br><span class="line">Error while executing topic command : RF: 1 larger than available brokers: 0</span><br><span class="line">ERROR AdminOperationException: RF: 1 larger than available brokers: 0</span><br><span class="line">    at kafka.admin.AdminUtils$.assignReplicasToBrokers(AdminUtils.scala:117)</span><br><span class="line">    at kafka.admin.AdminUtils$.createTopic(AdminUtils.scala:403)</span><br><span class="line">    at kafka.admin.TopicCommand$.createTopic(TopicCommand.scala:110)</span><br><span class="line">    at kafka.admin.TopicCommand$.main(TopicCommand.scala:61)</span><br><span class="line">    at kafka.admin.TopicCommand.main(TopicCommand.scala)</span><br><span class="line"></span><br><span class="line">$ bin/kafka-topics.sh --create --zookeeper localhost:2181/kafka \</span><br><span class="line">--replication-factor 1 --partitions 1 --topic test</span><br><span class="line">Created topic &quot;test&quot;.</span><br><span class="line">$ bin/kafka-topics.sh --list --zookeeper localhost:2181/kafka</span><br><span class="line">test</span><br></pre></td></tr></table></figure>
<p>生产者连接的是Kafka代理节点的地址，和ZooKeeper没有关系。而旧消费者连接的是ZooKeeper，所以也要加上<code>/kafka</code>才能读取到消息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line">this is a message</span><br><span class="line">this is another message</span><br><span class="line">$ bin/kafka-console-consumer.sh --zookeeper localhost:2181/kafka \</span><br><span class="line">  --topic test --from-beginning</span><br><span class="line">this is a message</span><br><span class="line">this is another message</span><br></pre></td></tr></table></figure>
<p>上面的实验通过在Kafka服务端的配置文件中设置ZooKeeper根节点，可以在一个ZooKeeper中区分多个Kafka集群。下面的实验就利用了该功能。</p>
<h3 id="11-4-2_MirrorMaker演示消费者线程数量">11.4.2 <code>MirrorMaker</code>演示消费者线程数量</h3><p>单机模拟多个Kafka集群，每个集群各自只有一台服务器。不同Kafka集群的<code>zookeeper.connect</code>配置项分别是：<code>localhost:2181/kafka</code>和<code>localhost:2181/kafka_dc</code>（这两个集群叫作kafka集群、kafka_dc集群）。查看ZooKeeper的节点，因为是不同的Kafka集群，所以代理节点的编号可以一样（当然由于在本机模拟多个集群，端口号不能一样）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 0] ls /</span><br><span class="line">[kafka_dc, zookeeper, kafka]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /kafka/brokers/ids</span><br><span class="line">[0]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 4] ls /kafka_dc/brokers/ids</span><br><span class="line">[0]</span><br></pre></td></tr></table></figure>
<p>在Kafka集群创建分区数只有一个的主题<code>test</code>，然后启动<code>MirrorMaker</code>，设置消费者线程数量为3：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-mirror-maker.sh --num.streams 3 \</span><br><span class="line">  --consumer.config config/consumer_source.properties \</span><br><span class="line">  --producer.config config/producer_dest.properties --whitelist test</span><br></pre></td></tr></table></figure>
<p>ZooKeeper中消费者的数量也有3个，但是因为分区只有一个，消费者<code>Owner</code>也只有一个：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181] ls /kafka/consumers/mm/ids</span><br><span class="line">[mm_zqhmac-dd52d0ea, mm_zqhmac-60c27086, mm_zqhmac-d0eece39]</span><br><span class="line">[zk: localhost:2181] get /kafka/consumers/mm/owners/test/0</span><br><span class="line">mm_zqhmac-60c27086-0</span><br><span class="line">[zk: localhost:2181] get /kafka/consumers/mm/ids/mm_zqhmac-60c27086</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;subscription&quot;:&#123;&quot;test&quot;:1&#125;,&quot;pattern&quot;:&quot;white_list&quot;&#125;</span><br></pre></td></tr></table></figure>
<p>因为消费者数量比分区的数量要多，所以有些消费者会分配不到分区。在执行<code>MirrorMaker</code>程序时，控制台会提示有两个消费者线程没有分配到分区。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">WARN No broker partitions consumed by consumer thread </span><br><span class="line">  mm_zqhmac-d0eece39-0 for topic test (kafka.consumer.RangeAssignor)</span><br><span class="line">WARN No broker partitions consumed by consumer thread </span><br><span class="line">  mm_zqhmac-dd52d0ea-0 for topic test (kafka.consumer.RangeAssignor)</span><br></pre></td></tr></table></figure>
<p>通过控制台的消费者检查<code>Mirror</code>（kafka_dc）目标集群是否有数据写入，可以看到虽然我们没有在kafka_dc集群创建<code>test</code>主题，但是通过镜像工具，源集群的数据会复制到目标集群。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --list --zookeeper localhost:2181/kafka_dc</span><br><span class="line">test</span><br><span class="line">$ bin/kafka-console-consumer.sh --zookeeper localhost:2181/kafka_dc \</span><br><span class="line">  --topic test --from-beginning</span><br><span class="line">this is third message</span><br><span class="line">this is fouth message</span><br></pre></td></tr></table></figure>
<p>检查消费组所有消费者的消费情况，也只有一个消费者：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker \</span><br><span class="line">    --group mm --zookeeper localhost:2181/kafka --topic test</span><br><span class="line">Group  Topic  Pid   Offset  logSize   Lag    Owner</span><br><span class="line">mm     test   0     4       4         0      mm_zqhmac-60c27086-0</span><br></pre></td></tr></table></figure>
<h3 id="11-4-3_生产者和消费者性能测试">11.4.3 生产者和消费者性能测试</h3><p>Kafka提供了一些工具类，包括生产者和消费者的性能测试，端到端的延迟。下面的实验是在一个小型的Kafka集群上，并且测试主题<code>test-rep-3</code>有3个副本、6个分区：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ zookeeper=192.168.6.55:2181,192.168.6.56:2181,192.168.6.57:2181/kafka010</span><br><span class="line">$ kafka=192.168.6.52:9092,192.168.6.52:9093,192.168.6.53:9094,192.168.6.53:9095</span><br><span class="line">$ bin/kafka-topics.sh --zookeeper $zookeeper --create \</span><br><span class="line">    --topic test-rep-3 --partitions 6 --replication-factor 3</span><br><span class="line">$ bin/kafka-topics.sh --describe --zookeeper $zookeeper --topic test-rep-3</span><br><span class="line">Topic:test-rep-3    PartitionCount:6    ReplicationFactor:3 Configs:</span><br><span class="line">Topic: test-rep-3   Partition: 0    主副本: 3   Replicas: 3,2,0 Isr: 3,2,0</span><br><span class="line">Topic: test-rep-3   Partition: 1    主副本: 0   Replicas: 0,3,1 Isr: 0,3,1</span><br><span class="line">Topic: test-rep-3   Partition: 2    主副本: 1   Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">Topic: test-rep-3   Partition: 3    主副本: 2   Replicas: 2,1,3 Isr: 2,1,3</span><br><span class="line">Topic: test-rep-3   Partition: 4    主副本: 3   Replicas: 3,0,1 Isr: 3,0,1</span><br><span class="line">Topic: test-rep-3   Partition: 5    主副本: 0   Replicas: 0,1,2 Isr: 0,1,2</span><br></pre></td></tr></table></figure>
<p>接着对生产者和消费者进行性能测试（笔者的测试环境还有其他服务，所以测试结果并不是很理想，如果要对Kafka进行压测，最好模拟线上的机器配置）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#####生产者性能测试#####</span><br><span class="line">$ bin/kafka-run-class.sh org.apache.kafka.tools.ProducerPerformance \</span><br><span class="line">    --topic test-rep-3 --num-records 50000000 --record-size 100 \</span><br><span class="line">    --throughput -1 --producer-props acks=1 buffer.memory=67108864 \</span><br><span class="line">    batch.size=8196 bootstrap.servers=$kafka</span><br><span class="line">## 第一次在集群内测试</span><br><span class="line">50000000 records sent, 749906.261717 records/sec (71.52 MB/sec), </span><br><span class="line">50.73 ms avg latency, 1356.00 ms max latency, </span><br><span class="line">2 ms 50th, 266 ms 95th, 603 ms 99th, 1327 ms 99.9th.</span><br><span class="line">## 第二次在集群内测试</span><br><span class="line">50000000 records sent, 84956.858907 records/sec (8.10 MB/sec), </span><br><span class="line">5781.48 ms avg latency, 17968.00 ms max latency, </span><br><span class="line">9872 ms 50th, 16705 ms 95th, 17492 ms 99th, 17909 ms 99.9th.</span><br><span class="line">## 第三次在集群外测试</span><br><span class="line">50000000 records sent, 42554.459069 records/sec (4.06 MB/sec), </span><br><span class="line">11455.58 ms avg latency, 51425.00 ms max latency, </span><br><span class="line">82 ms 50th, 29290 ms 95th, 30192 ms 99th, 36732 ms 99.9th.</span><br><span class="line"></span><br><span class="line">#####消费者性能测试#####</span><br><span class="line">$ bin/kafka-consumer-perf-test.sh --zookeeper $zookeeper \</span><br><span class="line">    --messages 50000000 --topic test-rep-3 --threads 1</span><br><span class="line">## 第一次在集群内测试</span><br><span class="line">start, end, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec</span><br><span class="line">17:00:32:149, 17:00:56:811, 4767.4932, 193.3133, 49990789, 2027037.1016</span><br><span class="line">## 第二次在集群内测试</span><br><span class="line">17:39:11:883, 17:44:03:117, 4768.3716, 16.3730, 50000000, 171683.2513</span><br><span class="line"></span><br><span class="line"># 消费者性能测试（多线程）</span><br><span class="line">$ bin/kafka-consumer-perf-test.sh --zookeeper $zookeeper \</span><br><span class="line">    --messages 50000000 --topic test-rep-3 --threads 6</span><br></pre></td></tr></table></figure>
<p>在生产者的测试过程中，有些分区由于网络或者其他原因会对ISR进行调整，日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">INFO Partition [test-rep-3,1] on broker 0: Shrinking ISR from 0,1,3 to 0,1</span><br><span class="line">INFO Partition [test-rep-3,5] on broker 0: Expanding ISR from 0,1 to 0,1,2</span><br><span class="line">INFO Partition [test-rep-3,1] on broker 0: Expanding ISR from 0,1 to 0,1,3</span><br></pre></td></tr></table></figure>
<p>这时如果查看主题信息，会发现主题中每个分区的ISR和最开始创建的时候不同。不过等生产者测试运行完毕，再过一段时间，就会恢复到刚开始的ISR，这是因为默认开启了主副本自动迁移：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ bin/kafka-topics.sh --describe --zookeeper $zookeeper --topic test-rep-3</span><br><span class="line">Topic:test-rep-3    PartitionCount:6    ReplicationFactor:3 Configs:</span><br><span class="line">Topic:test-rep-3   Partition: 0    主副本: 3   Replicas: 3,2,0 Isr: 3,2</span><br><span class="line">Topic:test-rep-3   Partition: 1    主副本: 0   Replicas: 0,3,1 Isr: 0,1,3</span><br><span class="line">Topic:test-rep-3   Partition: 2    主副本: 1   Replicas: 1,0,2 Isr: 1,0</span><br><span class="line">Topic:test-rep-3   Partition: 3    主副本: 2   Replicas: 2,1,3 Isr: 2,3</span><br><span class="line">Topic:test-rep-3   Partition: 4    主副本: 3   Replicas: 3,0,1 Isr: 3</span><br><span class="line">Topic:test-rep-3   Partition: 5    主副本: 0   Replicas: 0,1,2 Isr: 0,1,2</span><br></pre></td></tr></table></figure>
<h2 id="11-5_第三方工具">11.5 第三方工具</h2><h3 id="11-5-1_Confluent_Platform">11.5.1 Confluent Platform</h3><p>Confluent的各个组件和默认端口如下：</p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Default Port</th>
</tr>
</thead>
<tbody>
<tr>
<td>Zookeeper</td>
<td>2181</td>
</tr>
<tr>
<td>Apache Kafka brokers (plain text)</td>
<td>9092</td>
</tr>
<tr>
<td>Schema Registry REST API</td>
<td>8081</td>
</tr>
<tr>
<td>REST Proxy</td>
<td>8082</td>
</tr>
<tr>
<td>Kafka Connect REST API</td>
<td>8083</td>
</tr>
<tr>
<td>Confluent Control Center</td>
<td>9021</td>
</tr>
</tbody>
</table>
<p>安装包主要有三个目录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">confluent-3.3.0/bin/        # Driver scripts for starting/stopping services</span><br><span class="line">confluent-3.3.0/etc/        # Configuration files</span><br><span class="line">confluent-3.3.0/share/java/ # Jars</span><br></pre></td></tr></table></figure>
<p>启动各个组件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties &amp;</span><br><span class="line">./bin/kafka-server-start ./etc/kafka/server.properties &amp;</span><br><span class="line">./bin/schema-registry-start ./etc/schema-registry/schema-registry.properties &amp;</span><br></pre></td></tr></table></figure>
<h4 id="1-_控制中心（Controll_Center）">1. 控制中心（Controll Center）</h4><p>Confluent商业产品的一个重要功能是控制中心（Controll Center）。在启动控制中心之前呢，需要修改下面三个文件的配置信息：</p>
<ul>
<li>Kafka服务端的配置文件：etc/kafka/server.properties</li>
<li>Kafka Connect集群的配置文件：etc/kafka/connect-distributed.properties</li>
<li>控制中心中心的配置文件：etc/confluent-control-center/control-center.properties</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">sed &apos;s/#metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter/metric.reporters=io.confluent.metrics.reporter.ConfluentMetricsReporter/g&apos; &amp;&amp; \</span><br><span class="line">sed &apos;s/#confluent.metrics.reporter.bootstrap.servers=localhost:9092/confluent.metrics.reporter.bootstrap.servers=localhost:9092/g&apos; &amp;&amp; \</span><br><span class="line">sed &apos;s/#confluent.metrics.reporter.zookeeper.connect=localhost:2181/confluent.metrics.reporter.zookeeper.connect=localhost:2181/g&apos; &amp;&amp; \</span><br><span class="line">sed &apos;s/#confluent.metrics.reporter.topic.replicas=1/confluent.metrics.reporter.topic.replicas=1/g&apos; \</span><br><span class="line">etc/kafka/server.properties</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; etc/kafka/connect-distributed.properties</span><br><span class="line"></span><br><span class="line"># Interceptor setup</span><br><span class="line">consumer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor</span><br><span class="line">producer.interceptor.classes=io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &lt;&lt;EOF &gt;&gt; etc/confluent-control-center/control-center.properties</span><br><span class="line"></span><br><span class="line"># Quickstart partition and replication values</span><br><span class="line">confluent.controlcenter.internal.topics.partitions=1</span><br><span class="line">confluent.controlcenter.internal.topics.replication=1</span><br><span class="line">confluent.controlcenter.command.topic.replication=1</span><br><span class="line">confluent.monitoring.interceptor.topic.partitions=1</span><br><span class="line">confluent.monitoring.interceptor.topic.replication=1</span><br><span class="line">confluent.metrics.topic.partition=1</span><br><span class="line">confluent.metrics.topic.replication=1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<p>接着启动confluent-control-center和分布式的Kafka连接器集群：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/control-center-start etc/confluent-control-center/control-center.properties &amp;</span><br><span class="line">bin/connect-distributed etc/kafka/connect-distributed.properties  &amp;</span><br></pre></td></tr></table></figure>
<p>然后执行一些性能测试，比如执行生产者和消费者的性能测试脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics --zookeeper localhost:2181 --create \</span><br><span class="line">    --topic test-1 --partitions 1 --replication-factor 1</span><br><span class="line"></span><br><span class="line">bin/kafka-run-class org.apache.kafka.tools.ProducerPerformance \</span><br><span class="line">    --topic test-1 --num-records 50000000 --record-size 100 \</span><br><span class="line">    --throughput -1 --producer-props acks=1 buffer.memory=67108864 \</span><br><span class="line">    batch.size=8196 bootstrap.servers=localhost:9092</span><br><span class="line"></span><br><span class="line">bin/kafka-consumer-perf-test --zookeeper localhost:2181 \</span><br><span class="line">    --messages 50000000 --topic test-1 --threads 1</span><br></pre></td></tr></table></figure>
<p>打开浏览器：<a href="http://192.168.6.53:9021" target="_blank" rel="noopener">http://192.168.6.53:9021</a>，观察到页面实时显示集群的相关度量曲线图：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170928175456695" alt="controlcenter"></p>
<h4 id="2-_连接器（Kafka_Connect）">2. 连接器（Kafka Connect）</h4><p>自带的kafka-connect-elasticsearch插件的相关文件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 confluent-3.2.1]$ ll etc/kafka-connect-elasticsearch/</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users 803 9月  28 16:11 quickstart-elasticsearch.properties</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0653 confluent-3.2.1]$ ll share/java/kafka-connect-elasticsearch/</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  263965 9月  28 16:12 commons-codec-1.9.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  434678 9月  28 16:12 commons-lang3-3.4.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users   61829 9月  28 16:12 commons-logging-1.2.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  212164 9月  28 16:12 gson-2.4.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users 2256213 9月  28 16:12 guava-18.0.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  177013 9月  28 16:12 httpasyncclient-4.1.1.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  732765 9月  28 16:12 httpclient-4.5.1.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  326724 9月  28 16:12 httpcore-4.4.4.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  356091 9月  28 16:12 httpcore-nio-4.4.4.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users   18398 9月  28 16:12 jest-2.0.0.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users  216228 9月  28 16:12 jest-common-2.0.0.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users   44524 9月  28 16:12 kafka-connect-elasticsearch-3.2.1.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users   41071 9月  28 16:12 slf4j-api-1.7.21.jar</span><br><span class="line">-rw-r--r-- 1 qihuang.zheng users   10680 9月  28 16:12 slf4j-simple-1.7.5.jar</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kafka技术内幕附录&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/tags/kafka/"/>
    
  </entry>
  
  <entry>
    <title>图解Java多线程</title>
    <link href="http://github.com/zqhxuyuan/2017/10/25/Java-Threads/"/>
    <id>http://github.com/zqhxuyuan/2017/10/25/Java-Threads/</id>
    <published>2017-10-24T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.333Z</updated>
    
    <content type="html"><![CDATA[<p>图解Java多线程笔记：<a href="http://tutorials.jenkov.com/java-concurrency/java-memory-model.html" target="_blank" rel="noopener">http://tutorials.jenkov.com/java-concurrency/java-memory-model.html</a><br><a id="more"></a></p>
<p>Java内存模型（JMM）定义了：how and when <strong>different threads</strong> can see<br>values written to <strong>shared variables</strong> by other threads,<br>and how to <strong>synchronize access</strong> to shared variables when necessary.</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223259352" alt="1"></p>
<p>Java堆和栈中的对象存储位置：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223319776" alt="2"></p>
<p>Java内存模型与硬件模型：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223335612" alt="3"></p>
<p>线程读取主内存的数据到CPU缓冲中，当数据放在不同位置时，会有两个问题：可见性与静态条件</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223349934" alt="4"></p>
<p>A synchronized block in Java is synchronized on some object.<br>All synchronized blocks synchronized on the same object can only<br>have one thread executing inside them at the same time.<br>All other threads attempting to enter the synchronized block are blocked<br>until the thread inside the synchronized block exits the block.</p>
<p>The synchronized keyword can be used to mark four different types of blocks:</p>
<ol>
<li>Instance methods -&gt; on the instance (object) owning the method</li>
<li>Static methods -&gt; on the class object of the class belongs to …</li>
<li>Code blocks inside instance methods</li>
<li>Code blocks inside static methods</li>
</ol>
<p>Synchronized Instance methods（实例方法的同步）：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223405659" alt="5"></p>
<p>静态方法的同步：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223425575" alt="6"></p>
<p>代码块的同步：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223439855" alt="7"></p>
<p>用jstack查看，同一个监视器对象只允许有一个线程访问：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223505856" alt="d"></p>
<p>实例方法的同步加上代码块this的同步，仍然针对同一个实例对象：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223526228" alt="e"></p>
<p>自定义监视器对象：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223540422" alt="f"></p>
<p>同一个实例对象的加锁：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223553349" alt="g"></p>
<p>不同实例对象的加锁：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223606644" alt="17"></p>
<p>Volatile keyword guarantees visibility of changes to variables across threads.</p>
<blockquote>
<p>every read of a volatile variable will be<br>read from the computer’s main memory,<br>and not from the CPU cache.</p>
</blockquote>
<blockquote>
<p>every write to a volatile variable will be<br>written to main memory,<br>and not just to the CPU cache.</p>
</blockquote>
<p>If Thread A writes to a volatile variable and Thread B subsequently reads the same volatile variable, then all variables visible to Thread A before writing the volatile variable, will also be visible to Thread B after it has read the volatile variable. </p>
<p>The reading and writing instructions of volatile variables cannot be reordered by the JVM. Instructions before and after can be reordered, but the volatile read or write cannot be mixed with these instructions. Whatever instructions follow a read or write of a volatile variable are guaranteed to happen after the read or write.</p>
<p>volatile变量不保证事务：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223619840" alt="56"></p>
<p>volatile变量仍然会存在竞态条件：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223635898" alt="27"></p>
<p>volatile变量会禁止重排序：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223648548" alt="49"></p>
<p>如果变量在volatile变量更新之后，不保证写到主存：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223702928" alt="11"></p>
<p>为了保证可见性，不需要为每个变量都定义为volatile类型：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223715760" alt="36"></p>
<p>volatile变量是个内存屏障，在这之前和之后的指令可以重排序：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223729652" alt="56"></p>
<p>本地线程的示例：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223744977" alt="15"></p>
<p>下面的上图没有使用本地线程，下图使用了本地线程：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223805363" alt="29"></p>
<p>线程的信号量实现方式–busy waiting：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223821970" alt="53"></p>
<p>或者可以用volatile变量：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223835313" alt="16"></p>
<p>wait和notify的示例：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223847929" alt="34"></p>
<p>notify与notifyAll的示例：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223900285" alt="51"></p>
<p>等待线程有可能意外被唤醒，需要用while循环继续判断是否被唤醒线程notify：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223915171" alt="05"></p>
<p>一次唤醒所有线程，或者每次一个个地唤醒：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223931103" alt="21"></p>
<p>不同线程之间采用字符串作为监视器锁，会唤醒别的线程：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223943618" alt="43"></p>
<p>不同线程之间的信号没有共享，等待线程被唤醒后继续进入wait状态：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025223957776" alt="03"></p>
<p>不同线程的等待与唤醒示例：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171025224010374" alt="18"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图解Java多线程笔记：&lt;a href=&quot;http://tutorials.jenkov.com/java-concurrency/java-memory-model.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://tutorials.jenkov.com/java-concurrency/java-memory-model.html&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="java" scheme="http://github.com/zqhxuyuan/categories/java/"/>
    
    
      <category term="java" scheme="http://github.com/zqhxuyuan/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>读书笔记-Design Data Intensive Applications</title>
    <link href="http://github.com/zqhxuyuan/2017/10/22/BookNote-DDIA/"/>
    <id>http://github.com/zqhxuyuan/2017/10/22/BookNote-DDIA/</id>
    <published>2017-10-21T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.263Z</updated>
    
    <content type="html"><![CDATA[<p>Design Data Intensive Applications<br><a id="more"></a></p>
<h2 id="Ch3:_Storage_and_Retrieval">Ch3: Storage and Retrieval</h2><p>Hash Index的目的是为数据库构建一份索引，方便根据key快速查询对应的value。</p>
<p>Compaction操作合并多个文件，相同key只会保存一份最新的value。</p>
<p>SSTables和LSM树：数据写到MemTable中是排序的，刷写到磁盘上也是有序的，最后通过定期的Compaction再合并数据。</p>
<p>由于每个SSTable的key都是唯一的，多个SSTable文件合并时，如果key重复，选取最新Segment的值，去掉旧Segment的所有值。</p>
<p>读取Segment不可避免地要扫描文件，所以可以对文件进行压缩，提高I/O带宽和传输速率。</p>
<p>不需要为所有SSTable的key建全量索引，只需要稀疏索引。由于key是有序的，可以通过二分查找快速定位key的位置。</p>
<p>稀疏索引不是必须的，不过通常需要稀疏索引。如果key和value的长度是固定的，就可以不需要稀疏索引，不同实际情况value一般是变长的。</p>
<p>LSM树的优化方法有：为文件添加BloomFilter、不同的合并策略。</p>
<p>传统数据库使用B树就地更新数据。B树一般将数据库分成固定的块或页，比如4K。这样读写操作每次也是一页。<br>这种方式和底层硬件对应起来，比如磁盘就是按照4K固定块组织的。</p>
<p>新增key到B树会调整树的结构，比如拆分出两个子Page，然后更新父Page。</p>
<p>B树的优化方法有：Copy-On-Write、不存储整个key，而是对key进行简写、范围查询时，子页之间会有指针。</p>
<p>为了容错，B树和LSM都有WAL预写日志，用于节点宕机后的数据恢复。</p>
<p>虽然LSM在后台执行增量的Compaction操作，但是磁盘资源有限，当执行一个昂贵的Compaction，<br>客户端请求可能需要等待Compaction完成，造成响应时间上升。</p>
<p>磁盘的写带宽会被三个操作共享：写WAL日志、MemTable刷写磁盘、Compaction。<br>数据库一旦变得越来越大，Compaction操作需要的带宽也会越来越多。</p>
<p>Compaction如果没有配置好，一旦写吞吐量很高，那么Compaction操作跟不上写请求。未合并的文件会越来越多，读请求也会越来越慢。</p>
<p>B树和LSM树的区别是：每个键在B树中只有一条记录，但在LSM中可能存在多条。这也是B树可以提供强一致性事务的保证（只对行进行加锁）。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Design Data Intensive Applications&lt;br&gt;
    
    </summary>
    
      <category term="book" scheme="http://github.com/zqhxuyuan/categories/book/"/>
    
    
      <category term="book" scheme="http://github.com/zqhxuyuan/tags/book/"/>
    
  </entry>
  
  <entry>
    <title>深入解析中间件之-RocketMQ</title>
    <link href="http://github.com/zqhxuyuan/2017/10/18/Midd-RocketMQ/"/>
    <id>http://github.com/zqhxuyuan/2017/10/18/Midd-RocketMQ/</id>
    <published>2017-10-17T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.370Z</updated>
    
    <content type="html"><![CDATA[<p>Apache RocketMQ: <a href="http://rocketmq.apache.org/" target="_blank" rel="noopener">http://rocketmq.apache.org/</a><br><a id="more"></a></p>
<h1 id="QuickStart">QuickStart</h1><p>分别启动NameServer、Broker、生产者、消费者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&gt; nohup sh bin/mqnamesrv &amp;</span><br><span class="line">&gt; nohup sh bin/mqbroker -n localhost:9876 &amp;</span><br><span class="line"></span><br><span class="line">&gt; export NAMESRV_ADDR=localhost:9876</span><br><span class="line">&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D36B0000, offsetMsgId=AC112A0100002A9F0000000000000000, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=3], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D3F50001, offsetMsgId=AC112A0100002A9F00000000000000B2, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=0], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D3FB0002, offsetMsgId=AC112A0100002A9F0000000000000164, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=1], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D4000003, offsetMsgId=AC112A0100002A9F0000000000000216, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=2], queueOffset=0]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D4040004, offsetMsgId=AC112A0100002A9F00000000000002C8, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=3], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D4080005, offsetMsgId=AC112A0100002A9F000000000000037A, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=0], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D40C0006, offsetMsgId=AC112A0100002A9F000000000000042C, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=1], queueOffset=1]</span><br><span class="line">SendResult [sendStatus=SEND_OK, msgId=AC112A0140641B6D35866042D4100007, offsetMsgId=AC112A0100002A9F00000000000004DE, messageQueue=MessageQueue [topic=TopicTest, brokerName=dp0652, queueId=2], queueOffset=1]</span><br><span class="line"></span><br><span class="line">&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer</span><br><span class="line">Consumer Started.</span><br><span class="line">ConsumeMessageThread_6 Receive New Messages: [MessageExt [queueId=1, storeSize=178, queueOffset=1, sysFlag=0, bornTimestamp=1508402192396, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192398, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F000000000000042C, commitLogOffset=1068, bodyCRC=1307562618, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D40C0006, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_11 Receive New Messages: [MessageExt [queueId=1, storeSize=179, queueOffset=2, sysFlag=0, bornTimestamp=1508402192410, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192412, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F00000000000006F4, commitLogOffset=1780, bodyCRC=193412630, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243399, UNIQ_KEY=AC112A0140641B6D35866042D41A000A, WAIT=true, TAGS=TagA&#125;, body=17]]]</span><br><span class="line">ConsumeMessageThread_2 Receive New Messages: [MessageExt [queueId=2, storeSize=178, queueOffset=0, sysFlag=0, bornTimestamp=1508402192384, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192386, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F0000000000000216, commitLogOffset=534, bodyCRC=1032136437, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D4000003, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_1 Receive New Messages: [MessageExt [queueId=0, storeSize=178, queueOffset=0, sysFlag=0, bornTimestamp=1508402192373, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192377, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F00000000000000B2, commitLogOffset=178, bodyCRC=1401636825, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D3F50001, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_4 Receive New Messages: [MessageExt [queueId=3, storeSize=178, queueOffset=0, sysFlag=0, bornTimestamp=1508402192236, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192319, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F0000000000000000, commitLogOffset=0, bodyCRC=613185359, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243397, UNIQ_KEY=AC112A0140641B6D35866042D36B0000, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_7 Receive New Messages: [MessageExt [queueId=3, storeSize=178, queueOffset=1, sysFlag=0, bornTimestamp=1508402192388, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192390, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F00000000000002C8, commitLogOffset=712, bodyCRC=601994070, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D4040004, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_8 Receive New Messages: [MessageExt [queueId=2, storeSize=178, queueOffset=1, sysFlag=0, bornTimestamp=1508402192400, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192401, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F00000000000004DE, commitLogOffset=1246, bodyCRC=988340972, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D4100007, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line">ConsumeMessageThread_3 Receive New Messages: [MessageExt [queueId=1, storeSize=178, queueOffset=0, sysFlag=0, bornTimestamp=1508402192379, bornHost=/172.17.42.1:55844, storeTimestamp=1508402192382, storeHost=/172.17.42.1:10911, msgId=AC112A0100002A9F0000000000000164, commitLogOffset=356, bodyCRC=1250039395, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message [topic=TopicTest, flag=0, properties=&#123;MIN_OFFSET=0, MAX_OFFSET=250, CONSUME_START_TIME=1508402243398, UNIQ_KEY=AC112A0140641B6D35866042D3FB0002, WAIT=true, TAGS=TagA&#125;, body=16]]]</span><br><span class="line"></span><br><span class="line">17884 org.apache.rocketmq.namesrv.NamesrvStartup</span><br><span class="line">17965 org.apache.rocketmq.broker.BrokerStartup -n localhost:9876</span><br></pre></td></tr></table></figure>
<p>RocketMQ的数据目录在store下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ tree store</span><br><span class="line">store</span><br><span class="line">├── abort</span><br><span class="line">├── checkpoint</span><br><span class="line">├── commitlog</span><br><span class="line">│   ├── 00000000000000000000</span><br><span class="line">│   └── 00000000001073741824</span><br><span class="line">├── config</span><br><span class="line">│   ├── consumerFilter.json</span><br><span class="line">│   ├── consumerOffset.json</span><br><span class="line">│   ├── delayOffset.json</span><br><span class="line">│   ├── subscriptionGroup.json</span><br><span class="line">│   ├── topics.json</span><br><span class="line">├── consumequeue</span><br><span class="line">│   └── TopicTest</span><br><span class="line">│       ├── 0</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       ├── 1</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       ├── 2</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       └── 3</span><br><span class="line">│           └── 00000000000000000000</span><br><span class="line">└── index</span><br><span class="line">    └── 20171019163632344</span><br></pre></td></tr></table></figure>
<p>数据相关的文件夹有三个：</p>
<ul>
<li>commitlog：提交日志</li>
<li>consumequeue：消费队列</li>
<li>index：索引文件</li>
</ul>
<p>查看commitlog的内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ strings store/commitlog/00000000000000000000 | head -30</span><br><span class="line">Hello RocketMQ 0    TopicTest</span><br><span class="line">&gt;UNIQ_KEY</span><br><span class="line">AC112A0140641B6D35866042D36B0000</span><br><span class="line">WAIT</span><br><span class="line">true</span><br><span class="line">TAGS</span><br><span class="line">TagA</span><br><span class="line">Hello RocketMQ 1    TopicTest</span><br><span class="line">&gt;UNIQ_KEY</span><br><span class="line">AC112A0140641B6D35866042D3F50001</span><br><span class="line">WAIT</span><br><span class="line">true</span><br><span class="line">TAGS</span><br><span class="line">TagA</span><br><span class="line">Hello RocketMQ 2    TopicTest</span><br><span class="line">&gt;UNIQ_KEY</span><br><span class="line">AC112A0140641B6D35866042D3FB0002</span><br><span class="line">WAIT</span><br><span class="line">true</span><br><span class="line">TAGS</span><br><span class="line">TagA</span><br><span class="line">Hello RocketMQ 3    TopicTest</span><br></pre></td></tr></table></figure>
<p>消费者的相关配置：</p>
<ul>
<li>消费者对订阅主题的消费进度存储在<code>consumerOffset.json</code>配置文件中</li>
<li>消费者所属的消费组信息存储在<code>subscriptionGroup.json</code>配置文件中</li>
<li>消费者订阅的主题存储在<code>topics.json</code>配置文件中</li>
</ul>
<blockquote>
<p>Kafka中消费者订阅信息存储在ZooKeeper中</p>
</blockquote>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ cat store/config/consumerFilter.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"filterDataByTopic"</span>:&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line">[qihuang.zheng@dp0652 ~]$ cat store/config/delayOffset.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"offsetTable"</span>:&#123;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 ~]$ cat store/config/consumerOffset.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"offsetTable"</span>:&#123;</span><br><span class="line">        "TopicTest@please_rename_unique_group_name_4":&#123;0:250,1:250,2:250,3:250</span><br><span class="line">        &#125;,</span><br><span class="line">        "%RETRY%please_rename_unique_group_name_4@please_rename_unique_group_name_4":&#123;0:0</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 ~]$ cat store/config/subscriptionGroup.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"dataVersion"</span>:&#123;</span><br><span class="line">        <span class="attr">"counter"</span>:<span class="number">1</span>,</span><br><span class="line">        <span class="attr">"timestamp"</span>:<span class="number">1508402243205</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"subscriptionGroupTable"</span>:&#123;</span><br><span class="line">        <span class="attr">"please_rename_unique_group_name_4"</span>:&#123;</span><br><span class="line">            <span class="attr">"brokerId"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"consumeBroadcastEnable"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"consumeEnable"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"consumeFromMinEnable"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"groupName"</span>:<span class="string">"please_rename_unique_group_name_4"</span>,</span><br><span class="line">            <span class="attr">"notifyConsumerIdsChangedEnable"</span>:<span class="literal">true</span>,</span><br><span class="line">            <span class="attr">"retryMaxTimes"</span>:<span class="number">16</span>,</span><br><span class="line">            <span class="attr">"retryQueueNums"</span>:<span class="number">1</span>,</span><br><span class="line">            <span class="attr">"whichBrokerWhenConsumeSlowly"</span>:<span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 ~]$ cat store/config/topics.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="attr">"dataVersion"</span>:&#123;</span><br><span class="line">        <span class="attr">"counter"</span>:<span class="number">2</span>,</span><br><span class="line">        <span class="attr">"timestamp"</span>:<span class="number">1508402243219</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"topicConfigTable"</span>:&#123;</span><br><span class="line">        <span class="attr">"TopicTest"</span>:&#123;</span><br><span class="line">            <span class="attr">"order"</span>:<span class="literal">false</span>,</span><br><span class="line">            <span class="attr">"perm"</span>:<span class="number">6</span>,</span><br><span class="line">            <span class="attr">"readQueueNums"</span>:<span class="number">4</span>,</span><br><span class="line">            <span class="attr">"topicFilterType"</span>:<span class="string">"SINGLE_TAG"</span>,</span><br><span class="line">            <span class="attr">"topicName"</span>:<span class="string">"TopicTest"</span>,</span><br><span class="line">            <span class="attr">"topicSysFlag"</span>:<span class="number">0</span>,</span><br><span class="line">            <span class="attr">"writeQueueNums"</span>:<span class="number">4</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在本机测试时，没有遇到问题。但是IDE连接远程机器时，报错连接不上，这是因为服务端装了docker导致IP有问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">org.apache.rocketmq.client.exception.MQClientException: Send [3] times, still failed, cost [6915]ms, Topic: TopicTestA, BrokersSent: [dp0652, dp0652, dp0652]</span><br><span class="line">See http://rocketmq.apache.org/docs/faq/ for further details.</span><br><span class="line">    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.sendDefaultImpl(DefaultMQProducerImpl.java:544)</span><br><span class="line">    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:1065)</span><br><span class="line">    at org.apache.rocketmq.client.impl.producer.DefaultMQProducerImpl.send(DefaultMQProducerImpl.java:1023)</span><br><span class="line">    at org.apache.rocketmq.client.producer.DefaultMQProducer.send(DefaultMQProducer.java:212)</span><br><span class="line">    at org.apache.rocketmq.example.quickstart.Producer.main(Producer.java:69)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">    at java.lang.reflect.Method.invoke(Method.java:497)</span><br><span class="line">    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)</span><br><span class="line">Caused by: org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;172.17.42.1:10909&gt; failed</span><br></pre></td></tr></table></figure>
<p>172.17.42.1这个IP地址是docker的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 rocketmq]$ ifconfig</span><br><span class="line">docker0   Link encap:Ethernet  HWaddr CA:3E:ED:C2:67:20</span><br><span class="line">          inet addr:172.17.42.1  Bcast:0.0.0.0  Mask:255.255.0.0</span><br><span class="line"></span><br><span class="line">em1       Link encap:Ethernet  HWaddr B0:83:FE:C7:02:B3</span><br><span class="line">          inet addr:192.168.6.52  Bcast:192.168.6.255  Mask:255.255.255.0</span><br></pre></td></tr></table></figure>
<p>用模板生成，可以看到brokerIP1就是docker的IP:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 rocketmq]$ sh bin/mqbroker -m &gt; broker.p</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 rocketmq]$ cat broker.p</span><br><span class="line">2017-10-19 17\:58\:00 INFO main - namesrvAddr=localhost:9876</span><br><span class="line">2017-10-19 17\:58\:00 INFO main - brokerIP1=172.17.42.1</span><br><span class="line">2017-10-19 17\:58\:00 INFO main - brokerName=dp0652</span><br><span class="line">2017-10-19 17\:58\:00 INFO main - brokerClusterName=DefaultCluster</span><br><span class="line">2017-10-19 17\:58\:00 INFO main - brokerId=0</span><br></pre></td></tr></table></figure>
<p>接下来重启broker:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 rocketmq]$ sh bin/mqshutdown broker</span><br><span class="line">The mqbroker(29723) is running...</span><br><span class="line">Send shutdown request to mqbroker(29723) OK</span><br><span class="line">[qihuang.zheng@dp0652 rocketmq]$ nohup sh bin/mqbroker -n localhost:9876 -c broker.properties &amp;</span><br></pre></td></tr></table></figure>
<p>重启后发送消息正常，这里把Topic改成TopicTestA：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SendResult [sendStatus=SEND_OK, msgId=0A39F12CF5A6355DA25460935C280000, offsetMsgId=C0A8063400002A9F000000000002BEB2, messageQueue=MessageQueue [topic=TopicTestA, brokerName=dp0652, queueId=0], queueOffset=0]</span><br></pre></td></tr></table></figure>
<p>查看store，可以看到commitlog没有新增文件夹，而consumequeue则新增了TopicTestA文件夹：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">├── commitlog</span><br><span class="line">│   └── 00000000000000000000</span><br><span class="line">├── consumequeue</span><br><span class="line">│   ├── TopicTest</span><br><span class="line">│   │   ├── 0</span><br><span class="line">│   │   │   └── 00000000000000000000</span><br><span class="line">│   │   ├── 1</span><br><span class="line">│   │   │   └── 00000000000000000000</span><br><span class="line">│   │   ├── 2</span><br><span class="line">│   │   │   └── 00000000000000000000</span><br><span class="line">│   │   └── 3</span><br><span class="line">│   │       └── 00000000000000000000</span><br><span class="line">│   └── TopicTestA</span><br><span class="line">│       ├── 0</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       ├── 1</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       ├── 2</span><br><span class="line">│       │   └── 00000000000000000000</span><br><span class="line">│       └── 3</span><br><span class="line">│           └── 00000000000000000000</span><br></pre></td></tr></table></figure>
<h1 id="API示例">API示例</h1><h2 id="生产者">生产者</h2><p><strong>同步</strong>的生产者：<a href="http://rocketmq.apache.org/docs/simple-example/" target="_blank" rel="noopener">http://rocketmq.apache.org/docs/simple-example/</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQProducer producer = <span class="keyword">new</span> DefaultMQProducer(<span class="string">"please_rename_unique_group_name"</span>);</span><br><span class="line">producer.setNamesrvAddr(<span class="string">"192.168.6.52:9876"</span>);</span><br><span class="line">producer.start();</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">    Message msg = <span class="keyword">new</span> Message(<span class="string">"TopicTestA"</span>, <span class="string">"TagA"</span>, </span><br><span class="line">      (<span class="string">"RocketMQ "</span> + i).getBytes(RemotingHelper.DEFAULT_CHARSET)</span><br><span class="line">    );</span><br><span class="line">    SendResult sendResult = producer.send(msg);</span><br><span class="line">    System.out.printf(<span class="string">"%s%n"</span>, sendResult);</span><br><span class="line">&#125;</span><br><span class="line">producer.shutdown();</span><br></pre></td></tr></table></figure>
<p><strong>异步</strong>的生产者：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">producer.send(msg, <span class="keyword">new</span> SendCallback() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onSuccess</span><span class="params">(SendResult sendResult)</span> </span>&#123;</span><br><span class="line">        System.out.printf(<span class="string">"%-10d OK %s %n"</span>, index, sendResult.getMsgId());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<p><strong>一次性</strong>的生产者，主要用于日志收集：</p>
<p>一个 RPC 调用,通常是这样一个过程:</p>
<ol>
<li>客户端发送请求到服务器</li>
<li>服务器处理该请求</li>
<li>服务器向客户端返回应答</li>
</ol>
<p>所以一个 RPC 的耗时时间是上述三个步骤的总和,而某些场景要求耗时非常短,但是对可靠性要求并不高,<br>例如日志收集类应用,此类应用可以采用 oneway 形式调用,oneway 形式只发送请求不等待应答,<br>而发送请求在客户端实现层面仅仅是一个 os 系统调用的开销,即将数据写入客户端的 socket 缓冲区,此过程耗时通常在微秒级。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">producer.sendOneway(msg);</span><br></pre></td></tr></table></figure>
<p><strong>有序</strong>的生产者：<a href="http://rocketmq.apache.org/docs/order-example/" target="_blank" rel="noopener">http://rocketmq.apache.org/docs/order-example/</a></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">MQProducer producer = <span class="keyword">new</span> DefaultMQProducer(<span class="string">"example_group_name"</span>);</span><br><span class="line">producer.start();</span><br><span class="line">String[] tags = <span class="keyword">new</span> String[] &#123;<span class="string">"TagA"</span>, <span class="string">"TagB"</span>, <span class="string">"TagC"</span>, <span class="string">"TagD"</span>, <span class="string">"TagE"</span>&#125;;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> orderId = i % <span class="number">10</span>;</span><br><span class="line">    <span class="comment">//Create a message instance, specifying topic, tag , message key and body.</span></span><br><span class="line">    Message msg = <span class="keyword">new</span> Message(<span class="string">"TopicTestjjj"</span>, tags[i % tags.length], <span class="string">"KEY"</span> + i, (<span class="string">"Hello RocketMQ "</span> + i).getBytes(RemotingHelper.DEFAULT_CHARSET));</span><br><span class="line">    SendResult sendResult = producer.send(msg, <span class="keyword">new</span> MessageQueueSelector() &#123;</span><br><span class="line">        <span class="function"><span class="keyword">public</span> MessageQueue <span class="title">select</span><span class="params">(List&lt;MessageQueue&gt; mqs, Message msg, Object arg)</span> </span>&#123;</span><br><span class="line">            Integer id = (Integer) arg;</span><br><span class="line">            <span class="keyword">int</span> index = id % mqs.size();</span><br><span class="line">            <span class="keyword">return</span> mqs.get(index);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, orderId); <span class="comment">// 最后一个参数orderId作为第二个参数的arg值</span></span><br><span class="line">    System.out.printf(<span class="string">"%s%n"</span>, sendResult);</span><br><span class="line">&#125;</span><br><span class="line">producer.shutdown();</span><br></pre></td></tr></table></figure>
<p><strong>定时</strong>生产者：<a href="http://rocketmq.apache.org/docs/schedule-example/" target="_blank" rel="noopener">http://rocketmq.apache.org/docs/schedule-example/</a></p>
<p>定时消息是指消息发到 Broker 后,不能立刻被 Consumer 消费,要到特定的时间点或者等待特定的时间后才能被消费。<br>如果要支持任意的时间精度,在 Broker 局面,必须要做消息排序,如果再涉及到持久化,那么消息排序要不可避免的产生巨大性能开销。</p>
<p>RocketMQ 支持定时消息,但是不支持任意时间精度,仅支持特定的 level,例如定时 5s,10s,1m 等。<br>定时消息是在生产者端设置DelayTimeLevel，消费者端不做任何处理。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ScheduledMessageProducer</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        DefaultMQProducer producer = <span class="keyword">new</span> DefaultMQProducer(<span class="string">"ExampleProducerGroup"</span>);</span><br><span class="line">        producer.start();</span><br><span class="line">        <span class="keyword">int</span> totalMessagesToSend = <span class="number">100</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; totalMessagesToSend; i++) &#123;</span><br><span class="line">            Message message = <span class="keyword">new</span> Message(<span class="string">"TestTopic"</span>, (<span class="string">"Hello scheduled message "</span> + i).getBytes());</span><br><span class="line">            <span class="comment">// This message will be delivered to consumer 10 seconds later.</span></span><br><span class="line">            message.setDelayTimeLevel(<span class="number">3</span>);</span><br><span class="line">            producer.send(message);</span><br><span class="line">        &#125;</span><br><span class="line">        producer.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line">       </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>批量消息：<a href="http://rocketmq.apache.org/docs/batch-example/" target="_blank" rel="noopener">http://rocketmq.apache.org/docs/batch-example/</a></p>
<p>简单的批量消息只需要构造List<message>，调用producer.send()即可。不过在一个Batch中消息大小不能超过1Mib，需要程序手动进行切分。</message></p>
<h2 id="消费者">消费者</h2><p><strong>拉取</strong>消费者（PullConsumer）：</p>
<ul>
<li>首先根据Topic获取订阅的MessageQueue</li>
<li>对每个MessageQueue，都会调用pullBlockIfNotFound方法消费这个队列里的消息</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PullConsumer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Map&lt;MessageQueue, Long&gt; OFFSE_TABLE = <span class="keyword">new</span> HashMap&lt;MessageQueue, Long&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> MQClientException </span>&#123;</span><br><span class="line">        DefaultMQPullConsumer consumer = <span class="keyword">new</span> DefaultMQPullConsumer(<span class="string">"please_rename_unique_group_name_5"</span>);</span><br><span class="line">        consumer.start();</span><br><span class="line">        Set&lt;MessageQueue&gt; mqs = consumer.fetchSubscribeMessageQueues(<span class="string">"TopicTest1"</span>);</span><br><span class="line">        <span class="keyword">for</span> (MessageQueue mq : mqs) &#123;</span><br><span class="line">            System.out.printf(<span class="string">"Consume from the queue: "</span> + mq + <span class="string">"%n"</span>);</span><br><span class="line">            SINGLE_MQ:</span><br><span class="line">            <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">                PullResult pullResult = consumer.pullBlockIfNotFound(mq, <span class="keyword">null</span>, getMessageQueueOffset(mq), <span class="number">32</span>);</span><br><span class="line">                System.out.printf(<span class="string">"%s%n"</span>, pullResult);</span><br><span class="line">                putMessageQueueOffset(mq, pullResult.getNextBeginOffset());</span><br><span class="line">                <span class="keyword">switch</span> (pullResult.getPullStatus()) &#123;</span><br><span class="line">                    <span class="keyword">case</span> FOUND:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> NO_MATCHED_MSG:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">case</span> NO_NEW_MSG:</span><br><span class="line">                        <span class="keyword">break</span> SINGLE_MQ;</span><br><span class="line">                    <span class="keyword">case</span> OFFSET_ILLEGAL:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                    <span class="keyword">default</span>:</span><br><span class="line">                        <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        consumer.shutdown();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getMessageQueueOffset</span><span class="params">(MessageQueue mq)</span> </span>&#123;</span><br><span class="line">        Long offset = OFFSE_TABLE.get(mq);</span><br><span class="line">        <span class="keyword">if</span> (offset != <span class="keyword">null</span>) <span class="keyword">return</span> offset;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">putMessageQueueOffset</span><span class="params">(MessageQueue mq, <span class="keyword">long</span> offset)</span> </span>&#123;</span><br><span class="line">        OFFSE_TABLE.put(mq, offset);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>推送</strong>消费者（PushConsumer）：</p>
<ul>
<li>订阅方法的第二个参数为<code>*</code>，表示所有的Tag，不进行过滤</li>
<li>Push推送方式采用注册消息监听器的方式，当收到Broker推送的消息，就会触发监听器的回调</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"CID_JODIE_1"</span>);</span><br><span class="line">consumer.subscribe(<span class="string">"Jodie_topic_1023"</span>, <span class="string">"*"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 下面几个采用Push模式的消费者的监听器都一样</span></span><br><span class="line">consumer.registerMessageListener(<span class="keyword">new</span> MessageListenerConcurrently() &#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumeConcurrentlyStatus <span class="title">consumeMessage</span><span class="params">(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context)</span> </span>&#123;</span><br><span class="line">        System.out.printf(Thread.currentThread().getName() + <span class="string">" Receive New Messages: "</span> + msgs + <span class="string">"%n"</span>);</span><br><span class="line">        <span class="keyword">return</span> ConsumeConcurrentlyStatus.CONSUME_SUCCESS;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">consumer.start();</span><br><span class="line">System.out.printf(<span class="string">"Consumer Started.%n"</span>);</span><br></pre></td></tr></table></figure>
<p><strong>广播</strong>模式的推送消费者，相比上一个示例增加了设置消息模型（<code>setMessageModel</code>），其他没有变化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"please_rename_unique_group_name_1"</span>);</span><br><span class="line">consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);</span><br><span class="line">consumer.setMessageModel(MessageModel.BROADCASTING);</span><br><span class="line">consumer.subscribe(<span class="string">"TopicTest"</span>, <span class="string">"TagA || TagC || TagD"</span>);</span><br></pre></td></tr></table></figure>
<p><strong>过滤器</strong>的消费者。过滤器采用Push方式时，过滤逻辑在Broker实现，Broker把过滤过的数据发送给消费者。<br>如果过滤器采用Pull模式，所有的数据都会传送到消费者，然后在消费者端执行过滤逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"ConsumerGroupNamecc4"</span>);</span><br><span class="line">ClassLoader classLoader = Thread.currentThread().getContextClassLoader();</span><br><span class="line">File classFile = <span class="keyword">new</span> File(classLoader.getResource(<span class="string">"MessageFilterImpl.java"</span>).getFile());</span><br><span class="line">String filterCode = MixAll.file2String(classFile);</span><br><span class="line"><span class="comment">// 订阅方法的第二个参数是过滤器的实现类，而前面示例的第二个参数是Tag过滤</span></span><br><span class="line">consumer.subscribe(<span class="string">"TopicTest"</span>, <span class="string">"org.apache.rocketmq.example.filter.MessageFilterImpl"</span>, filterCode);</span><br></pre></td></tr></table></figure>
<p><strong>SQL</strong>消费者（生产者发送消息时通过putUserProperty可以指定自定义的属性，除了Tag外，自定义属性也可以被过滤）：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"please_rename_unique_group_name_4"</span>);</span><br><span class="line"><span class="comment">// 订阅方法的第二个参数是消息选择器</span></span><br><span class="line">consumer.subscribe(<span class="string">"TopicTest"</span>, MessageSelector.bySql(</span><br><span class="line">    <span class="string">"(TAGS is not null and TAGS in ('TagA', 'TagB'))"</span> +</span><br><span class="line">    <span class="string">"and (a is not null and a between 0  3)"</span>));</span><br></pre></td></tr></table></figure>
<p><strong>有序</strong>的消费者：前面几种消费者注册的监听器是：MessageListenerConcurrently，这里是MessageListenerOrderly。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">DefaultMQPushConsumer consumer = <span class="keyword">new</span> DefaultMQPushConsumer(<span class="string">"example_group_name"</span>);</span><br><span class="line">consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET);</span><br><span class="line">consumer.subscribe(<span class="string">"TopicTest"</span>, <span class="string">"TagA || TagC || TagD"</span>);</span><br><span class="line">consumer.registerMessageListener(<span class="keyword">new</span> MessageListenerOrderly() &#123;</span><br><span class="line">    AtomicLong consumeTimes = <span class="keyword">new</span> AtomicLong(<span class="number">0</span>);</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumeOrderlyStatus <span class="title">consumeMessage</span><span class="params">(List&lt;MessageExt&gt; msgs, ConsumeOrderlyContext context)</span> </span>&#123;</span><br><span class="line">        context.setAutoCommit(<span class="keyword">false</span>);</span><br><span class="line">        System.out.printf(Thread.currentThread().getName() + <span class="string">" Receive New Messages: "</span> + msgs + <span class="string">"%n"</span>);</span><br><span class="line">        <span class="keyword">this</span>.consumeTimes.incrementAndGet();</span><br><span class="line">        <span class="keyword">if</span> ((<span class="keyword">this</span>.consumeTimes.get() % <span class="number">2</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ConsumeOrderlyStatus.SUCCESS;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((<span class="keyword">this</span>.consumeTimes.get() % <span class="number">3</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ConsumeOrderlyStatus.ROLLBACK;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((<span class="keyword">this</span>.consumeTimes.get() % <span class="number">4</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> ConsumeOrderlyStatus.COMMIT;</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> ((<span class="keyword">this</span>.consumeTimes.get() % <span class="number">5</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">            context.setSuspendCurrentQueueTimeMillis(<span class="number">3000</span>);</span><br><span class="line">            <span class="keyword">return</span> ConsumeOrderlyStatus.SUSPEND_CURRENT_QUEUE_A_MOMENT;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ConsumeOrderlyStatus.SUCCESS;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line">consumer.start();</span><br></pre></td></tr></table></figure>
<p>消费者的监听器有两种形式：并发和有序。参考：<a href="http://rocketmq.apache.org/docs/best-practice-consumer/" target="_blank" rel="noopener">http://rocketmq.apache.org/docs/best-practice-consumer/</a></p>
<table>
<thead>
<tr>
<th>监听器</th>
<th>上下文</th>
<th>返回状态</th>
<th>返回码</th>
</tr>
</thead>
<tbody>
<tr>
<td>MessageListenerConcurrently</td>
<td>ConsumeConcurrentlyContext</td>
<td>ConsumeConcurrentlyStatus</td>
<td>CONSUME_SUCCESS</td>
</tr>
<tr>
<td>MessageListenerOrderly</td>
<td>ConsumeOrderlyContext</td>
<td>ConsumeOrderlyStatus</td>
<td>SUCCESS、ROLLBACK、COMMIT、SUSPEND_CURRENT_QUEUE_A_MOMENT</td>
</tr>
</tbody>
</table>
<p>消息消费的顺序问题：</p>
<ul>
<li>并发情况下，返回RECONSUME_LATER，表示过一会儿再消费，先去消费其他消息</li>
<li>有序情况下，返回SUSPEND_CURRENT_QUEUE_A_MOMENT，表示等一会儿再消费，无法消费其他消息</li>
</ul>
<h1 id="基本流程">基本流程</h1><h2 id="Remoting_RPC示例">Remoting RPC示例</h2><p>rocketmq-remoting模块采用Netty封装了RPC的调用，包括客户端和服务端之间的交互。</p>
<p>不同分布式系统在通信上都会实现RPC模块，比如Kafka、Hadoop等都有各自的RPC实现。</p>
<p>先来查看测试用例RemotingServerTest的使用方法：</p>
<ul>
<li>启动RemotingServer和RemotingClient</li>
<li>调用RemotingClient的invokeAsync()或者invokeSync()、invokeOneway()方法</li>
</ul>
<p>以异步调用为例，RemotingClient的invokeAsync()方法主要有三个参数：</p>
<ul>
<li>服务端地址，RPC调用需要指定服务端的地址，这样客户端才能发送请求，让服务端处理</li>
<li>远程指令（RemotingCommand），即客户端发送的请求</li>
<li>回调对象（InvokeCallback），即客户端收到服务端返回的响应结果后，如何处理</li>
</ul>
<p>RPC调用的具体步骤如下：</p>
<ul>
<li>启动客户端和服务端</li>
<li>客户端构造远程指令对象</li>
<li>客户端通过RemotingClient同步或者异步调用</li>
<li>服务端在启动时注册的处理器，会处理客户端发送的请求，即调用处理器的processRequest()方法</li>
<li>服务端处理完请求后，返回响应给客户端</li>
<li>客户端收到服务端返回的响应结果，会触发回调对象调用operationComplete()方法</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">public static RemotingServer createRemotingServer() throws InterruptedException &#123;</span><br><span class="line">    NettyServerConfig config = new NettyServerConfig();</span><br><span class="line">    RemotingServer remotingServer = new NettyRemotingServer(config);</span><br><span class="line">    remotingServer.registerProcessor(0, new NettyRequestProcessor() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public RemotingCommand processRequest(ChannelHandlerContext ctx, RemotingCommand request) &#123;</span><br><span class="line">            request.setRemark(&quot;Hi &quot; + ctx.channel().remoteAddress());</span><br><span class="line">            return request;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;, Executors.newCachedThreadPool());</span><br><span class="line">    remotingServer.start();</span><br><span class="line">    return remotingServer;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void testInvokeAsync() throws InterruptedException, RemotingConnectException,</span><br><span class="line">    RemotingTimeoutException, RemotingTooMuchRequestException, RemotingSendRequestException &#123;</span><br><span class="line"></span><br><span class="line">    final CountDownLatch latch = new CountDownLatch(1);</span><br><span class="line">    RemotingCommand request = RemotingCommand.createRequestCommand(0, null);</span><br><span class="line">    request.setRemark(&quot;messi&quot;);</span><br><span class="line">    remotingClient.invokeAsync(&quot;localhost:8888&quot;, request, 1000 * 3, new InvokeCallback() &#123;</span><br><span class="line">        public void operationComplete(ResponseFuture responseFuture) &#123;</span><br><span class="line">            latch.countDown();</span><br><span class="line">            assertThat(responseFuture.getResponseCommand().getExtFields()).hasSize(2);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    latch.await();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>RemotingServer的registerProcessor()方法有三个参数：</p>
<ul>
<li>请求编码，比如SEND_MESSAGE表示（生产者）客户端发送消息的请求</li>
<li>请求处理器，比如服务端如何处理客户端发送消息的处理器，实现类为：SendMessageProcessor</li>
<li>处理线程，每种请求编码都对应一个处理线程池。如果没有指定，则使用默认的线程池</li>
</ul>
<p>客户端调用服务端有三种方式：同步（Sync）、异步（Async）、一次性（OneWay）。前两种有响应结果，最后一种不产生响应结果。</p>
<h2 id="Netty_RPC">Netty RPC</h2><p><code>NettyRemotingServer</code>在启动时，会绑定NettyServerHandler。Netty RPC的特点如下：</p>
<ul>
<li>请求和响应都是用RemotingCommand对象来表示</li>
<li>服务端（NettyRemotingServer）和客户端（NettyRemotingClient）实现了抽象的NettyRemotingAbstract</li>
<li>抽象类根据不同的指令类型调用不同的处理方法，比如处理请求调用processRequestCommand，处理响应调用processResponseCommand</li>
</ul>
<p>下面举例客户端和服务端执行一次RPC调用链路的过程：</p>
<ul>
<li>客户端发送请求给服务端，通过Netty的Channel发送请求给服务端</li>
<li>服务端处理客户端发送的请求，NettyServerHandler接收的消息类型为REQUEST_COMMAND，调用processRequestCommand方法</li>
<li>服务端处理完成后，通过Netty的Channel发送响应结果给客户端</li>
<li>客户端处理服务端发送的响应，NettyClientHandler接收的消息类型为RESPONSE_COMMAND，调用processResponseCommand方法</li>
</ul>
<p>NettyRemotingAbstract用<code>processorTable</code>变量记录了请求编码、处理器、线程池之间的关系。</p>
<ul>
<li>每个请求编码都对应了一种唯一的处理器，相同请求编码的处理器是相同的</li>
<li>由于处理器与线程池组成一对，所以相同请求编码的请求在相同的线程池中执行</li>
</ul>
<p>不同的请求编码在不同的线程池中运行，以发送消息和消费消息为例：</p>
<table>
<thead>
<tr>
<th>请求编码（request code）</th>
<th>处理器</th>
<th>线程池</th>
</tr>
</thead>
<tbody>
<tr>
<td>SEND_MESSAGE</td>
<td>SendMessageProcessor</td>
<td>ExecutorService#1</td>
</tr>
<tr>
<td>GET_MESSAGE</td>
<td>PullMessageProcessor</td>
<td>ExecutorService#2</td>
</tr>
</tbody>
</table>
<p>以经典的RPC通信模型来看，客户端向服务端发起RPC调用请求。那么<code>processorTable</code>主要针对服务端，<code>responseTable</code>则主要针对客户端。</p>
<ul>
<li>客户端发起RPC调动时，会创建异步的响应对象，并放入将opaque和ResponseFuture的映射关系放入responseTable</li>
<li>当客户端收到服务端发送的响应结果后，会将opaque以及ResponseFuture从responseTable中移除</li>
</ul>
<p>那么opaque是如何在请求和响应之间进行关联的呢？下面代码中的注释说明了opaque在请求和响应之间的设置和获取流程。</p>
<blockquote>
<p>opaque表示：请求发起方在同一连接上不同的请求标识代码,多线程连接复用使用</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">protected final HashMap&lt;Integer/* request code */, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt; processorTable =</span><br><span class="line">    new HashMap&lt;Integer, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt;(64);</span><br><span class="line"> protected final ConcurrentMap&lt;Integer /* opaque */, ResponseFuture&gt; responseTable =</span><br><span class="line">    new ConcurrentHashMap&lt;Integer, ResponseFuture&gt;(256);</span><br><span class="line"></span><br><span class="line">public void processMessageReceived(ChannelHandlerContext ctx, RemotingCommand msg) throws Exception &#123;</span><br><span class="line">    final RemotingCommand cmd = msg;</span><br><span class="line">    if (cmd != null) &#123;</span><br><span class="line">        switch (cmd.getType()) &#123;</span><br><span class="line">            case REQUEST_COMMAND:</span><br><span class="line">                processRequestCommand(ctx, cmd);</span><br><span class="line">                break;</span><br><span class="line">            case RESPONSE_COMMAND:</span><br><span class="line">                processResponseCommand(ctx, cmd);</span><br><span class="line">                break;</span><br><span class="line">            default:</span><br><span class="line">                break;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 处理请求，比如服务端处理客户端发送的请求，NettyServerHandler会调用到这里</span><br><span class="line">public void processRequestCommand(final ChannelHandlerContext ctx, final RemotingCommand cmd) &#123;</span><br><span class="line">    final Pair&lt;NettyRequestProcessor, ExecutorService&gt; matched = this.processorTable.get(cmd.getCode());</span><br><span class="line">    // 4. 从请求对象中获取opaque，那么什么时候opaque设置到请求中？</span><br><span class="line">    // 这里的cmd实际上是步骤3的request，因为步骤1已经有opaque，所以这里也能取到opaque</span><br><span class="line">    final int opaque = cmd.getOpaque();</span><br><span class="line">    final RemotingCommand response = pair.getObject1().processRequest(ctx, cmd);</span><br><span class="line">    // 5. 将opaque设置到响应对象中</span><br><span class="line">    response.setOpaque(opaque);</span><br><span class="line">    // 6. 发送响应对象给客户端</span><br><span class="line">    ctx.writeAndFlush(response);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 处理响应，比如客户端处理服务端发送的响应，NettyClientHandler会调用到这里</span><br><span class="line">public void processResponseCommand(ChannelHandlerContext ctx, RemotingCommand cmd) &#123;</span><br><span class="line">    // 7. 从响应对象中获取opaque，那么什么时候opaque设置到响应里？答案在步骤5中</span><br><span class="line">    // 这里的cmd是步骤5的response，而response的opaque来自于request</span><br><span class="line">    final int opaque = cmd.getOpaque();</span><br><span class="line">    // 8. 根据opaque从responseTable中获取出对应的ResponseFuture</span><br><span class="line">    final ResponseFuture responseFuture = responseTable.get(opaque);</span><br><span class="line">    if (responseFuture != null) &#123;</span><br><span class="line">        responseFuture.setResponseCommand(cmd);</span><br><span class="line">        responseFuture.release();</span><br><span class="line">        // 9. 将opaque与ResponseFuture的映射关系从responseTable中移除，与步骤2互相对应</span><br><span class="line">        responseTable.remove(opaque);</span><br><span class="line">        // 执行客户端在发送RPC调用时定义的回调函数</span><br><span class="line">        if (responseFuture.getInvokeCallback() != null) &#123;</span><br><span class="line">            executeInvokeCallback(responseFuture);</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            responseFuture.putResponse(cmd);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 客户端发起RPC调用</span><br><span class="line">public RemotingCommand invokeSyncImpl(final Channel channel, final RemotingCommand request) &#123;</span><br><span class="line">    // 1. 从请求中获取opaque</span><br><span class="line">    final int opaque = request.getOpaque();</span><br><span class="line">    final ResponseFuture responseFuture = new ResponseFuture(opaque, timeoutMillis, null, null);</span><br><span class="line">    // 2. 创建ResponseFuture，并记录到responseTable</span><br><span class="line">    this.responseTable.put(opaque, responseFuture);</span><br><span class="line">    final SocketAddress addr = channel.remoteAddress();</span><br><span class="line">    // 3. 发起RPC调用</span><br><span class="line">    channel.writeAndFlush(request);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="生产者发送消息">生产者发送消息</h2><p>以example/quickstart下的Producer发送消息为例，入口方法走到DefaultMQProducerImpl的sendDefaultImpl()方法。</p>
<p>发送消息过程涉及下面几个步骤：</p>
<ul>
<li>tryToFindTopicPublishInfo()：根据消息的Topic获取TopicPublishInfo</li>
<li>selectOneMessageQueue()：选择一个MessageQueue</li>
<li>sendKernelImpl()：调用内核的发送方法</li>
<li>如果是同步调用，返回SendResult，否则返回空</li>
</ul>
<p>接下来进入DefaultMQProducerImpl的内核发送方法，主要的参数有：Message、MessageQueue、TopicPublishInfo</p>
<ul>
<li>如果有Hook，构造SendMessageContext，将Message、MessageQueue等都设置为上下文对象的成员变量</li>
<li>构造SendMessageRequestHeader</li>
<li>从MQClientFactory获取getMQClientAPIImpl()实现类MQClientAPIImpl，调用sendMessage()方法</li>
</ul>
<p>接下来进入MQClientAPIImpl的sendMessage()方法</p>
<ul>
<li>根据RequestCode.SEND_MESSAGE（请求编码）和SendMessageRequestHeader（请求头）创建RemotingCommand对象</li>
<li>设置请求的body为消息内容：request.setBody(msg.getBody())</li>
<li>调用remotingClient.invokeAsync()或者invokeSync()方法</li>
<li>对于同步调用，因为要等待结果返回，所以会立即调用processSendResponse()</li>
<li>processSendResponse()方法返回一个SendResult对象</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">private SendResult sendMessageSync(</span><br><span class="line">    final String addr, // Broker的地址</span><br><span class="line">    final String brokerName, // Broker的名字</span><br><span class="line">    final Message msg, // 消息内容</span><br><span class="line">    final long timeoutMillis,</span><br><span class="line">    final RemotingCommand request // 请求对象</span><br><span class="line">) &#123;</span><br><span class="line">    // RPC调用示例，这里的客户端是生产者，通过MQClientAPIImpl调用</span><br><span class="line">    RemotingCommand response = this.remotingClient.invokeSync(addr, request, timeoutMillis);</span><br><span class="line">    assert response != null;</span><br><span class="line">    return this.processSendResponse(brokerName, msg, response);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>生产者通过MQClientAPIImpl发起RPC调用，request请求对象的编码是SEND_MESSAGE。这里的地址指的是Broker的地址，而不是NameServer。<br>虽然生产者连接的是NameServer，但这中间会有选择MessageQueue，再选择Broker的过程，由于这里先关注整体的流程，暂时不去分析具体的细节。</p>
<p>客户端通过<code>RemotingClient</code>调用了服务端Broker，接下来看服务端<code>BrokerController</code>的处理。</p>
<p>BrokerController启动时会为各种请求类型注册不同的请求处理器，比如SEND_MESSAGE注册了SendMessageProcessor处理器：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public void registerProcessor() &#123;</span><br><span class="line">    SendMessageProcessor sendProcessor = new SendMessageProcessor(this);</span><br><span class="line">    // SendMessageProcessor有两个Hook：发送消息和消费消息的Hook。</span><br><span class="line">    sendProcessor.registerSendMessageHook(sendMessageHookList);</span><br><span class="line">    sendProcessor.registerConsumeMessageHook(consumeMessageHookList);</span><br><span class="line">    this.remotingServer.registerProcessor(RequestCode.SEND_MESSAGE, sendProcessor, this.sendMessageExecutor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>SendMessageProcessor的processRequest()方法会处理生产者客户端发送的SEND_MESSAGE请求。</p>
<p>客户端在发送请求之前构建了<code>SendMessageContext</code>和<code>SendMessageRequestHeader</code>，这里对应的会首先从RemotingCommand反解析出着两个对象</p>
<ul>
<li>解析请求的body，创建MessageExtBrokerInner对象</li>
<li>获取MessageStore，并调用putMessage方法，传入MessageExtBrokerInner对象</li>
<li>返回PutMessageResult，并调用handlePutMessageResult方法</li>
<li>最后返回的是一个RemotingCommand响应对象，会返回给客户端</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public RemotingCommand processRequest(ChannelHandlerContext ctx,</span><br><span class="line">    RemotingCommand request) throws RemotingCommandException &#123;</span><br><span class="line">    SendMessageContext mqtraceContext;</span><br><span class="line">    switch (request.getCode()) &#123;</span><br><span class="line">        case RequestCode.CONSUMER_SEND_MSG_BACK:</span><br><span class="line">            return this.consumerSendMsgBack(ctx, request);</span><br><span class="line">        default: // SEND_MESSAGE的处理逻辑...</span><br><span class="line">            SendMessageRequestHeader requestHeader = parseRequestHeader(request);</span><br><span class="line">            mqtraceContext = buildMsgContext(ctx, requestHeader);</span><br><span class="line">            this.executeSendMessageHookBefore(ctx, request, mqtraceContext);</span><br><span class="line">            RemotingCommand response;</span><br><span class="line">            if (requestHeader.isBatch()) &#123;</span><br><span class="line">                response = this.sendBatchMessage(ctx, request, mqtraceContext, requestHeader);</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                response = this.sendMessage(ctx, request, mqtraceContext, requestHeader);</span><br><span class="line">            &#125;</span><br><span class="line">            this.executeSendMessageHookAfter(response, mqtraceContext);</span><br><span class="line">            return response;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来进入DefaultMessageStore的putMessage()方法，这个方法会调用CommitLog的putMessage()方法</p>
<ul>
<li>BrokerController和SendMessageProcessor都在broker模块</li>
<li>MessageStore和CommitLog则在store模块</li>
</ul>
<p>CommitLog首先获取最近的MappedFile，然后追加消息到映射文件中。</p>
<ul>
<li>追加消息的回调类DefaultAppendMessageCallback是执行数据写入文件的真正方法。</li>
<li>追加完成后，有多种的磁盘刷写方式，比如同步和异步</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">public PutMessageResult putMessage(final MessageExtBrokerInner msg) &#123;</span><br><span class="line">    MappedFile mappedFile = this.mappedFileQueue.getLastMappedFile();</span><br><span class="line">    AppendMessageResult result = mappedFile.appendMessage(msg, this.appendMessageCallback);</span><br><span class="line">    PutMessageResult putMessageResult = new PutMessageResult(PutMessageStatus.PUT_OK, result);</span><br><span class="line">    handleDiskFlush(result, putMessageResult, msg);</span><br><span class="line">    handleHA(result, putMessageResult, msg);</span><br><span class="line">    return putMessageResult;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>同样，我们省略了具体写入到CommitLog中的细节，以及如何处理磁盘的刷写、HA等细枝末节。实际上，到这里为止，<br>生产者客户端发起RPC调用，到服务端处理请求，服务端返回响应，客户端接收响应结果，这个过程已经分析完毕了。</p>
<h2 id="Pull_Consumer">Pull Consumer</h2><p>PULL_MESSAGE对应的处理器是PullMessageProcessor。与生产消息调用MessageStore的putMessage()类似，<br>消费消息调用MessageStore的getMessage()方法，并返回GetMessageResult。</p>
<table>
<thead>
<tr>
<th>请求编码</th>
<th>消息处理器</th>
<th>消息存储</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>SEND_MESSAGE</td>
<td>SendMessageProcessor</td>
<td>putMessage()</td>
<td>PutMessageResult</td>
</tr>
<tr>
<td>PULL_MESSAGE</td>
<td>PullMessageProcessor</td>
<td>getMessage()</td>
<td>GetMessageResult</td>
</tr>
</tbody>
</table>
<p>消费者还需要提交偏移量，对应ConsumerOffsetManager的commitOffset()方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">private RemotingCommand processRequest(final Channel channel, RemotingCommand request, boolean brokerAllowSuspend) &#123;</span><br><span class="line">    final GetMessageResult getMessageResult = this.brokerController.getMessageStore().getMessage(</span><br><span class="line">            requestHeader.getConsumerGroup(),   // 消费组</span><br><span class="line">            requestHeader.getTopic(),           // 主题</span><br><span class="line">            requestHeader.getQueueId(),         // 队列编号</span><br><span class="line">            requestHeader.getQueueOffset(),     // 队列的偏移量</span><br><span class="line">            requestHeader.getMaxMsgNums(),      // 最大的消息数量</span><br><span class="line">            messageFilter);                     // 过滤器</span><br><span class="line">    // .......................................................</span><br><span class="line">    if (storeOffsetEnable) &#123;</span><br><span class="line">        this.brokerController.getConsumerOffsetManager().commitOffset(</span><br><span class="line">            RemotingHelper.parseChannelRemoteAddr(channel),</span><br><span class="line">            requestHeader.getConsumerGroup(), </span><br><span class="line">            requestHeader.getTopic(), </span><br><span class="line">            requestHeader.getQueueId(), </span><br><span class="line">            requestHeader.getCommitOffset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>存储层设计到文件操作时，生产消息会写到CommitLog，消费消息则会调用getMessage方法，给定偏移量和大小。</p>
<h1 id="设计">设计</h1><h2 id="架构">架构</h2><p><img src="http://rocketmq.apache.org/assets/images/rmq-basic-arc.png" alt="arch"></p>
<ul>
<li><strong>NameServer Cluster</strong>：<br>Name Servers provide lightweight <em>service discovery and routing</em>.<br>Each Name Server records full routing information（路由信息表）,<br>provides corresponding reading and writing service,<br>and supports fast storage expansion.</li>
<li><strong>Broker Cluster</strong>：<br>Brokers take care of message storage by providing lightweight TOPIC and QUEUE mechanisms.<br>They support the Push and Pull model, contains fault tolerance mechanism (2 copies or 3 copies),<br>and provides strong padding of peaks and capacity of accumulating hundreds of billion messages in their original time order.<br>In addition, Brokers provide disaster recovery, rich metrics statistics, and alert mechanisms, all of which are lacking in traditional messaging systems.</li>
<li><strong>Producer Cluster</strong>：<br>Producers support distributed deployment.<br>Distributed Producers send messages to the Broker cluster through multiple load balancing modes.<br>The sending processes support fast failure and have low latency.</li>
<li><strong>Consumer Cluster</strong>：<br>Consumers support distributed deployment in the Push and Pull model as well.<br>It also supports cluster consumption（集群消费） and message broadcasting（消息广播）.<br>It provides real-time message subscription mechanism and can meet most consumer requirements. R</li>
</ul>
<p>NameServer is a fully functional server, which mainly includes two features:</p>
<ul>
<li>Broker Management, NameServer accepts the <em>register</em> from Broker cluster and provides <em>heartbeat</em> mechanism to check whether a broker is alive.</li>
<li>Routing Management, each NameServer will hold whole <em>routing info</em> about the broker cluster and the <em>queue info</em> for clients query.</li>
</ul>
<p>Broker server is responsible for message store and delivery, message query, HA guarantee, and so on.</p>
<ul>
<li>Remoting Module, the entry of broker, handles the requests from clients（处理客户端请求）.</li>
<li>Client Manager, manages the clients (Producer/Consumer) and maintains topic subscription of consumer（维护消费者的主题订阅）.</li>
<li>Store Service, provides simple APIs to store or query message in physical disk（磁盘文件存储和查询消息）.</li>
<li>HA Service, provides data sync feature between master broker and slave broker（主从节点的数据同步）.</li>
<li>Index Service, builds index for messages by specified key and provides quick message query（构建消息索引）.</li>
</ul>
<p>Name server follows the share-nothing design paradigm. Brokers send heartbeat data to all name servers.<br>Producers and consumers can query meta data from any of name servers available while sending / consuming messages.</p>
<p>Brokers can be divided into two categories according to their roles: master and slave.<br>Master brokers provide <em>RW</em> access while slave brokers only accept <em>read</em> access.</p>
<p>To deploy a high-availability RocketMQ cluster with no single point of failure, a series of broker sets should be deployed.<br>A broker set contains one master with brokerId set to 0 and several slaves with non-zero brokerIDs.<br>All of the brokers in one set have the same brokerName. In serious scenarios,<br>we should have at least two brokers in one broker set. Each topic resides in two or more brokers.</p>
<p>Broker is a major component of the RocketMQ system.<br>It receives messages sent from producers, store them and prepare to handle pull requests from consumers.<br>It also stores message related meta data, including consumer groups, consuming progress offsets and topic / queue info.</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021122552876" alt="mqs"></p>
<h2 id="物理部署结构（服务端）">物理部署结构（服务端）</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019140724552" alt="brokers"></p>
<p>Name Server 是一个几乎无状态节点,可集群部署,节点之间无任何信息同步。</p>
<p>Broker 部署相对复杂,Broker 分为 Master 与 Slave,<br>一个 Master 可以对应多个 Slave, 但是一个 Slave 只能对应一个 Master,<br>Master 与 Slave 的对应关系通过指定相同的 BrokerName,不同的 BrokerId 来定义,<br>BrokerId为 0 表示 Master,非 0 表示 Slave。Master 也可以部署多个。<br><strong>每个 Broker 与 Name Server 集群中的所有节点建立长连接,定时注册 Topic 信息到所有 Name Server。</strong></p>
<p>Producer 与 Name Server 集群中的其中一个节点(随机选择)建立长连接,<br>定期从 Name Server 取 Topic 路由信息,<br>并<strong>向提供 Topic 服务的 Master 建立长连接</strong>,<br>且定时向 Master 发送心跳。<br>Producer 完全无状态,可集群部署。  </p>
<p>Consumer 与 Name Server 集群中的其中一个节点(随机选择)建立长连接,<br>定期从 Name Server 取 Topic 路由信息,<br>并<strong>向提供 Topic 服务的 Master、Slave 建立长连接</strong>,<br>且定时向 Master、Slave 发送心跳。<br>Consumer 既可以从 Master 订阅消息,也可以从 Slave 订阅消息,订阅规则由 Broker 配置决定。</p>
<h2 id="逻辑部署结构（客户端）">逻辑部署结构（客户端）</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019141837964" alt="logical"></p>
<p><strong>Producer Group</strong> 用来表示一个发送消息应用,一个 Producer Group 下包含多个 Producer 实例,<br>可以是多台机器,也可以是一台机器的多个进程,或者一个进程的多个 Producer 对象。<br>一个 Producer Group 可以发送多个 Topic 消息,Producer Group 作用如下:  </p>
<ol>
<li>标识一类 Producer</li>
<li>可以通过运维工具查询这个发送消息应用下有多个 Producer 实例</li>
<li>发送分布式事务消息时,如果 Producer 中途意外宕机,Broker 会主动回调 Producer Group 内的任意一台机器来确认事务状态</li>
</ol>
<p><strong>Consumer Group</strong> 用来表示一个消费消息应用,一个 Consumer Group 下包含多个 Consumer 实例,<br>可以是多台机器,也可以是多个进程,或者是一个进程的多个 Consumer 对象。<br>一个 Consumer Group 下的多个 Consumer 以均摊/集群（CLUSTER）方式消费消息,<br>如果设置为广播方式(BROADCAST),那么这个 Consumer Group 下的每个实例都消费全量数据。  </p>
<h2 id="存储结构">存储结构</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019195642571" alt="store"></p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019195656785" alt="store"></p>
<ol>
<li>所有数据单独存储到一个 Commit Log,完全顺序写,随机读。<br>RocketMQ 的所有消息都是持久化,先写入系统 PAGECACHE,然后刷盘,可以保证内存不磁盘都有一份数据, 访问时,直接从内存读叏。</li>
<li>对最终用户展现的队列(ConsumeQueue)实际只存储消息在CommitLog的位置信息,并且串行方式刷盘。</li>
<li>消费者的读取流程是：先读ConsumeQueue,再读CommitLog</li>
<li>由于ConsumeQueue存储数据量极少,并且是顺序读,在PAGECACHE预读作用下,<br>ConsumeQueue的读性能几乎与内存一致,即使堆积情况下。所以可认为 Consume Queue 完全不会阻碍读性能。</li>
<li>要保证CommitLog与ConsumeQueue完全的一致,增加了编程的复杂度。<br>Commit Log 中存储了所有的元信息,包含消息体,类似于 Mysql、Oracle 的 redolog,<br>所以只要有 Commit Log 在,Consume Queue 即使数据丢失,仍然可以恢复出来。  </li>
</ol>
<p>总结一句话：生产消息时先写入PageCache，然后刷写到磁盘。  </p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019201635395" alt="pagecache"></p>
<p>同步刷盘与异步刷盘的唯一区别是异步刷盘写完 PAGECACHE 直接返回,而同步刷盘需要等待刷盘完成才返回, 同步刷盘流程如下:  </p>
<ol>
<li>写入 PAGECACHE 后,线程等待,通知刷盘线程刷盘。</li>
<li>刷盘线程刷盘后,唤醒前端等待线程,可能是一批线程。</li>
<li>前端等待线程向用户返回成功。</li>
</ol>
<p>读取消息的ConsumeQueue文件也会加载到PageCache，读PageCache和内存速度差不多。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021085345842" alt="pc"></p>
<ol>
<li>Producer 发送消息,消息从 socket 进入 java 堆。</li>
<li>Producer 发送消息,消息从 java 堆转入 PAGACACHE,物理内存。</li>
<li>Producer 发送消息,由异步线程刷盘,消息从 PAGECACHE 刷入磁盘。</li>
<li>Consumer 拉消息(正常消费),消息直接从PAGECACHE(数据在物理内存)转入socket,到达consumer,不经过 java 堆。<br>这种消费场景最多,线上 96G 物理内存,按照 1K 消息算,可以在物理内存缓存 1 亿条消息。</li>
<li>Consumer 拉消息(异常消费),消息直接从 PAGECACHE(数据在虚拟内存)转入 socket。</li>
<li>Consumer 拉消息(异常消费),由于 socket 访问了虚拟内存,产生缺页中断,此时会产生磁盘 IO,<br>从磁盘 Load 消息到 PAGECACHE,然后直接从 socket 发出去。</li>
<li>同5</li>
<li>同6</li>
</ol>
<h2 id="负载均衡（7-8/7-9）">负载均衡（7.8/7.9）</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021145704818" alt="loadbalance"></p>
<h2 id="消息查询(TODO_7-3)">消息查询(TODO 7.3)</h2><h3 id="按照MessageId查询">按照MessageId查询</h3><h3 id="按照MessageKey查询">按照MessageKey查询</h3><h2 id="消息过滤(TODO_7-4)">消息过滤(TODO 7.4)</h2><p>有两种类型的消息过滤：</p>
<ul>
<li>Broker 端消息过滤：在 Broker 中,按照 Consumer 的要求做过滤,优点是减少了对于 Consumer 无用消息的网络传输。缺点是增加了 Broker 的负担,实现相对复杂。</li>
<li>Consumer 端消息过滤：这种过滤方式可由应用完全自定义实现,但是缺点是很多无用的消息要传输到 Consumer 端。</li>
</ul>
<h2 id="长轮询Pull(TODO_7-5)">长轮询Pull(TODO 7.5)</h2><p>RocketMQ 的 Consumer 都是从 Broker 拉消息来消费,但是为了能做到实时收消息,<br>RocketMQ 使用长轮询方式,可以保证消息实时性同 Push 方式一致。简单说就是<strong>长轮询Pull = Push</strong>。</p>
<h2 id="顺序消息(TODO_7-6)">顺序消息(TODO 7.6)</h2><p>消息有序指的是一类消息消费时,能按照发送的顺序来消费。<br>例如:一个订单产生了 3 条消息,分别是订单创建,订单付款,订单完成。<br>消费时,要按照这个顺序消费才能有意义。但是同时订单之间是可以并行消费的。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021144845680" alt="order"></p>
<p>缺点：</p>
<ul>
<li>发送顺序消息无法利用集群 FailOver 特性<br>􏰀- 消费顺序消息的并行度依赖于队列数量（MessageQueue的数量）<br>􏰀- 队列热点问题,个别队列由于哈希不均导致消息过多,消费速度跟不上,产生消息堆积问题 􏰀 </li>
<li>遇到消息失败的消息,无法跳过,当前队列消费暂停（等一段时间再消费）</li>
</ul>
<h2 id="消费线程(单队列并行消费,_7-10)">消费线程(单队列并行消费, 7.10)</h2><p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021133510386" alt="singlequeue"></p>
<p>单队列并行消费采用滑动窗口方式并行消费,如图所示,3~7的消息在一个滑动窗口区间,可以有多个线程并行消费,但是每次提交的 Offset 都是最小 Offset,例如 3。</p>
<p>修改消费并行度的两种方法：</p>
<ol>
<li>同一个 ConsumerGroup 下,通过增加 Consumer 实例数量来提高并行度（超过订阅队列数的 Consumer 实例无效）。<br>可以通过加机器,或者在已有机器启动多个进程的方式。</li>
<li>提高单个 Consumer 的消费并行线程,通过修改两个参数：consumeThreadMin/consumeThreadMax。</li>
</ol>
<p>批量方式消费：</p>
<p>某些业务流程如果支持批量方式消费,则可以很大程度上提高消费吞吏量,例如订单扣款类应用,<br>一次处理一个订单耗时 1 秒钟,一次处理 10 个订单可能也只耗时 2 秒钟,这样即可大幅度提高消费的吞吏量。<br>通过设置 consumer 的 consumeMessageBatchMaxSize 返个参数,<br>默认是 1,即一次只消费一条消息,例如设置为 N,那么每次消费的 消息数小于等于 N。</p>
<h2 id="消息堆积、消息重试">消息堆积、消息重试</h2><ul>
<li>消息堆积（4.12）和消息重试（4.15）</li>
<li>解决办法（7.15）</li>
<li>跳过非重要消息（14.3）</li>
</ul>
<h2 id="事务(TODO)">事务(TODO)</h2><p>分布式事务涉及到两阶段提交问题,在数据存储方面的方面必然需要 KV 存储的支持,<br>因为第二阶段的提交回滚需要修改消息状态,一定涉及到根据 Key 去查找 Message 的动作。<br>RocketMQ 在第二阶段绕过了根据 Key 去查找 Message 的问题,<br>采用第一阶段发送 Prepared 消息时,拿到了消息的 Offset,<br>第二阶段通过 Offset 去访问消息, 并修改状态,Offset 就是数据的地址。</p>
<p>RocketMQ 这种实现事务方式,没有通过 KV 存储做,而是通过 Offset 方式,<br>存在一个显著缺陷,即通过 Offset 更改数据,会令系统的脏页过多,需要特别关注。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171021151527711" alt="trans"></p>
<h3 id="Producer_Group">Producer Group</h3><p>Producers of the same role are grouped together.<br>A different producer instance of the same producer group<br>may be contacted by a broker to commit or roll back a transaction<br>in case the original producer crashed after the transaction.</p>
<p>Warning: Considering the provided producer is sufficiently powerful at sending messages,<br>only one instance is allowed per producer group to avoid unnecessary initialization of producer instances.</p>
<h2 id="扩容">扩容</h2><p>扩容是整个系统中的很重要的一个环节。在保证顺序的情况下进行扩容的难度会更大。<br>基本的策略是让向一个队列写入数据的消息发送者能够知道应该把消息写入迁移到新的队列中，<br>并且需要让消息的订阅者知道，当前的队列消费完数据后需要迁移到新队列去消费消息。关键点如下:</p>
<ul>
<li>原队列在开始扩容后需要有一个标志，即便有新消息过来，也不再接收。</li>
<li>通知消息发送端新的队列的位置。</li>
<li>对于消息接受端，对原来队列的定位会收到新旧两个位置，当旧队列的数据接受完毕后，则会只关心新队列的位置，完成切换。</li>
</ul>
<p>那么对于Metaq顺序消息，如何做到不停写扩容呢？我说说自己的看法：<br>在队列扩容的时候考虑到需要处理最新的消息服务，为了不丢失这部分消息，<br>可以采取让Producer暂存消息在本地磁盘设备中，<br>等扩容完成后再与Broker交互。这是我目前能想到的不停写扩容方式。</p>
<h1 id="参考文档">参考文档</h1><ul>
<li>RocketMQ原理简介</li>
<li>RocketMQ开发指南</li>
<li>&lt;pull类型消息中间件-消息服务端(三)&gt;(<a href="http://www.cnblogs.com/zhulongchao/p/5792770.html" target="_blank" rel="noopener">http://www.cnblogs.com/zhulongchao/p/5792770.html</a>)</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache RocketMQ: &lt;a href=&quot;http://rocketmq.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://rocketmq.apache.org/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="mq" scheme="http://github.com/zqhxuyuan/categories/mq/"/>
    
    
      <category term="rocketmq" scheme="http://github.com/zqhxuyuan/tags/rocketmq/"/>
    
  </entry>
  
  <entry>
    <title>深入解析中间件之-TCC事务</title>
    <link href="http://github.com/zqhxuyuan/2017/10/18/Midd-TCC-Transactions/"/>
    <id>http://github.com/zqhxuyuan/2017/10/18/Midd-TCC-Transactions/</id>
    <published>2017-10-17T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.372Z</updated>
    
    <content type="html"><![CDATA[<p>TCC<br><a id="more"></a></p>
<p>重拾了一把JavaWeb的部署流程，使用Jetty在Idea专业版上运行。步骤如下：</p>
<ol>
<li>本地下载Jetty，Idea启用Jetty Intergration插件（注意：不是Jetty Runner）</li>
<li>在Run Configuration中选择Jetty Server/Local，配置本地的Jetty服务器（图1）</li>
<li>新建Jetty Server，在Deployment中添加web工程的war exploded（图2）</li>
<li>保存后，点击右上角的三角箭头启动Jetty</li>
</ol>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171124234107133" alt="1"></p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171124234123638" alt="2"></p>
<p>坑爹的是由于有三个工程，第二个Jetty工程即使在vm.options中添加-Djetty.http.port=8081，使用的还是8080端口</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171124234137287" alt="3"></p>
<p>遂放弃，采用mvn jetty:run的方式</p>
<p>在根pom.xml中添加jetty-plugin的配置（注意：不需要在capital/order/red下添加）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;jetty-maven-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;9.4.7.v20170914&lt;/version&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure>
<p>然后进入到tcc-transaction的根目录（注意不要进入到实际的capital/order/red等目录）分别执行（开三个终端）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">➜  tcc-transaction git:(master-1.2.x) ✗ mvn jetty:run -projects tcc-transaction-tutorial-sample/tcc-transaction-http-sample/tcc-transaction-http-capital -am</span><br><span class="line">➜  tcc-transaction git:(master-1.2.x) ✗ mvn -Djetty.http.port=8088 jetty:run -projects tcc-transaction-tutorial-sample/tcc-transaction-http-sample/tcc-transaction-http-redpacket -am</span><br><span class="line">➜  tcc-transaction git:(master-1.2.x) ✗ mvn -Djetty.http.port=8086 jetty:run -projects tcc-transaction-tutorial-sample/tcc-transaction-http-sample/tcc-transaction-http-order -am</span><br></pre></td></tr></table></figure>
<p>解释下这里的参数含义：</p>
<ul>
<li>-Djetty.http.port=8088表示web端口，端口不能相同，默认是8080</li>
<li>通过-projects定位到具体的web子项目</li>
<li>最后还要加上-am，表示会编译相关的依赖模块</li>
</ul>
<p>比如capital的依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[INFO] tcc-transaction</span><br><span class="line">[INFO] tcc-transaction-api</span><br><span class="line">[INFO] tcc-transaction-core</span><br><span class="line">[INFO] tcc-transaction-spring</span><br><span class="line">[INFO] tcc-transaction-tutorial-sample</span><br><span class="line">[INFO] tcc-transaction-http-sample</span><br><span class="line">[INFO] tcc-transaction-http-capital-api</span><br><span class="line">[INFO] tcc-transaction-http-capital</span><br></pre></td></tr></table></figure>
<p>red的依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[INFO] tcc-transaction</span><br><span class="line">[INFO] tcc-transaction-api</span><br><span class="line">[INFO] tcc-transaction-core</span><br><span class="line">[INFO] tcc-transaction-spring</span><br><span class="line">[INFO] tcc-transaction-tutorial-sample</span><br><span class="line">[INFO] tcc-transaction-http-sample</span><br><span class="line">[INFO] tcc-transaction-http-redpacket-api</span><br><span class="line">[INFO] tcc-transaction-http-redpacket</span><br></pre></td></tr></table></figure>
<p>order的依赖：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[INFO] tcc-transaction</span><br><span class="line">[INFO] tcc-transaction-api</span><br><span class="line">[INFO] tcc-transaction-core</span><br><span class="line">[INFO] tcc-transaction-spring</span><br><span class="line">[INFO] tcc-transaction-tutorial-sample</span><br><span class="line">[INFO] tcc-transaction-http-sample</span><br><span class="line">[INFO] tcc-transaction-http-capital-api</span><br><span class="line">[INFO] tcc-transaction-http-redpacket-api</span><br><span class="line">[INFO] tcc-transaction-http-order</span><br></pre></td></tr></table></figure>
<p>如果没有报错，会输出下面类似的启动成功日志（以order的8086端口为例）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[INFO] Started ServerConnector@6bc6692e&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:8086&#125;</span><br><span class="line">[INFO] Started @17821ms</span><br><span class="line">[INFO] Started Jetty Server</span><br></pre></td></tr></table></figure>
<p>既然命令行方式启动，也可以通过Idea的maven插件代替执行：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171125105731746" alt="5"></p>
<p>但是要使用Debug时，还是会出现地址已经被使用的情况。可以通过在命令行启动mvnDebug：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">➜  tcc-transaction git:(master-1.2.x) ✗ mvnDebug -Djetty.http.port=8086 jetty:run -projects tcc-transaction-tutorial-sample/tcc-transaction-http-sample/tcc-transaction-http-order -am</span><br><span class="line">Preparing to Execute Maven in Debug Mode</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0</span><br><span class="line">Listening for transport dt_socket at address: 8000</span><br><span class="line"></span><br><span class="line">//当下一步执行Debug后，在终端这里会打印日志：</span><br><span class="line">...</span><br><span class="line">[INFO] Started ServerConnector@74d776fb&#123;HTTP/1.1,[http/1.1]&#125;&#123;0.0.0.0:8086&#125;</span><br><span class="line">[INFO] Started @113766ms</span><br><span class="line">[INFO] Started Jetty Server</span><br></pre></td></tr></table></figure>
<p>然后在Idea中配置Remote，保存后，在右上角点击Debug（也只有Debug，无法选Run）：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171125105941840" alt="6"></p>
<p>打开order的页面，这里是<a href="http://localhost:8086" target="_blank" rel="noopener">http://localhost:8086</a></p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171124235054711" alt="4"></p>
<p>购买一个IPhonx后，三个终端的日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//资金终端</span><br><span class="line">capital try record called. time seq:2017-11-24 23:26:29</span><br><span class="line">capital confirm record called. time seq:2017-11-24 23:26:33</span><br><span class="line">capital try record called. time seq:2017-11-24 23:26:50</span><br><span class="line"></span><br><span class="line">//红包终端</span><br><span class="line">red packet try record called. time seq:2017-11-24 23:26:31</span><br><span class="line">red packet confirm record called. time seq:2017-11-24 23:26:34</span><br><span class="line"></span><br><span class="line">//订单终端</span><br><span class="line">order try make payment called.time seq:2017-11-24 23:26:28</span><br><span class="line">order confirm make payment called. time seq:2017-11-24 23:26:32</span><br><span class="line">order try make payment called.time seq:2017-11-24 23:26:49</span><br></pre></td></tr></table></figure>
<p>再买一个Mac，资金都不够了，最终订单失败：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//资金终端</span><br><span class="line">java.lang.RuntimeException: not enough balance!</span><br><span class="line">capital cancel record called. time seq:2017-11-24 23:26:52</span><br><span class="line"></span><br><span class="line">//红包终端没有新的输出</span><br><span class="line"></span><br><span class="line">//订单终端</span><br><span class="line">java.lang.RuntimeException: not enough balance!</span><br><span class="line">order cancel make payment called.time seq:2017-11-24 23:26:51</span><br></pre></td></tr></table></figure>
<p>数据库信息：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171125114005926" alt="6"></p>
<p>TCC的事务调用流程设计本地事务和远程事务、根事务与分支事务，并且还有一个Proxy代理层。<br>本地事务、代理事务、远程事务都加上了@Conpensable注解，并且都定义了try/confirm/cancel方法。<br>为了弄清楚各种事务的调用链，在相关代码上加上日志（补偿事务、资源协调者、业务类）：</p>
<p>sample-http-order（订单主事务）: </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">1.根事务（订单）的两个拦截器</span><br><span class="line">16:48:22,476 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:ROOT</span><br><span class="line">16:48:22,548 [CompensableTransactionInterceptor] ⭐️root transaction begin, participants size:0</span><br><span class="line">16:48:22,607 [ResourceCoordinatorInterceptor] 添加参与者到事务中,participant:Participant@23b7d68f,当前参与者数量:1</span><br><span class="line">16:48:22,610 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,状态:TRYING,参与者数量:1</span><br><span class="line">16:48:22,835 [PaymentServiceImpl] ⭐️order try make payment called.time seq:2017-11-26 16:48:22</span><br><span class="line"></span><br><span class="line">2.分支事务1（账户代理）的两个拦截器。这里会先远程RPC调用账户远程事务，完成后，才会接着执行分支事务2</span><br><span class="line">16:48:22,843 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:NORMAL</span><br><span class="line">16:48:22,871 [ResourceCoordinatorInterceptor] 添加参与者到事务中,participant:Participant@a03f70e,当前参与者数量:2</span><br><span class="line">16:48:22,871 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,状态:TRYING,参与者数量:2</span><br><span class="line">16:48:22,891 [TradeOrderServiceProxy] ⭐️capital proxy record..事务状态:TRYING</span><br><span class="line"></span><br><span class="line">4.分支事务2（红包代理）的两个拦截器。等待账户分支事务的try方法完成后（包括RPC调用），才会开始分支事务2</span><br><span class="line">16:48:24,321 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:NORMAL</span><br><span class="line">16:48:24,345 [ResourceCoordinatorInterceptor] 添加参与者到事务中,participant:Participant@226f03da,当前参与者数量:3</span><br><span class="line">16:48:24,345 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,状态:TRYING,参与者数量:3</span><br><span class="line">16:48:24,346 [TradeOrderServiceProxy] ⭐️redPacket proxy record..事务状态:TRYING</span><br><span class="line"></span><br><span class="line">6.根事务的try方法执行完成，两个分支事务代理以及远程事务的try方法也都完成了</span><br><span class="line">16:48:25,737 [CompensableTransactionInterceptor] root transaction proceed finished!</span><br><span class="line"></span><br><span class="line">7.根事务的commit方法开始。订单（第一个参与者）：23b7d68f，账户（第二个参与者）：a03f70e，红包（最后一个参与者）：226f03da。</span><br><span class="line">16:48:25,737 [CompensableTransactionInterceptor] root transaction commit begins, participants size:3</span><br><span class="line">16:48:25,744 [TransactionManager] 事务状态更新为CONFIRMING,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,参与者数量:3</span><br><span class="line">16:48:25,744 [Transaction] 参与者提交事务,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:78226cf3-348f-327d-a21d-4df15d8a1c0b,participant:Participant@23b7d68f</span><br><span class="line"></span><br><span class="line">8.根事务的confirm方法。注意这里调用参与者的commit方法先从订单（第一个参与者）开始，而不是红包（最后一个参与者）开始。</span><br><span class="line">️16:48:26,746 [PaymentServiceImpl] ⭐️order confirm make payment called. time seq:2017-11-26 16:48:26</span><br><span class="line"></span><br><span class="line">9.分支事务（账户代理，第二个参与者）的confirm方法</span><br><span class="line">16:48:26,756 [Transaction] 参与者提交事务,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:e9e80142-1477-3161-ad50-fc474e70ade1,participant:Participant@a03f70e</span><br><span class="line">16:48:26,756 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:NORMAL</span><br><span class="line">16:48:26,757 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,状态:CONFIRMING,参与者数量:3</span><br><span class="line">16:48:26,757 [TradeOrderServiceProxy] ⭐️capital proxy record..事务状态:CONFIRMING</span><br><span class="line"></span><br><span class="line">11.分支事务（红包代理，第三个参与者）的confirm方法</span><br><span class="line">16:48:27,873 [Transaction] 参与者提交事务,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:16a32620-06a8-3228-8046-e60516206fdd,participant:Participant@226f03da</span><br><span class="line">16:48:27,874 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:NORMAL</span><br><span class="line">16:48:27,874 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:11137112-7fee-3a85-b17d-13cc79e2f02b,状态:CONFIRMING,参与者数量:3</span><br><span class="line">16:48:27,874 [TradeOrderServiceProxy] ⭐️redPacket proxy record..事务状态:CONFIRMING</span><br><span class="line"></span><br><span class="line">13.根事务的commit完成</span><br><span class="line">16:48:28,959 [CompensableTransactionInterceptor] root transaction commit finish, participants size:3</span><br><span class="line">16:48:28,960 [CompensableTransactionInterceptor] root transaction finally, participants size:3</span><br></pre></td></tr></table></figure>
<p>sample-http-capital（资金分支事务）:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">3.远程事务的try方法：</span><br><span class="line">16:48:23,024 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:PROVIDER</span><br><span class="line">16:48:23,068 [CompensableTransactionInterceptor] [TRYING]provider transaction propagationNewBegin, participants size:0</span><br><span class="line">16:48:23,096 [ResourceCoordinatorInterceptor] 添加参与者到事务中,participant:Participant@3d279b8e,当前参与者数量:1</span><br><span class="line">16:48:23,097 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:e9e80142-1477-3161-ad50-fc474e70ade1,状态:TRYING,参与者数量:1</span><br><span class="line">16:48:24,276 [CapitalTradeOrderServiceImpl] ⭐️capital try record called. time seq:2017-11-26 16:48:24TRYING</span><br><span class="line">16:48:24,315 [CompensableTransactionInterceptor] [TRYING]provider transaction proceed finish, participants size:1</span><br><span class="line">16:48:24,315 [CompensableTransactionInterceptor] provider transaction finally, participants size:1</span><br><span class="line"></span><br><span class="line">10.远程事务的confirm方法：</span><br><span class="line">16:48:26,770 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:PROVIDER</span><br><span class="line">16:48:26,771 [CompensableTransactionInterceptor] [CONFIRMING]provider transaction propagationExistBegin, participants size:1</span><br><span class="line">16:48:26,782 [TransactionManager] 事务状态更新为CONFIRMING,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:e9e80142-1477-3161-ad50-fc474e70ade1,参与者数量:1</span><br><span class="line">16:48:26,782 [Transaction] 参与者提交事务,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:f4b5c1ed-f64c-3e1d-b99d-b57bc2449937,participant:Participant@3d279b8e</span><br><span class="line">16:48:27,788 [CapitalTradeOrderServiceImpl] ⭐️capital confirm record called. time seq:2017-11-26 16:48:27CONFIRMING</span><br><span class="line">16:48:27,871 [CompensableTransactionInterceptor] [CONFIRMING]provider transaction commit finish, participants size:1</span><br><span class="line">16:48:27,871 [CompensableTransactionInterceptor] provider transaction finally, participants size:1</span><br></pre></td></tr></table></figure>
<p>sample-http-redpacket（红包分支事务）:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">5.远程事务的try方法：</span><br><span class="line">16:48:24,404 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:PROVIDER</span><br><span class="line">16:48:24,432 [CompensableTransactionInterceptor] [TRYING]provider transaction propagationNewBegin, participants size:0</span><br><span class="line">16:48:24,468 [ResourceCoordinatorInterceptor] 添加参与者到事务中,participant:Participant@403700a,当前参与者数量:1</span><br><span class="line">16:48:24,469 [ResourceCoordinatorInterceptor] 事务xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:16a32620-06a8-3228-8046-e60516206fdd,状态:TRYING,参与者数量:1</span><br><span class="line">16:48:25,705 [RedPacketTradeOrderServiceImpl] ⭐️red packet try record called. time seq:2017-11-26 16:48:25TRYING</span><br><span class="line">16:48:25,732 [CompensableTransactionInterceptor] [TRYING]provider transaction proceed finish, participants size:1</span><br><span class="line">16:48:25,732 [CompensableTransactionInterceptor] provider transaction finally, participants size:1</span><br><span class="line"></span><br><span class="line">12.远程事务的confirm方法：</span><br><span class="line">16:48:27,886 [CompensableTransactionInterceptor] [CompensableTransactionInterceptor拦截器],方法类型:PROVIDER</span><br><span class="line">16:48:27,886 [CompensableTransactionInterceptor] [CONFIRMING]provider transaction propagationExistBegin, participants size:1</span><br><span class="line">16:48:27,895 [TransactionManager] 事务状态更新为CONFIRMING,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:16a32620-06a8-3228-8046-e60516206fdd,参与者数量:1</span><br><span class="line">16:48:27,897 [Transaction] 参与者提交事务,xid:globalTransactionId:c1acaf66-0151-3d16-874d-b89840ba49b5,branchQualifier:4812b875-712e-3cd7-92e5-42c1d8d0d8f1,participant:Participant@403700a</span><br><span class="line">16:48:28,901 [RedPacketTradeOrderServiceImpl] ⭐️red packet confirm record called. time seq:2017-11-26 16:48:28CONFIRMING</span><br><span class="line">16:48:28,935 [CompensableTransactionInterceptor] [CONFIRMING]provider transaction commit finish, participants size:1</span><br><span class="line">16:48:28,935 [CompensableTransactionInterceptor] provider transaction finally, participants size:1</span><br></pre></td></tr></table></figure>
<p>调用图如下：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171126195052058" alt="1"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TCC&lt;br&gt;
    
    </summary>
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/categories/midd/"/>
    
    
      <category term="tcc" scheme="http://github.com/zqhxuyuan/tags/tcc/"/>
    
  </entry>
  
  <entry>
    <title>深入解析中间件之-Dubbo</title>
    <link href="http://github.com/zqhxuyuan/2017/10/18/Midd-Dubbo/"/>
    <id>http://github.com/zqhxuyuan/2017/10/18/Midd-Dubbo/</id>
    <published>2017-10-17T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.368Z</updated>
    
    <content type="html"><![CDATA[<p>Alibaba Dubbo<br><a id="more"></a></p>
<h1 id="QuickStart">QuickStart</h1><p>在IDEA中运行Dubbo的快速入门：</p>
<ol>
<li>不需要编译源码，不需要安装监控中心（dubbo-monitor）和管理中心（dubbo-admin）</li>
<li>安装并启动ZooKeeper，使用ZK作为Dubbo的注册中心</li>
<li>创建dubbo-demo项目，包含三个模块：api、prodvider、consumer</li>
<li>修改provider模块和consumer模块的注册方式使用ZooKeeper：<code>zookeeper://127.0.0.1:2181</code></li>
<li>IDEA中启动provider模块的Provider</li>
<li>IDEA中启动consumer模块的Consumer</li>
</ol>
<p>Provider启动后会一直运行，日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[20/10/17 09:29:07:007 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Register: dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 09:29:07:007 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Subscribe: provider://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 09:29:07:007 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Notify urls for subscribe url provider://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, urls: [empty://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325], dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[09:30:47] Hello world, request from consumer: /10.57.241.44:54685</span><br><span class="line">[20/10/17 09:30:47:047 CST] New I/O server worker #1-1  WARN transport.AbstractServer:  [DUBBO] All clients has discontected from /10.57.241.44:20880. You can graceful shutdown now., dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboServerHandler-10.57.241.44:20880-thread-3  INFO dubbo.DubboProtocol:  [DUBBO] disconected from /10.57.241.44:54685,url:dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;channel.readonly.sent=true&amp;codec=dubbo&amp;dubbo=2.5.6&amp;generic=false&amp;heartbeat=60000&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>Consumer启动后，运行完成，终端就关闭，表示一次RPC调用完成，日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[20/10/17 09:30:45:045 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Register: consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=consumers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:46:046 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Subscribe: consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:46:046 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Notify urls for subscribe url consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, urls: [dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, empty://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, empty://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:46:046 CST] main  INFO transport.AbstractClient:  [DUBBO] Successed connect to server /10.57.241.44:20880 from NettyClient 10.57.241.44 using dubbo version 2.5.6, channel is NettyChannel [channel=[id: 0x0f2ff811, /10.57.241.44:54685 =&gt; /10.57.241.44:20880]], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:46:046 CST] main  INFO transport.AbstractClient:  [DUBBO] Start NettyClient zqhmac/10.57.241.44 connect to the server /10.57.241.44:20880, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:46:046 CST] main  INFO config.AbstractConfig:  [DUBBO] Refer dubbo service com.alibaba.dubbo.demo.DemoService from url zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?anyhost=true&amp;application=demo-consumer&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;remote.timestamp=1508462946325&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">Hello world, response form provider: 10.57.241.44:20880</span><br><span class="line"></span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO config.AbstractConfig:  [DUBBO] Run shutdown hook now., dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO support.AbstractRegistryFactory:  [DUBBO] Close all registries [zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-consumer&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.registry.RegistryService&amp;pid=4324&amp;timestamp=1508463045739], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy registry:zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-consumer&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.registry.RegistryService&amp;pid=4324&amp;timestamp=1508463045739, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy unregister url consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=consumers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy unsubscribe url consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] ZkClient-EventThread-12-127.0.0.1:2181  INFO zkclient.ZkEventThread: Terminate ZkClient event thread.</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO zookeeper.ZooKeeper: Session: 0x15f376495000001 closed</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO dubbo.DubboProtocol:  [DUBBO] Close dubbo connect: /10.57.241.44:54685--&gt;/10.57.241.44:20880, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO netty.NettyChannel:  [DUBBO] Close netty channel [id: 0x0f2ff811, /10.57.241.44:54685 =&gt; /10.57.241.44:20880], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] main-EventThread  INFO zookeeper.ClientCnxn: EventThread shut down for session: 0x15f376495000001</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboSharedHandler-thread-1  INFO dubbo.DubboProtocol:  [DUBBO] disconected from /10.57.241.44:20880,url:dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-consumer&amp;check=false&amp;codec=dubbo&amp;dubbo=2.5.6&amp;generic=false&amp;heartbeat=60000&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;remote.timestamp=1508462946325&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO dubbo.DubboProtocol:  [DUBBO] Close dubbo connect: 10.57.241.44:0--&gt;10.57.241.44:20880, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:30:47:047 CST] DubboShutdownHook  INFO dubbo.DubboProtocol:  [DUBBO] Destroy reference: dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-consumer&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4324&amp;remote.timestamp=1508462946325&amp;side=consumer&amp;timestamp=1508463045694, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line"></span><br><span class="line">Process finished with exit code 0</span><br></pre></td></tr></table></figure>
<p>provider和consumer的注册中心配置都是ZooKeeper，查看ZooKeeper的节点信息。<br>可以看出DemoService的providers目前有<code>dubbo://10.57.241.44:20880</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 12] ls /dubbo/com.alibaba.dubbo.demo.DemoService</span><br><span class="line">[consumers, configurators, routers, providers]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 13] ls /dubbo/com.alibaba.dubbo.demo.DemoService/providers</span><br><span class="line">[dubbo%3A%2F%2F10.57.241.44%3A20880%2Fcom.alibaba.dubbo.demo.DemoService%3Fanyhost%3Dtrue%26application%3Ddemo-provider%26dubbo%3D2.5.6%26generic%3Dfalse%26interface%3Dcom.alibaba.dubbo.demo.DemoService%26methods%3DsayHello%26pid%3D4308%26side%3Dprovider%26timestamp%3D1508462946325]</span><br></pre></td></tr></table></figure>
<p>provider提供了服务：<code>dubbo:service</code>，consumer引用服务：<code>dubbo:reference</code>。<br>除此之外，provider在启动后，只要没有停止，就需要一直暴露dubbo协议：<code>dubbo:protocol</code>。</p>
<p>provider.xml：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">beans</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"demo-provider"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:protocol</span> <span class="attr">name</span>=<span class="string">"dubbo"</span> <span class="attr">port</span>=<span class="string">"20880"</span>/&gt;</span> <span class="comment">&lt;!-- 用dubbo协议在20880端口暴露服务 --&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">bean</span> <span class="attr">id</span>=<span class="string">"demoService"</span> <span class="attr">class</span>=<span class="string">"com.alibaba.dubbo.demo.provider.DemoServiceImpl"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:service</span> <span class="attr">interface</span>=<span class="string">"com.alibaba.dubbo.demo.DemoService"</span> <span class="attr">ref</span>=<span class="string">"demoService"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>consumer.xml：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">beans</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:application</span> <span class="attr">name</span>=<span class="string">"demo-consumer"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:registry</span> <span class="attr">address</span>=<span class="string">"zookeeper://127.0.0.1:2181"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dubbo:reference</span> <span class="attr">id</span>=<span class="string">"demoService"</span> <span class="attr">check</span>=<span class="string">"false"</span> <span class="attr">interface</span>=<span class="string">"com.alibaba.dubbo.demo.DemoService"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">beans</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>为了模拟provider的负载均衡，我们再启动一个provider，并且更改协议端口为20881。再次查看ZK：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 20] ls /dubbo/com.alibaba.dubbo.demo.DemoService/providers</span><br><span class="line">[dubbo%3A%2F%2F10.57.241.44%3A20881%2Fcom.alibaba.dubbo.demo.DemoService%3Fanyhost%3Dtrue%26application%3Ddemo-provider%26dubbo%3D2.5.6%26generic%3Dfalse%26interface%3Dcom.alibaba.dubbo.demo.DemoService%26methods%3DsayHello%26pid%3D4427%26side%3Dprovider%26timestamp%3D1508464040452</span><br><span class="line">,dubbo%3A%2F%2F10.57.241.44%3A20880%2Fcom.alibaba.dubbo.demo.DemoService%3Fanyhost%3Dtrue%26application%3Ddemo-provider%26dubbo%3D2.5.6%26generic%3Dfalse%26interface%3Dcom.alibaba.dubbo.demo.DemoService%26methods%3DsayHello%26pid%3D4308%26side%3Dprovider%26timestamp%3D1508462946325]</span><br></pre></td></tr></table></figure>
<p>新启动的Provider的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[20/10/17 09:47:21:021 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Register: dubbo://10.57.241.44:20881/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4427&amp;side=provider&amp;timestamp=1508464040452, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 09:47:21:021 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Subscribe: provider://10.57.241.44:20881/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4427&amp;side=provider&amp;timestamp=1508464040452, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 09:47:21:021 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Notify urls for subscribe url provider://10.57.241.44:20881/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4427&amp;side=provider&amp;timestamp=1508464040452, urls: [empty://10.57.241.44:20881/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4427&amp;side=provider&amp;timestamp=1508464040452], dubbo version: 2.5.6, current host: 127.0.0.1</span><br></pre></td></tr></table></figure>
<p>启动Consumer，为了观察RPC调用期间，消费者的相关流程，我们在RPC调用完，sleep了1分钟</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[20/10/17 09:50:13:013 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Register: consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=consumers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;side=consumer&amp;timestamp=1508464212849, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:13:013 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Subscribe: consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;side=consumer&amp;timestamp=1508464212849, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:13:013 CST] main  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Notify urls for subscribe url consumer://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=providers,configurators,routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;side=consumer&amp;timestamp=1508464212849, urls: [dubbo://10.57.241.44:20881/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4427&amp;side=provider&amp;timestamp=1508464040452, dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, empty://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;side=consumer&amp;timestamp=1508464212849, empty://10.57.241.44/com.alibaba.dubbo.demo.DemoService?application=demo-consumer&amp;category=routers&amp;check=false&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;side=consumer&amp;timestamp=1508464212849], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:13:013 CST] main  INFO transport.AbstractClient:  [DUBBO] Successed connect to server /10.57.241.44:20881 from NettyClient 10.57.241.44 using dubbo version 2.5.6, channel is NettyChannel [channel=[id: 0x0f2ff811, /10.57.241.44:54772 =&gt; /10.57.241.44:20881]], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:13:013 CST] main  INFO transport.AbstractClient:  [DUBBO] Start NettyClient zqhmac/10.57.241.44 connect to the server /10.57.241.44:20881, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:14:014 CST] main  INFO transport.AbstractClient:  [DUBBO] Successed connect to server /10.57.241.44:20880 from NettyClient 10.57.241.44 using dubbo version 2.5.6, channel is NettyChannel [channel=[id: 0x4efc180e, /10.57.241.44:54773 =&gt; /10.57.241.44:20880]], dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:14:014 CST] main  INFO transport.AbstractClient:  [DUBBO] Start NettyClient zqhmac/10.57.241.44 connect to the server /10.57.241.44:20880, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">[20/10/17 09:50:14:014 CST] main  INFO config.AbstractConfig:  [DUBBO] Refer dubbo service com.alibaba.dubbo.demo.DemoService from url zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?anyhost=true&amp;application=demo-consumer&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4434&amp;remote.timestamp=1508464040452&amp;side=consumer&amp;timestamp=1508464212849, dubbo version: 2.5.6, current host: 10.57.241.44</span><br><span class="line">Hello world, response form provider: 10.57.241.44:20880</span><br></pre></td></tr></table></figure>
<p>在这一分钟内，查看ZK的consumers信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 26] ls /dubbo/com.alibaba.dubbo.demo.DemoService/consumers</span><br><span class="line">[consumer%3A%2F%2F10.57.241.44%2Fcom.alibaba.dubbo.demo.DemoService%3Fapplication%3Ddemo-consumer%26category%3Dconsumers%26check%3Dfalse%26dubbo%3D2.5.6%26interface%3Dcom.alibaba.dubbo.demo.DemoService%26methods%3DsayHello%26pid%3D4434%26side%3Dconsumer%26timestamp%3D1508464212849]</span><br></pre></td></tr></table></figure>
<p>再调用多次consumer，可以看到每次RPC调用会负载到不同的provider上：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171020095939720" alt="dubboproviders"></p>
<p>关闭provider：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO config.AbstractConfig:  [DUBBO] Run shutdown hook now., dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO support.AbstractRegistryFactory:  [DUBBO] Close all registries [zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.registry.RegistryService&amp;pid=4308&amp;timestamp=1508462946295], dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy registry:zookeeper://127.0.0.1:2181/com.alibaba.dubbo.registry.RegistryService?application=demo-provider&amp;dubbo=2.5.6&amp;interface=com.alibaba.dubbo.registry.RegistryService&amp;pid=4308&amp;timestamp=1508462946295, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy unregister url dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO zookeeper.ZookeeperRegistry:  [DUBBO] Destroy unsubscribe url provider://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;category=configurators&amp;check=false&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] ZkClient-EventThread-14-127.0.0.1:2181  INFO zkclient.ZkEventThread: Terminate ZkClient event thread.</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO zookeeper.ZooKeeper: Session: 0x15f376495000000 closed</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO dubbo.DubboProtocol:  [DUBBO] Close dubbo server: /10.57.241.44:20880, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO transport.AbstractServer:  [DUBBO] Close NettyServer bind /0.0.0.0:20880, export /10.57.241.44:20880, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] main-EventThread  INFO zookeeper.ClientCnxn: EventThread shut down for session: 0x15f376495000000</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO dubbo.DubboProtocol:  [DUBBO] Unexport service: dubbo://10.57.241.44:20880/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line">[20/10/17 11:38:58:058 CST] DubboShutdownHook  INFO injvm.InjvmProtocol:  [DUBBO] Unexport service: injvm://127.0.0.1/com.alibaba.dubbo.demo.DemoService?anyhost=true&amp;application=demo-provider&amp;dubbo=2.5.6&amp;generic=false&amp;interface=com.alibaba.dubbo.demo.DemoService&amp;methods=sayHello&amp;pid=4308&amp;side=provider&amp;timestamp=1508462946325, dubbo version: 2.5.6, current host: 127.0.0.1</span><br><span class="line"></span><br><span class="line">Process finished with exit code 130</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Alibaba Dubbo&lt;br&gt;
    
    </summary>
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/categories/midd/"/>
    
    
      <category term="dubbo" scheme="http://github.com/zqhxuyuan/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>深入解析中间件之-Canal</title>
    <link href="http://github.com/zqhxuyuan/2017/10/10/Midd-canal/"/>
    <id>http://github.com/zqhxuyuan/2017/10/10/Midd-canal/</id>
    <published>2017-10-09T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.375Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/alibaba/canal">canal</a>: 阿里巴巴mysql数据库binlog的增量订阅&amp;消费组件<br><a id="more"></a></p>
<h2 id="MySQL_binlog">MySQL binlog</h2><h3 id="MySQL主从复制">MySQL主从复制</h3><p>mysql服务端修改配置并重启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/my.cnf</span><br><span class="line">[mysqld]</span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">binlog-format=ROW</span><br><span class="line">server_id=1</span><br><span class="line"></span><br><span class="line">$ mysql -uroot</span><br><span class="line">CREATE USER canal IDENTIFIED BY &apos;canal&apos;;  </span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos; ;</span><br><span class="line">FLUSH PRIVILEGES;</span><br><span class="line"></span><br><span class="line">$ sudo service mysqld start</span><br></pre></td></tr></table></figure>
<blockquote>
<p>问题：创建canal用户的目的是什么？直接使用现有的用户名可以吗，比如root。<br>答案：有些用户没有REPLICATION SLAVE, REPLICATION CLIENT的权限，用这些用户连接canal时，无法获取到binlog。<br>这里的canal用户授权了全部权限，所以客户端可以从canal中获取binlog。</p>
</blockquote>
<p>明确两个概念：canal server连接mysql，客户端连接canal server。</p>
<ul>
<li>canal指的是canal server，它会读取mysql的binlog，解析后存储起来</li>
<li>客户端指的是消费canal server的binlog</li>
</ul>
<p>本机连接服务端，验证binlog的格式是ROW</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -h192.168.6.52 -ucanal -pcanal</span><br><span class="line">mysql&gt; show variables like &apos;%binlog_format%&apos;;</span><br><span class="line">+---------------+-------+</span><br><span class="line">| Variable_name | Value |</span><br><span class="line">+---------------+-------+</span><br><span class="line">| binlog_format | ROW   |</span><br><span class="line">+---------------+-------+</span><br></pre></td></tr></table></figure>
<p>mysql主从复制的原理：</p>
<ul>
<li>master将改变记录到二进制日志(binary log)中；</li>
<li>slave将master的binary log events拷贝到它的中继日志(relay log)；</li>
<li>slave重做中继日志中的事件，将改变反映它自己的数据。</li>
</ul>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20160914112042547" alt="mysql replication"></p>
<h3 id="binlog">binlog</h3><p>在启动canal之前，先来了解下什么是mysql的binlog:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; show binlog events;</span><br><span class="line">| Log_name         | Pos   | Event_type  | Server_id | End_log_pos | Info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |</span><br><span class="line">+------------------+-------+-------------+-----------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| mysql-bin.000001 |     4 | Format_desc |         1 |         106 | Server ver: 5.1.73-log, Binlog ver: 4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |</span><br><span class="line">| mysql-bin.000001 |   106 | Query       |         1 |        1864 | use `mysql`; CREATE TABLE IF NOT EXISTS db (   Host char(60) binary DEFAULT &apos;&apos; NOT NULL, Db char(64) binary DEFAULT &apos;&apos; NOT NULL, User char(16) binary DEFAULT &apos;&apos; NOT NULL, Select_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Insert_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Update_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Delete_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Drop_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Grant_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, References_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Index_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Alter_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_tmp_table_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Lock_tables_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_view_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Show_view_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_routine_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Alter_routine_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Execute_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Event_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Trigger_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment=&apos;Database privileges&apos; |</span><br><span class="line">| mysql-bin.000001 |  1864 | Query       |         1 |        3518 | use `mysql`; CREATE TABLE IF NOT EXISTS host (  Host char(60) binary DEFAULT &apos;&apos; NOT NULL, Db char(64) binary DEFAULT &apos;&apos; NOT NULL, Select_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Insert_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Update_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Delete_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Drop_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Grant_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, References_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Index_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Alter_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_tmp_table_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Lock_tables_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_view_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Show_view_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Create_routine_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Alter_routine_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Execute_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, Trigger_priv enum(&apos;N&apos;,&apos;Y&apos;) COLLATE utf8_general_ci DEFAULT &apos;N&apos; NOT NULL, PRIMARY KEY Host (Host,Db) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment=&apos;Host privileges;  Merged with database privileges&apos; |</span><br></pre></td></tr></table></figure>
<p>mysql数据文件下会生成mysql-bin.xxx的binlog文件，以及索引文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ ll /var/lib/mysql/</span><br><span class="line">总用量 26228</span><br><span class="line">drwx------ 2 mysql mysql     4096 10月 11 14:05 canal_test</span><br><span class="line">-rw-rw---- 1 mysql mysql 10485760 9月  30 22:12 ibdata1</span><br><span class="line">-rw-rw---- 1 mysql mysql  5242880 10月 11 09:57 ib_logfile0</span><br><span class="line">-rw-rw---- 1 mysql mysql  5242880 10月 11 09:57 ib_logfile1</span><br><span class="line">drwx------ 2 mysql mysql     4096 8月   2 11:01 mysql</span><br><span class="line">-rw-rw---- 1 mysql mysql    18451 8月   2 11:01 mysql-bin.000001</span><br><span class="line">-rw-rw---- 1 mysql mysql   929226 8月   2 11:01 mysql-bin.000002</span><br><span class="line">-rw-rw---- 1 mysql mysql  4890698 9月  30 22:12 mysql-bin.000003</span><br><span class="line">-rw-rw---- 1 mysql mysql      897 10月 11 14:06 mysql-bin.000004</span><br><span class="line">-rw-rw---- 1 mysql mysql       76 10月 11 09:57 mysql-bin.index</span><br><span class="line">srwxrwxrwx 1 mysql mysql        0 10月 11 09:57 mysql.sock</span><br></pre></td></tr></table></figure>
<p>针对mysql的操作都会有二进制的事件记录到binlog文件中。下面的一些操作包括创建用户，授权，创建数据库，创建表，插入一条记录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ sudo strings /var/lib/mysql/mysql-bin.000004</span><br><span class="line">5.1.73-log</span><br><span class="line">CREATE USER canal IDENTIFIED BY &apos;canal&apos;</span><br><span class="line">root    localhost</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &apos;canal&apos;@&apos;%&apos;</span><br><span class="line">FLUSH PRIVILEGES</span><br><span class="line">canal_test</span><br><span class="line">create database canal_test    ===》创建数据库</span><br><span class="line">canal_test</span><br><span class="line">create table test (   uid int (4) primary key not null auto_increment,   name varchar(10) not null)  ==》创建表</span><br><span class="line">canal_test</span><br><span class="line">BEGIN     ==》插入记录，这里有事务。但是没有把具体的语句打印出来</span><br><span class="line">canal_test</span><br><span class="line">test</span><br><span class="line">canal_test</span><br><span class="line">COMMIT</span><br></pre></td></tr></table></figure>
<h2 id="Canal_QuickStart">Canal QuickStart</h2><h3 id="canal_&amp;_config">canal &amp; config</h3><p>部署canal server到6.52，并启动。查看canal的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/canal/canal.log</span><br><span class="line">2017-10-11 11:31:52.076 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.</span><br><span class="line">2017-10-11 11:31:52.151 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.6.52:11111]</span><br><span class="line">2017-10-11 11:31:52.644 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ......</span><br></pre></td></tr></table></figure>
<p>查看instance的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/example/example.log</span><br><span class="line">2017-10-11 11:31:52.435 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]</span><br><span class="line">2017-10-11 11:31:52.444 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties]</span><br><span class="line">2017-10-11 11:31:52.587 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example</span><br><span class="line">2017-10-11 11:31:52.599 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful....</span><br><span class="line">2017-10-11 11:31:52.679 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status</span><br></pre></td></tr></table></figure>
<p>canal server的conf下有几个配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">➜  canal.deployer-1.0.24 tree conf</span><br><span class="line">conf</span><br><span class="line">├── canal.properties</span><br><span class="line">├── example</span><br><span class="line">│   └── instance.properties</span><br><span class="line">├── logback.xml</span><br><span class="line">└── spring</span><br><span class="line">    ├── default-instance.xml</span><br><span class="line">    ├── file-instance.xml</span><br><span class="line">    ├── group-instance.xml</span><br><span class="line">    ├── local-instance.xml</span><br><span class="line">    └── memory-instance.xml</span><br></pre></td></tr></table></figure>
<p>先来看<code>canal.properties</code>的<strong>common</strong>属性前四个配置项：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">canal.id= 1</span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= 11111</span><br><span class="line">canal.zkServers=</span><br></pre></td></tr></table></figure>
<p>canal.id是canal的编号，在集群环境下，不同canal的id不同，注意它和mysql的server_id不同。<br>ip这里不指定，默认为本机，比如上面是192.168.6.52，端口号是11111。zk用于canal cluster。</p>
<p>再看下<code>canal.properties</code>下<strong>destinations</strong>相关的配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#################################################</span><br><span class="line">#########       destinations        ############# </span><br><span class="line">#################################################</span><br><span class="line">canal.destinations = example</span><br><span class="line">canal.conf.dir = ../conf</span><br><span class="line">canal.auto.scan = true</span><br><span class="line">canal.auto.scan.interval = 5</span><br><span class="line"></span><br><span class="line">canal.instance.global.mode = spring </span><br><span class="line">canal.instance.global.lazy = false</span><br><span class="line">canal.instance.global.spring.xml = classpath:spring/file-instance.xml</span><br></pre></td></tr></table></figure>
<p>这里的canal.destinations = example可以设置多个，比如example1,example2，<br>则需要创建对应的两个文件夹，并且每个文件夹下都有一个instance.properties文件。</p>
<p>全局的canal实例管理用spring，这里的<code>file-instance.xml</code>最终会实例化所有的destinations instances:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&lt;bean class=&quot;com.alibaba.otter.canal.instance.spring.support.PropertyPlaceholderConfigurer&quot; lazy-init=&quot;false&quot;&gt;</span><br><span class="line">    &lt;property name=&quot;ignoreResourceNotFound&quot; value=&quot;true&quot; /&gt;</span><br><span class="line">    &lt;property name=&quot;systemPropertiesModeName&quot; value=&quot;SYSTEM_PROPERTIES_MODE_OVERRIDE&quot;/&gt;&lt;!-- 允许system覆盖 --&gt;</span><br><span class="line">    &lt;property name=&quot;locationNames&quot;&gt;</span><br><span class="line">        &lt;list&gt;</span><br><span class="line">            &lt;value&gt;classpath:canal.properties&lt;/value&gt;</span><br><span class="line">            &lt;value&gt;classpath:$&#123;canal.instance.destination:&#125;/instance.properties&lt;/value&gt;</span><br><span class="line">        &lt;/list&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line">&lt;bean id=&quot;instance&quot; class=&quot;com.alibaba.otter.canal.instance.spring.CanalInstanceWithSpring&quot;&gt;</span><br><span class="line">    &lt;property name=&quot;destination&quot; value=&quot;$&#123;canal.instance.destination&#125;&quot; /&gt;</span><br><span class="line">    &lt;property name=&quot;eventParser&quot;&gt;&lt;ref local=&quot;eventParser&quot; /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;eventSink&quot;&gt;&lt;ref local=&quot;eventSink&quot; /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;eventStore&quot;&gt;&lt;ref local=&quot;eventStore&quot; /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;metaManager&quot;&gt;&lt;ref local=&quot;metaManager&quot; /&gt;&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;alarmHandler&quot;&gt;&lt;ref local=&quot;alarmHandler&quot; /&gt;&lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure>
<p>比如<code>canal.instance.destination</code>等于example，就会加载<code>example/instance.properties</code>配置文件</p>
<p>example下instance.properties配置文件不需要修改。一个canal server可以运行多个canal instance。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#################################################</span><br><span class="line">## mysql serverId，这里的slaveId不能和myql集群中已有的server_id一样</span><br><span class="line">canal.instance.mysql.slaveId = 1234</span><br><span class="line"></span><br><span class="line"># position info 这里连接的是mysql master的地址。</span><br><span class="line">canal.instance.master.address = 127.0.0.1:3306</span><br><span class="line">canal.instance.master.journal.name = </span><br><span class="line">canal.instance.master.position = </span><br><span class="line">canal.instance.master.timestamp = </span><br><span class="line"></span><br><span class="line">#canal.instance.standby.address = </span><br><span class="line">#canal.instance.standby.journal.name =</span><br><span class="line">#canal.instance.standby.position = </span><br><span class="line">#canal.instance.standby.timestamp = </span><br><span class="line"></span><br><span class="line"># username/password</span><br><span class="line">canal.instance.dbUsername = canal</span><br><span class="line">canal.instance.dbPassword = canal</span><br><span class="line">canal.instance.defaultDatabaseName =</span><br><span class="line">canal.instance.connectionCharset = UTF-8</span><br><span class="line"></span><br><span class="line">canal.instance.filter.regex = .*\\..*</span><br><span class="line">canal.instance.filter.black.regex =  </span><br><span class="line">#################################################</span><br></pre></td></tr></table></figure>
<h3 id="simple_client">simple client</h3><p>在mysql上创建数据库，创建表，插入一条记录，再修改记录。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database canal_test;</span><br><span class="line">use canal_test;</span><br><span class="line">create table test (   uid int (4) primary key not null auto_increment,   name varchar(10) not null);</span><br><span class="line">insert into test (name) values(&apos;10&apos;);</span><br></pre></td></tr></table></figure>
<p>修改<a href="https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/SimpleCanalClientTest.java">客户端测试例子</a>的连接信息。其中example对应了canal实例的名称。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">public class SimpleCanalClientTest extends AbstractCanalClientTest &#123;</span><br><span class="line">    public static void main(String args[]) &#123;</span><br><span class="line">        String destination = &quot;example&quot;;</span><br><span class="line">        CanalConnector connector = CanalConnectors.newSingleConnector(</span><br><span class="line">            new InetSocketAddress(&quot;192.168.6.52&quot;, 11111), destination, &quot;canal&quot;, &quot;canal&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：如果连接有错误，客户端测试例子会立即结束，打印## stop the canal client。正常的话，终端不会退出，会一直运行。</p>
</blockquote>
<p>SimpleCanalClientTest控制台的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [1] ,count : [2] , memsize : [263] , Time : 2017-10-11 14:06:06</span><br><span class="line">* Start : [mysql-bin.000004:396:1507701897000(2017-10-11 14:04:57)] </span><br><span class="line">* End : [mysql-bin.000004:491:1507701904000(2017-10-11 14:05:04)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:396] , name[canal_test,] , eventType : QUERY , executeTime : 1507701897000 , delay : 69710ms</span><br><span class="line"> sql ----&gt; create database canal_test</span><br><span class="line"></span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:491] , name[canal_test,test] , eventType : CREATE , executeTime : 1507701904000 , delay : 62723ms</span><br><span class="line"> sql ----&gt; create table test (   uid int (4) primary key not null auto_increment,   name varchar(10) not null)</span><br></pre></td></tr></table></figure>
<p>插入一条记录：（其中uid和name的update都等于true）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [2] ,count : [3] , memsize : [186] , Time : 2017-10-11 14:06:32</span><br><span class="line">* Start : [mysql-bin.000004:659:1507701989000(2017-10-11 14:06:29)] </span><br><span class="line">* End : [mysql-bin.000004:822:1507701989000(2017-10-11 14:06:29)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:659] , executeTime : 1507701989000 , delay : 3142ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 11</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:785] , name[canal_test,test] , eventType : INSERT , executeTime : 1507701989000 , delay : 3154ms</span><br><span class="line">uid : 1    type=int(4)    update=true</span><br><span class="line">name : 10    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:822] , executeTime : 1507701989000 , delay : 3179ms</span><br></pre></td></tr></table></figure>
<p>修改记录：（其中name的update等于true）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [3] ,count : [3] , memsize : [202] , Time : 2017-10-11 14:49:11</span><br><span class="line">* Start : [mysql-bin.000004:897:1507704547000(2017-10-11 14:49:07)] </span><br><span class="line">* End : [mysql-bin.000004:1076:1507704547000(2017-10-11 14:49:07)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:897] , executeTime : 1507704547000 , delay : 4048ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 13</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1023] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507704547000 , delay : 4059ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhxuyuan    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1076] , executeTime : 1507704547000 , delay : 4096ms</span><br></pre></td></tr></table></figure>
<p>canal安装包下的example instance下除了example.log外，还有一个<code>meta.log</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal]$ cat logs/example/meta.log</span><br><span class="line">2017-10-11 14:06:03.728 - clientId:1001 cursor:[mysql-bin.000004,396,1507701897000] address[/127.0.0.1:3306]</span><br><span class="line">2017-10-11 14:06:04.589 - clientId:1001 cursor:[mysql-bin.000004,491,1507701904000] address[localhost/127.0.0.1:3306]</span><br><span class="line">2017-10-11 14:06:29.589 - clientId:1001 cursor:[mysql-bin.000004,822,1507701989000] address[localhost/127.0.0.1:3306]</span><br><span class="line">2017-10-11 14:49:08.589 - clientId:1001 cursor:[mysql-bin.000004,1076,1507704547000] address[localhost/127.0.0.1:3306]</span><br></pre></td></tr></table></figure>
<h2 id="Cannal_Internal_Overview">Cannal Internal Overview</h2><h3 id="canal_client_&amp;_server">canal client &amp; server</h3><p><a href="https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/AbstractCanalClientTest.java">canal client</a>与canal server之间是C/S模式的通信，客户端采用NIO，服务端采用Netty。<br>canal server启动后，如果没有canal client，那么canal server不会去mysql拉取binlog。<br>即Canal客户端主动发起拉取请求，服务端才会模拟一个MySQL Slave节点去主节点拉取binlog。<br>通常Canal客户端是一个死循环，这样客户端一直调用get方法，服务端也就会一直拉取binlog。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public class AbstractCanalClientTest &#123;</span><br><span class="line">    protected void process() &#123;</span><br><span class="line">        int batchSize = 5 * 1024; // 一次请求拉取多条记录</span><br><span class="line">        try &#123;</span><br><span class="line">            connector.connect(); // 先连接服务端</span><br><span class="line">            connector.subscribe(); // 订阅</span><br><span class="line">            // keep send request to canal server, thus canal server can fetch binlog from mysql</span><br><span class="line">            while (running) &#123; </span><br><span class="line">                Message message = connector.getWithoutAck(batchSize); // 获取指定数量的数据</span><br><span class="line">                long batchId = message.getId();</span><br><span class="line">                int size = message.getEntries().size();</span><br><span class="line">                printSummary(message, batchId, size);</span><br><span class="line">                printEntry(message.getEntries());</span><br><span class="line">                connector.ack(batchId); // 提交确认</span><br><span class="line">                //connector.rollback(batchId); // 处理失败, 回滚数据</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            connector.disconnect();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>canal client与canal server之间属于增量订阅/消费，流程图如下：（其中C端是canal client，S端是canal server）</p>
<p><img src="https://camo.githubusercontent.com/db1debcfa50f4ebea1f56a1fa0e18a4e960cafcc/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333239372f39643765643133652d366138362d333836642d393266342d3835323233386334373562662e6a7067" alt="canal protocol"></p>
<p>canal client调用<a href="https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L129"><code>connect()</code></a>方法时，发送的数据包（PacketType）类型为：</p>
<ol>
<li><a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/HandshakeInitializationHandler.java"><strong>HANDSHAKE</strong></a>，</li>
<li><a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/ClientAuthenticationHandler.java"><strong>CLIENTAUTHENTICATION</strong></a>。</li>
</ol>
<p>canal client调用<code>subscribe()</code>方法，类型为[<strong>SUBSCRIPTION</strong>]。</p>
<p>对应服务端采用netty处理RPC请求（<a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/CanalServerWithNetty.java"><code>CanalServerWithNetty</code></a>）:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">public class CanalServerWithNetty extends AbstractCanalLifeCycle implements CanalServer &#123;</span><br><span class="line">    public void start() &#123;</span><br><span class="line">        bootstrap.setPipelineFactory(new ChannelPipelineFactory() &#123;</span><br><span class="line">            public ChannelPipeline getPipeline() throws Exception &#123;</span><br><span class="line">                ChannelPipeline pipelines = Channels.pipeline();</span><br><span class="line">                pipelines.addLast(FixedHeaderFrameDecoder.class.getName(), new FixedHeaderFrameDecoder());</span><br><span class="line">                // 处理客户端的HANDSHAKE请求</span><br><span class="line">                pipelines.addLast(HandshakeInitializationHandler.class.getName(),</span><br><span class="line">                    new HandshakeInitializationHandler(childGroups));</span><br><span class="line">                // 处理客户端的CLIENTAUTHENTICATION请求</span><br><span class="line">                pipelines.addLast(ClientAuthenticationHandler.class.getName(),</span><br><span class="line">                    new ClientAuthenticationHandler(embeddedServer));</span><br><span class="line"></span><br><span class="line">                // 处理客户端的会话请求，包括SUBSCRIPTION，GET等</span><br><span class="line">                SessionHandler sessionHandler = new SessionHandler(embeddedServer);</span><br><span class="line">                pipelines.addLast(SessionHandler.class.getName(), sessionHandler);</span><br><span class="line">                return pipelines;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ClientAuthenticationHandler处理鉴权后，会移除HandshakeInitializationHandler和<a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/ClientAuthenticationHandler.java#L81">ClientAuthenticationHandler</a>。<br>最重要的是会话处理器<a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/SessionHandler.java"><strong>SessionHandler</strong></a>。</p>
<p>以client发送GET，server从mysql得到binlog后，返回<strong>MESSAGES</strong>给client为例，说明client和server的rpc交互过程：</p>
<p>SimpleCanalConnector发送<a href="https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L272"><strong>GET</strong></a>请求，并读取响应结果的流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">public Message getWithoutAck(int batchSize, Long timeout, TimeUnit unit) throws CanalClientException &#123;</span><br><span class="line">    waitClientRunning();</span><br><span class="line">    int size = (batchSize &lt;= 0) ? 1000 : batchSize;</span><br><span class="line">    long time = (timeout == null || timeout &lt; 0) ? -1 : timeout; // -1代表不做timeout控制</span><br><span class="line">    if (unit == null) unit = TimeUnit.MILLISECONDS;</span><br><span class="line"></span><br><span class="line">    // client发送GET请求</span><br><span class="line">    writeWithHeader(Packet.newBuilder()</span><br><span class="line">        .setType(PacketType.GET)</span><br><span class="line">        .setBody(Get.newBuilder()</span><br><span class="line">            .setAutoAck(false)</span><br><span class="line">            .setDestination(clientIdentity.getDestination())</span><br><span class="line">            .setClientId(String.valueOf(clientIdentity.getClientId()))</span><br><span class="line">            .setFetchSize(size)</span><br><span class="line">            .setTimeout(time)</span><br><span class="line">            .setUnit(unit.ordinal())</span><br><span class="line">            .build()</span><br><span class="line">            .toByteString())</span><br><span class="line">        .build()</span><br><span class="line">        .toByteArray());</span><br><span class="line">    // client获取GET结果    </span><br><span class="line">    return receiveMessages();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private Message receiveMessages() throws IOException &#123;</span><br><span class="line">    // 读取server发送的数据包</span><br><span class="line">    Packet p = Packet.parseFrom(readNextPacket());</span><br><span class="line">    switch (p.getType()) &#123;</span><br><span class="line">        case MESSAGES: &#123;</span><br><span class="line">            Messages messages = Messages.parseFrom(p.getBody());</span><br><span class="line">            Message result = new Message(messages.getBatchId());</span><br><span class="line">            for (ByteString byteString : messages.getMessagesList()) &#123;</span><br><span class="line">                result.addEntry(Entry.parseFrom(byteString));</span><br><span class="line">            &#125;</span><br><span class="line">            return result;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端SessionHandler处理客户端发送的<a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/netty/handler/SessionHandler.java#L105"><strong>GET</strong></a>请求流程：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">case GET:</span><br><span class="line">    // 读取客户端发送的数据包，封装为Get对象</span><br><span class="line">    Get get = CanalPacket.Get.parseFrom(packet.getBody());</span><br><span class="line">    // destination表示canal instance</span><br><span class="line">    if (StringUtils.isNotEmpty(get.getDestination()) &amp;&amp; StringUtils.isNotEmpty(get.getClientId())) &#123;</span><br><span class="line">        clientIdentity = new ClientIdentity(get.getDestination(), Short.valueOf(get.getClientId()));</span><br><span class="line">        Message message = null;</span><br><span class="line">        if (get.getTimeout() == -1) &#123;// 是否是初始值</span><br><span class="line">            message = embeddedServer.getWithoutAck(clientIdentity, get.getFetchSize());</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            TimeUnit unit = convertTimeUnit(get.getUnit());</span><br><span class="line">            message = embeddedServer.getWithoutAck(clientIdentity, get.getFetchSize(), get.getTimeout(), unit);</span><br><span class="line">        &#125;</span><br><span class="line">        // 设置返回给客户端的数据包类型为MESSAGES   </span><br><span class="line">        Packet.Builder packetBuilder = CanalPacket.Packet.newBuilder();</span><br><span class="line">        packetBuilder.setType(PacketType.MESSAGES);</span><br><span class="line">        // 构造Message</span><br><span class="line">        Messages.Builder messageBuilder = CanalPacket.Messages.newBuilder();</span><br><span class="line">        messageBuilder.setBatchId(message.getId());</span><br><span class="line">        if (message.getId() != -1 &amp;&amp; !CollectionUtils.isEmpty(message.getEntries())) &#123;</span><br><span class="line">            for (Entry entry : message.getEntries()) &#123;</span><br><span class="line">                messageBuilder.addMessages(entry.toByteString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        packetBuilder.setBody(messageBuilder.build().toByteString());</span><br><span class="line">        // 输出数据，返回给客户端</span><br><span class="line">        NettyUtils.write(ctx.getChannel(), packetBuilder.build().toByteArray(), null);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>get/ack/rollback协议介绍：</p>
<ul>
<li><code>Message getWithoutAck(int batchSize)</code>，允许指定batchSize，一次可以获取多条，每次返回的对象为Message，包含的内容为：<br>– batch id 唯一标识<br>– entries 具体的数据对象，对应的数据对象格式：<a href="https://github.com/alibaba/canal/blob/master/protocol/src/main/java/com/alibaba/otter/canal/protocol/EntryProtocol.proto">EntryProtocol.proto</a></li>
<li><a href="https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L325"><code>void rollback(long batchId)</code></a>，回滚上次的get请求，重新获取数据。基于get获取的batchId进行提交，避免误操作</li>
<li><a href="https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L343"><code>void ack(long batchId)</code></a>，确认已经消费成功，通知server删除数据。基于get获取的batchId进行提交，避免误操作</li>
</ul>
<p>EntryProtocol.protod对应的canal消息结构如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Entry  </span><br><span class="line">    Header  </span><br><span class="line">        logfileName [binlog文件名]  </span><br><span class="line">        logfileOffset [binlog position]  </span><br><span class="line">        executeTime [binlog里记录变更发生的时间戳,精确到秒]  </span><br><span class="line">        schemaName   </span><br><span class="line">        tableName  </span><br><span class="line">        eventType [insert/update/delete类型]  </span><br><span class="line">    entryType   [事务头BEGIN/事务尾END/数据ROWDATA]  </span><br><span class="line">    storeValue  [byte数据,可展开，对应的类型为RowChange]  </span><br><span class="line">      </span><br><span class="line">RowChange  </span><br><span class="line">    isDdl       [是否是ddl变更操作，比如create table/drop table]  </span><br><span class="line">    sql         [具体的ddl sql]  </span><br><span class="line">    rowDatas    [具体insert/update/delete的变更数据，可为多条，1个binlog event事件可对应多条变更，比如批处理]  </span><br><span class="line">        beforeColumns [Column类型的数组，变更前的数据字段]  </span><br><span class="line">        afterColumns [Column类型的数组，变更后的数据字段]  </span><br><span class="line">          </span><br><span class="line">Column   </span><br><span class="line">    index         </span><br><span class="line">    sqlType     [jdbc type]  </span><br><span class="line">    name        [column name]  </span><br><span class="line">    isKey       [是否为主键]  </span><br><span class="line">    updated     [是否发生过变更]  </span><br><span class="line">    isNull      [值是否为null]  </span><br><span class="line">    value       [具体的内容，注意为string文本]</span><br></pre></td></tr></table></figure>
<p>SessionHandler中服务端处理客户端的其他类型请求，都会调用<a href="https://github.com/alibaba/canal/blob/master/server/src/main/java/com/alibaba/otter/canal/server/embedded/CanalServerWithEmbedded.java">CanalServerWithEmbedded</a>的相关方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case SUBSCRIPTION:</span><br><span class="line">        Sub sub = Sub.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.subscribe(clientIdentity);</span><br><span class="line">case GET:</span><br><span class="line">        Get get = CanalPacket.Get.parseFrom(packet.getBody());</span><br><span class="line">        message = embeddedServer.getWithoutAck(clientIdentity, get.getFetchSize());</span><br><span class="line">case CLIENTACK:</span><br><span class="line">        ClientAck ack = CanalPacket.ClientAck.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.ack(clientIdentity, ack.getBatchId());</span><br><span class="line">case CLIENTROLLBACK:</span><br><span class="line">        ClientRollback rollback = CanalPacket.ClientRollback.parseFrom(packet.getBody());</span><br><span class="line">        embeddedServer.rollback(clientIdentity);// 回滚所有批次</span><br></pre></td></tr></table></figure>
<p>所以真正的处理逻辑在CanalServerWithEmbedded中，下面重点来了。。。</p>
<h3 id="CanalServerWithEmbedded">CanalServerWithEmbedded</h3><p>CanalServer包含多个Instance，它的成员变量<code>canalInstances</code>记录了instance名称与<a href="https://github.com/alibaba/canal/blob/master/instance/core/src/main/java/com/alibaba/otter/canal/instance/core/AbstractCanalInstance.java">实例</a>的映射关系。<br>因为是一个Map，所以同一个Server不允许出现相同instance名称（本例中实例名称为example），<br>比如不能同时有两个example在一个server上。但是允许一个Server上有example1和example2。</p>
<blockquote>
<p>注意：<code>CanalServer</code>中最重要的是<code>CanalServerWithEmbedded</code>，而CanalServerWithEmbedded中最重要的是<code>CanalInstance</code>。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public class CanalServerWithEmbedded extends AbstractCanalLifeCycle implements CanalServer, CanalService &#123;</span><br><span class="line">    private Map&lt;String, CanalInstance&gt; canalInstances;</span><br><span class="line">    private CanalInstanceGenerator     canalInstanceGenerator;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图表示一个server配置了两个Canal实例（instance），每个Client连接一个Instance。<br>每个Canal实例模拟为一个MySQL的slave，所以每个Instance的slaveId必须不一样。<br>比如图中两个Instance的id分别是1234和1235，它们都会拉取MySQL主节点的binlog。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171011202259253" alt="instances"></p>
<p>这里每个Canal Client都对应一个Instance，每个Client在启动时，<br>都会指定一个Destination，这个Destination就表示Instance的名称。<br>所以CanalServerWithEmbedded处理各种请求时的参数都有ClientIdentity，<br>从ClientIdentity中获取destination，就可以获取出对应的CanalInstance。</p>
<p>理解下各个组件的对应关系：</p>
<ul>
<li>Canal Client通过destination找出Canal Server中对应的Canal Instance。</li>
<li>一个Canal Server可以配置多个Canal Instances。</li>
</ul>
<p>下面以CanalServerWithEmbedded的订阅方法为例：</p>
<ol>
<li>根据客户端标识获取CanalInstance</li>
<li>向CanalInstance的元数据管理器订阅当前客户端</li>
<li>从元数据管理中获取客户端的游标</li>
<li>通知CanalInstance订阅关系发生变化</li>
</ol>
<blockquote>
<p>注意：提供订阅方法的作用是：MySQL新增了一张表，客户端原先没有同步这张表，现在需要同步，所以需要重新订阅。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">public void subscribe(ClientIdentity clientIdentity) throws CanalServerException &#123;</span><br><span class="line">    // ClientIdentity表示Canal Client客户端，从中可以获取出客户端指定连接的Destination</span><br><span class="line">    // 由于CanalServerWithEmbedded记录了每个Destination对应的Instance，可以获取客户端对应的Instance</span><br><span class="line">    CanalInstance canalInstance = canalInstances.get(clientIdentity.getDestination());</span><br><span class="line">    if (!canalInstance.getMetaManager().isStart()) &#123;</span><br><span class="line">        canalInstance.getMetaManager().start(); // 启动Instance的元数据管理器</span><br><span class="line">    &#125;</span><br><span class="line">    canalInstance.getMetaManager().subscribe(clientIdentity); // 执行一下meta订阅</span><br><span class="line">    Position position = canalInstance.getMetaManager().getCursor(clientIdentity);</span><br><span class="line">    if (position == null) &#123;</span><br><span class="line">        position = canalInstance.getEventStore().getFirstPosition();// 获取一下store中的第一条</span><br><span class="line">        if (position != null) &#123;</span><br><span class="line">            canalInstance.getMetaManager().updateCursor(clientIdentity, position); // 更新一下cursor</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 通知下订阅关系变化</span><br><span class="line">    canalInstance.subscribeChange(clientIdentity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个CanalInstance中包括了四个组件：<strong>EventParser、EventSink、EventStore、MetaManager</strong>。</p>
<p>服务端主要的处理方法包括get/ack/rollback，这三个方法都会用到Instance上面的几个内部组件，主要还是EventStore和MetaManager：</p>
<p>在这之前，要先理解EventStore的含义，EventStore是一个RingBuffer，有三个指针：<strong>Put、Get、Ack</strong>。</p>
<ul>
<li>Put: Canal Server从MySQL拉取到数据后，放到内存中，Put增加</li>
<li>Get: 消费者（Canal Client）从内存中消费数据，Get增加</li>
<li>Ack: 消费者消费完成，Ack增加。并且会删除Put中已经被Ack的数据</li>
</ul>
<p>这三个操作与Instance组件的关系如下：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171011211529169" alt="ops"></p>
<p>客户端通过canal server获取mysql binlog有几种方式（get方法和getWithoutAck）：</p>
<ul>
<li>如果timeout为null，则采用tryGet方式，即时获取  </li>
<li>如果timeout不为null  <ol>
<li>timeout为0，则采用get阻塞方式，获取数据，不设置超时，直到有足够的batchSize数据才返回  </li>
<li>timeout不为0，则采用get+timeout方式，获取数据，超时还没有batchSize足够的数据，有多少返回多少  </li>
</ol>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">private Events&lt;Event&gt; getEvents(CanalEventStore eventStore, Position start, int batchSize, Long timeout,</span><br><span class="line">                                TimeUnit unit) &#123;</span><br><span class="line">    if (timeout == null) &#123;</span><br><span class="line">        return eventStore.tryGet(start, batchSize); // 即时获取</span><br><span class="line">    &#125; else if (timeout &lt;= 0)&#123;</span><br><span class="line">        return eventStore.get(start, batchSize); // 阻塞获取</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        return eventStore.get(start, batchSize, timeout, unit); // 异步获取</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意：EventStore的实现采用了类似Disruptor的RingBuffer环形缓冲区。RingBuffer的实现类是MemoryEventStoreWithBuffer</p>
</blockquote>
<p>get方法和getWithoutAck方法的区别是：</p>
<ul>
<li>get方法会立即调用ack</li>
<li>getWithoutAck方法不会调用ack</li>
</ul>
<h3 id="EventStore">EventStore</h3><p>以10条数据为例，初始时current=-1，第一个元素起始next=0，end=9，循环<code>[0,9]</code>所有元素。<br>List元素为(A,B,C,D,E,F,G,H,I,J)</p>
<table>
<thead>
<tr>
<th>next</th>
<th>entries[next]</th>
<th>next-current-1</th>
<th>list element</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>entries[0]</td>
<td>0-(-1)-1=0</td>
<td>A</td>
</tr>
<tr>
<td>1</td>
<td>entries[1]</td>
<td>1-(-1)-1=1</td>
<td>B</td>
</tr>
<tr>
<td>2</td>
<td>entries[2]</td>
<td>2-(-1)-1=2</td>
<td>C</td>
</tr>
<tr>
<td>3</td>
<td>entries[3]</td>
<td>3-(-1)-1=3</td>
<td>D</td>
</tr>
<tr>
<td>.</td>
<td>……….</td>
<td>……….</td>
<td>.</td>
</tr>
<tr>
<td>9</td>
<td>entries[9]</td>
<td>9-(-1)-1=9</td>
<td>J</td>
</tr>
</tbody>
</table>
<p>第一批10个元素put完成后，putSequence设置为end=9。假设第二批又Put了5个元素:(K,L,M,N,O)</p>
<p>current=9，起始next=9+1=10，end=9+5=14，在Put完成后，putSequence设置为end=14。</p>
<table>
<thead>
<tr>
<th>next</th>
<th>entries[next]</th>
<th>next-current-1</th>
<th>list element</th>
</tr>
</thead>
<tbody>
<tr>
<td>10</td>
<td>entries[10]</td>
<td>10-(9)-1=0</td>
<td>K</td>
</tr>
<tr>
<td>11</td>
<td>entries[11]</td>
<td>11-(9)-1=1</td>
<td>L</td>
</tr>
<tr>
<td>12</td>
<td>entries[12]</td>
<td>12-(9)-1=2</td>
<td>M</td>
</tr>
<tr>
<td>13</td>
<td>entries[13]</td>
<td>13-(9)-1=3</td>
<td>N</td>
</tr>
<tr>
<td>14</td>
<td>entries[14]</td>
<td>14-(9)-1=3</td>
<td>O</td>
</tr>
</tbody>
</table>
<p>这里假设环形缓冲区的最大大小为15个（源码中是16MB），那么上面两批一共产生了15个元素，刚好填满了环形缓冲区。<br>如果又有Put事件进来，由于环形缓冲区已经满了，没有可用的slot，则Put操作会被阻塞，直到被消费掉。</p>
<p>下面是Put填充环形缓冲区的代码，检查可用slot（checkFreeSlotAt方法）在几个put方法中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">public class MemoryEventStoreWithBuffer extends AbstractCanalStoreScavenge implements CanalEventStore&lt;Event&gt;, CanalStoreScavenge &#123;</span><br><span class="line">    private static final long INIT_SQEUENCE = -1;</span><br><span class="line">    private int               bufferSize    = 16 * 1024;</span><br><span class="line">    private int               bufferMemUnit = 1024;                         // memsize的单位，默认为1kb大小</span><br><span class="line">    private int               indexMask;</span><br><span class="line">    private Event[]           entries;</span><br><span class="line"></span><br><span class="line">    // 记录下put/get/ack操作的三个下标</span><br><span class="line">    private AtomicLong        putSequence   = new AtomicLong(INIT_SQEUENCE); // 代表当前put操作最后一次写操作发生的位置</span><br><span class="line">    private AtomicLong        getSequence   = new AtomicLong(INIT_SQEUENCE); // 代表当前get操作读取的最后一条的位置</span><br><span class="line">    private AtomicLong        ackSequence   = new AtomicLong(INIT_SQEUENCE); // 代表当前ack操作的最后一条的位置</span><br><span class="line"></span><br><span class="line">    // 启动EventStore时，创建指定大小的缓冲区，Event数组的大小是16*1024</span><br><span class="line">    // 也就是说算个数的话，数组可以容纳16000个事件。算内存的话，大小为16MB</span><br><span class="line">    public void start() throws CanalStoreException &#123;</span><br><span class="line">        super.start();</span><br><span class="line">        indexMask = bufferSize - 1;</span><br><span class="line">        entries = new Event[bufferSize];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // EventParser解析后，会放入内存中（Event数组，缓冲区）</span><br><span class="line">    private void doPut(List&lt;Event&gt; data) &#123;</span><br><span class="line">        long current = putSequence.get(); // 取得当前的位置，初始时为-1，第一个元素为-1+1=0</span><br><span class="line">        long end = current + data.size(); // 最末尾的位置，假设Put了10条数据，end=-1+10=9</span><br><span class="line">        // 先写数据，再更新对应的cursor,并发度高的情况，putSequence会被get请求可见，拿出了ringbuffer中的老的Entry值</span><br><span class="line">        for (long next = current + 1; next &lt;= end; next++) &#123;</span><br><span class="line">            entries[getIndex(next)] = data.get((int) (next - current - 1));</span><br><span class="line">        &#125;</span><br><span class="line">        putSequence.set(end);</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Put是生产数据，Get是消费数据，Get一定不会超过Put。比如Put了10条数据，Get最多只能获取到10条数据。但有时候为了保证Get处理的速度，Put和Get并不会相等。<br>可以把Put看做是生产者，Get看做是消费者。生产者速度可以很快，消费者则可以慢慢地消费。比如Put了1000条，而Get我们只需要每次处理10条数据。</p>
<p>仍然以前面的示例来说明Get的流程，初始时current=-1，假设Put了两批数据一共15条，maxAbleSequence=14，而Get的BatchSize假设为10。<br>初始时next=current=-1，end=-1。通过startPosition，会设置next=0。最后end又被赋值为9，即循环缓冲区[0,9]一共10个元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">private Events&lt;Event&gt; doGet(Position start, int batchSize) throws CanalStoreException &#123;</span><br><span class="line">    LogPosition startPosition = (LogPosition) start;</span><br><span class="line"></span><br><span class="line">    long current = getSequence.get();</span><br><span class="line">    long maxAbleSequence = putSequence.get();</span><br><span class="line">    long next = current;</span><br><span class="line">    long end = current;</span><br><span class="line">    // 如果startPosition为null，说明是第一次，默认+1处理</span><br><span class="line">    if (startPosition == null || !startPosition.getPostion().isIncluded()) &#123; // 第一次订阅之后，需要包含一下start位置，防止丢失第一条记录</span><br><span class="line">        next = next + 1;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    end = (next + batchSize - 1) &lt; maxAbleSequence ? (next + batchSize - 1) : maxAbleSequence;</span><br><span class="line">    // 提取数据并返回</span><br><span class="line">    for (; next &lt;= end; next++) &#123;</span><br><span class="line">        Event event = entries[getIndex(next)];</span><br><span class="line">        if (ddlIsolation &amp;&amp; isDdl(event.getEntry().getHeader().getEventType())) &#123;</span><br><span class="line">            // 如果是ddl隔离，直接返回</span><br><span class="line">            if (entrys.size() == 0) &#123;</span><br><span class="line">                entrys.add(event);// 如果没有DML事件，加入当前的DDL事件</span><br><span class="line">                end = next; // 更新end为当前</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                // 如果之前已经有DML事件，直接返回了，因为不包含当前next这记录，需要回退一个位置</span><br><span class="line">                end = next - 1; // next-1一定大于current，不需要判断</span><br><span class="line">            &#125;</span><br><span class="line">            break;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            entrys.add(event);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    // 处理PositionRange，然后设置getSequence为end</span><br><span class="line">    getSequence.compareAndSet(current, end)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ack操作的上限是Get，假设Put了15条数据，Get了10条数据，最多也只能Ack10条数据。Ack的目的是清空缓冲区中已经被Get过的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">public void ack(Position position) throws CanalStoreException &#123;</span><br><span class="line">    cleanUntil(position);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void cleanUntil(Position position) throws CanalStoreException &#123;</span><br><span class="line">    long sequence = ackSequence.get();</span><br><span class="line">    long maxSequence = getSequence.get();</span><br><span class="line"></span><br><span class="line">    boolean hasMatch = false;</span><br><span class="line">    long memsize = 0;</span><br><span class="line">    for (long next = sequence + 1; next &lt;= maxSequence; next++) &#123;</span><br><span class="line">        Event event = entries[getIndex(next)];</span><br><span class="line">        memsize += calculateSize(event);</span><br><span class="line">        boolean match = CanalEventUtils.checkPosition(event, (LogPosition) position);</span><br><span class="line">        if (match) &#123;// 找到对应的position，更新ack seq</span><br><span class="line">            hasMatch = true;</span><br><span class="line"></span><br><span class="line">            if (batchMode.isMemSize()) &#123;</span><br><span class="line">                ackMemSize.addAndGet(memsize);</span><br><span class="line">                // 尝试清空buffer中的内存，将ack之前的内存全部释放掉</span><br><span class="line">                for (long index = sequence + 1; index &lt; next; index++) &#123;</span><br><span class="line">                    entries[getIndex(index)] = null;// 设置为null</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            ackSequence.compareAndSet(sequence, next)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>rollback回滚方法的实现则比较简单，将getSequence回退到ack位置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">public void rollback() throws CanalStoreException &#123;</span><br><span class="line">    getSequence.set(ackSequence.get());</span><br><span class="line">    getMemSize.set(ackMemSize.get());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>下图展示了RingBuffer的几个操作示例：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171011225116791" alt="ringbuffer"></p>
<h3 id="EventParser_WorkFlow">EventParser WorkFlow</h3><p>EventStore负责存储解析后的Binlog事件，而解析动作负责拉取Binlog，它的流程比较复杂。需要和MetaManager进行交互。<br>比如要记录每次拉取的Position，这样下一次就可以从上一次的最后一个位置继续拉取。所以MetaManager应该是有状态的。</p>
<p>EventParser的流程如下：</p>
<ol>
<li>Connection获取上一次解析成功的位置 (如果第一次启动，则获取初始指定的位置或者是当前数据库的binlog位点)</li>
<li>Connection建立链接，发送BINLOG_DUMP指令</li>
<li>Mysql开始推送Binaly Log</li>
<li>接收到的Binaly Log的通过Binlog parser进行协议解析，补充一些特定信息</li>
<li>传递给EventSink模块进行数据存储，是一个阻塞操作，直到存储成功</li>
<li>存储成功后，定时记录Binaly Log位置</li>
</ol>
<p><img src="https://camo.githubusercontent.com/031db3aa27461d13faa2dea479ef639f93386a00/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333134332f37393531633136392d663764662d336362332d616562622d6439323466353733313163622e6a7067" alt="parser"></p>
<p>上面提到的Connection指的是实现了<code>ErosaConnection</code>接口的<code>MysqlConnection</code>。<br><code>EventParser</code>的实现类是实现了<code>AbstractEventParser</code>的<code>MysqlEventParser</code>。</p>
<p><code>EventParser</code>解析binlog后通过<code>EventSink</code>写入到<code>EventStore</code>，这条链路可以通过EventStore的put方法串联起来：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171011234800632" alt="put"></p>
<p>其实这里还有一个EventTransactionBuffer缓冲区，即Parser解析后先放到缓冲区中，<br>当事务发生时或者数据超过阈值，就会执行刷新操作：即消费缓冲区的数据，放到EventStore中。<br>这个缓冲区有两个偏移量指针：putSequence和flushSequence。</p>
<h2 id="Canal_HA">Canal HA</h2><p>单机模拟两个Canal Server，将单机模式复制出两个文件夹，并修改相关配置</p>
<p>canal_m/conf/canal.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">canal.id= 2</span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= 11112</span><br><span class="line">canal.zkServers=localhost:2181</span><br><span class="line">canal.instance.global.spring.xml = classpath:spring/default-instance.xml</span><br></pre></td></tr></table></figure>
<p>canal_m/conf/example/instance.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canal.instance.mysql.slaveId = 1235</span><br></pre></td></tr></table></figure>
<p>canal_s</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">canal.id= 3</span><br><span class="line">canal.ip=</span><br><span class="line">canal.port= 11113</span><br><span class="line">canal.zkServers=localhost:2181</span><br><span class="line">canal.instance.global.spring.xml = classpath:spring/default-instance.xml</span><br></pre></td></tr></table></figure>
<p>canal_s/conf/example/instance.properties</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">canal.instance.mysql.slaveId = 1236</span><br></pre></td></tr></table></figure>
<p>启动canal_m</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017-10-12 14:51:45.202 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.</span><br><span class="line">2017-10-12 14:51:45.776 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.6.52:11112]</span><br><span class="line">2017-10-12 14:51:46.687 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ......</span><br></pre></td></tr></table></figure>
<p>启动canal_s</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2017-10-12 14:52:18.999 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.</span><br><span class="line">2017-10-12 14:52:19.208 [main] INFO  com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[192.168.6.52:11113]</span><br><span class="line">2017-10-12 14:52:19.364 [main] INFO  com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ......</span><br></pre></td></tr></table></figure>
<p>master提供服务，canal_m/logs/example/example.log下有日志，而canal_s/logs没有example文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ tail -f canal_m/logs/example/example.log</span><br><span class="line">2017-10-12 14:51:46.453 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]</span><br><span class="line">2017-10-12 14:51:46.463 [main] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties]</span><br><span class="line">2017-10-12 14:51:46.624 [main] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example</span><br><span class="line">2017-10-12 14:51:46.644 [main] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful....</span><br><span class="line">2017-10-12 14:51:46.658 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just show master status</span><br></pre></td></tr></table></figure>
<p>查看Canal HA记录在ZK的信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 7] ls /otter/canal/destinations/example/cluster</span><br><span class="line">[192.168.6.52:11112, 192.168.6.52:11113]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 10] get /otter/canal/destinations/example/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;192.168.6.52:11112&quot;,&quot;cid&quot;:2&#125;</span><br></pre></td></tr></table></figure>
<p>启动example的<a href="https://github.com/alibaba/canal/blob/master/example/src/main/java/com/alibaba/otter/canal/example/ClusterCanalClientTest.java">ClusterCanalClientTest</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CanalConnector connector = CanalConnectors.newClusterConnector(&quot;192.168.6.52:2181&quot;, destination, &quot;canal&quot;, &quot;canal&quot;);</span><br></pre></td></tr></table></figure>
<p>执行SQL：<code>update test set name = &#39;zqh&#39; where uid=1;</code>，控制台打印日志如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [1] ,count : [3] , memsize : [203] , Time : 2017-10-12 15:05:20</span><br><span class="line">* Start : [mysql-bin.000004:1151:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line">* End : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1151] , executeTime : 1507791918000 , delay : 2080ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 763</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1277] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507791918000 , delay : 2092ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqh    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1331] , executeTime : 1507791918000 , delay : 2130ms</span><br></pre></td></tr></table></figure>
<p>再次查看ZK中记录的客户端信息：</p>
<ul>
<li>一个Instance对应一个Client，这里的Instance名称为example，对应的客户端编号是1001</li>
<li>为了验证Instance确实是由指定的Client连接，在Server上查看11112端口</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 18] get /otter/canal/destinations/example/1001/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.241.44:53942&quot;,&quot;clientId&quot;:1001&#125;</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 19] get /otter/canal/destinations/example/1001/cursor</span><br><span class="line">&#123;&quot;@type&quot;:&quot;com.alibaba.otter.canal.protocol.position.LogPosition&quot;,</span><br><span class="line">&quot;identity&quot;:&#123;&quot;slaveId&quot;:-1,&quot;sourceAddress&quot;:&#123;&quot;address&quot;:&quot;localhost&quot;,&quot;port&quot;:3306&#125;&#125;,</span><br><span class="line">&quot;postion&quot;:&#123;&quot;included&quot;:false,&quot;journalName&quot;:&quot;mysql-bin.000004&quot;,&quot;position&quot;:1331,&quot;serverId&quot;:1,&quot;timestamp&quot;:1507791918000&#125;&#125; ==》serverId表示MySQL的server_id</span><br><span class="line"></span><br><span class="line">[qihuang.zheng@dp0652 ~]$ netstat -anpt|grep 11112</span><br><span class="line">tcp        0      0 0.0.0.0:11112               0.0.0.0:*                   LISTEN      27816/java   ==》Canal服务端</span><br><span class="line">tcp        0     19 192.168.6.52:11112          10.57.241.44:53942          ESTABLISHED 27816/java   ==》Canal客户端</span><br></pre></td></tr></table></figure>
<p>停止canal_m</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 canal_m]$ bin/stop.sh</span><br><span class="line">dp0652: stopping canal 27816 ...</span><br><span class="line">Oook! cost:1</span><br></pre></td></tr></table></figure>
<p>Instance会在slave节点即canal_s上启动</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 ~]$ tail -f canal_s/logs/example/example.log</span><br><span class="line">2017-10-12 15:17:21.452 [New I/O server worker #1-1] ERROR com.alibaba.otter.canal.server.netty.NettyUtils - ErrotCode:400 , Caused by :</span><br><span class="line">something goes wrong with channel:[id: 0x0c182149, /10.57.241.44:54008 =&gt; /192.168.6.52:11113], exception=com.alibaba.otter.canal.server.exception.CanalServerException: destination:example should start first</span><br><span class="line"></span><br><span class="line">2017-10-12 15:17:21.661 [pool-1-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]</span><br><span class="line">2017-10-12 15:17:21.663 [pool-1-thread-1] INFO  c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties]</span><br><span class="line">2017-10-12 15:17:21.767 [pool-1-thread-1] WARN  org.springframework.beans.TypeConverterDelegate - PropertyEditor [com.sun.beans.editors.EnumEditor] found through deprecated global PropertyEditorManager fallback - consider using a more isolated form of registration, e.g. on the BeanWrapper/BeanFactory!</span><br><span class="line">2017-10-12 15:17:21.968 [pool-1-thread-1] INFO  c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example</span><br><span class="line">2017-10-12 15:17:21.998 [pool-1-thread-1] INFO  c.a.otter.canal.instance.core.AbstractCanalInstance - start successful....</span><br><span class="line">2017-10-12 15:17:22.071 [destination = example , address = /127.0.0.1:3306 , EventParser] WARN  c.a.otter.canal.parse.inbound.mysql.MysqlEventParser - prepare to find start position just last position</span><br><span class="line"> &#123;&quot;identity&quot;:&#123;&quot;slaveId&quot;:-1,&quot;sourceAddress&quot;:&#123;&quot;address&quot;:&quot;localhost&quot;,&quot;port&quot;:3306&#125;&#125;,&quot;postion&quot;:&#123;&quot;included&quot;:false,&quot;journalName&quot;:&quot;mysql-bin.000004&quot;,&quot;position&quot;:1331,&quot;serverId&quot;:1,&quot;timestamp&quot;:1507791918000&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>停止canal_m后，只剩下canal_s，所以Canal集群只有一个节点了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 14] ls /otter/canal/cluster</span><br><span class="line">[192.168.6.52:11113]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 5] get /otter/canal/destinations/example/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;192.168.6.52:11113&quot;,&quot;cid&quot;:3&#125;</span><br></pre></td></tr></table></figure>
<p>切换过程中，Client的日志</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">2017-10-12 15:17:22.524 [Thread-2] WARN  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - failed to connect to:/192.168.6.52:11113 after retry 0 times</span><br><span class="line">2017-10-12 15:17:22.529 [Thread-2] WARN  c.a.otter.canal.client.impl.running.ClientRunningMonitor - canal is not run any in node</span><br><span class="line">2017-10-12 15:17:27.695 [Thread-2] INFO  c.alibaba.otter.canal.client.impl.ClusterCanalConnector - restart the connector for next round retry.</span><br><span class="line"></span><br><span class="line">****************************************************</span><br><span class="line">* Batch Id: [1] ,count : [1] , memsize : [75] , Time : 2017-10-12 15:17:27</span><br><span class="line">* Start : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line">* End : [mysql-bin.000004:1331:1507791918000(2017-10-12 15:05:18)] </span><br><span class="line">****************************************************</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1331] , executeTime : 1507791918000 , delay : 729763ms</span><br></pre></td></tr></table></figure>
<p>再次执行SQL语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [2] ,count : [3] , memsize : [198] , Time : 2017-10-12 15:20:56</span><br><span class="line">* Start : [mysql-bin.000004:1406:1507792855000(2017-10-12 15:20:55)] </span><br><span class="line">* End : [mysql-bin.000004:1581:1507792855000(2017-10-12 15:20:55)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1406] , executeTime : 1507792855000 , delay : 1539ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 763</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1532] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507792855000 , delay : 1539ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhx    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1581] , executeTime : 1507792855000 , delay : 1540ms</span><br></pre></td></tr></table></figure>
<p>停止客户端后，查询ZK中的客户端信息。注意，仍然有cursor信息，但是没有running，因为instance没有对应的client了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 1] ls /otter/canal/destinations/example</span><br><span class="line">[running, cluster, 1001]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 0] ls /otter/canal/destinations/example/1001</span><br><span class="line">[cursor]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 6] get /otter/canal/destinations/example/1001/cursor</span><br><span class="line">&#123;&quot;@type&quot;:&quot;com.alibaba.otter.canal.protocol.position.LogPosition&quot;,</span><br><span class="line">&quot;identity&quot;:&#123;&quot;slaveId&quot;:-1,&quot;sourceAddress&quot;:&#123;&quot;address&quot;:&quot;localhost&quot;,&quot;port&quot;:3306&#125;&#125;,</span><br><span class="line">&quot;postion&quot;:&#123;&quot;included&quot;:false,&quot;journalName&quot;:&quot;mysql-bin.000004&quot;,&quot;position&quot;:1581,&quot;serverId&quot;:1,&quot;timestamp&quot;:1507792855000&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>cursor信息是instance消费binlog的位置，即使客户端停掉了，也仍然保留在zk中。</p>
<blockquote>
<p>注意：1001是ClientIdentity的固定编号，相关源码在<a href="https://github.com/alibaba/canal/blob/master/client/src/main/java/com/alibaba/otter/canal/client/impl/SimpleCanalConnector.java#L88">SimpleCanalConnector</a>的构造方法里。</p>
</blockquote>
<p>下面总结下zk中的相关记录：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">/otter/canal/</span><br><span class="line">  |- cluster          ==&gt; [192.168.6.52:11112, 192.168.6.52:11113]</span><br><span class="line">  |- destinations     ==&gt; instances</span><br><span class="line">     |- example1/     ==&gt; instance name</span><br><span class="line">     |  |- cluster    ==&gt; [192.168.6.52:11112, 192.168.6.52:11113]</span><br><span class="line">     |  |- running    ==&gt; &#123;&quot;active&quot;:true,&quot;address&quot;:&quot;192.168.6.52:11112&quot;,&quot;cid&quot;:2&#125;</span><br><span class="line">     |  |- 1001</span><br><span class="line">     |     |- running  ==&gt; &#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.241.44:53942&quot;,&quot;clientId&quot;:1001&#125;</span><br><span class="line">     |     |- cursor  ==&gt; &#123;localhost:3306,&quot;journalName&quot;:&quot;mysql-bin.000004&quot;,&quot;position&quot;:1331,&quot;serverId&quot;:1&#125;</span><br><span class="line">     |- example2/</span><br><span class="line">     |  |- cluster    ==&gt; [192.168.6.52:11112, 192.168.6.52:11113]</span><br><span class="line">     |  |- running    ==&gt; &#123;&quot;active&quot;:true,&quot;address&quot;:&quot;192.168.6.52:11112&quot;,&quot;cid&quot;:2&#125;</span><br><span class="line">     |  |- 1001</span><br><span class="line">     |     |- running  ==&gt; &#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.241.44:53942&quot;,&quot;clientId&quot;:1001&#125;</span><br><span class="line">     |     |- cursor  ==&gt; &#123;localhost:3306,&quot;journalName&quot;:&quot;mysql-bin.000004&quot;,&quot;position&quot;:1331,&quot;serverId&quot;:1&#125;</span><br></pre></td></tr></table></figure>
<p>注意这里有两个running节点，第一个是CanalServer，第二个是CanalClient。</p>
<ul>
<li><code>/otter/canal/destinations/example1/running</code>: <em>{“active”:true,”address”:”192.168.6.52:11112”,”cid”:2}</em></li>
<li><code>/otter/canal/destinations/example1/1001/running</code>: <em>{“active”:true,”address”:”10.57.241.44:53942”,”clientId”:1001}</em></li>
</ul>
<p>下图是Canal Server HA的流程图：</p>
<ol>
<li>canal server要启动某个canal instance时都先向zookeeper进行一次尝试启动判断 (实现：创建EPHEMERAL节点，谁创建成功就允许谁启动)</li>
<li>创建zookeeper节点成功后，对应的canal server就启动对应的canal instance，没有创建成功的canal instance就会处于standby状态</li>
<li>一旦zookeeper发现canal server A创建的节点消失后，立即通知其他的canal server再次进行步骤1的操作，重新选出一个canal server启动instance.</li>
<li>canal client每次进行connect时，会首先向zookeeper询问当前是谁启动了canal instance，然后和其建立链接，一旦链接不可用，会重新尝试connect.</li>
</ol>
<p><img src="https://camo.githubusercontent.com/c8f1d98268a307821273e94e7eefcd29a26f9b78/687474703a2f2f646c2e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303038302f333330332f64333230326332362d653935342d333563302d613331392d3537363034313032633537642e6a7067" alt="server ha"></p>
<h2 id="Canal_Client_HA">Canal Client HA</h2><p>Canal Client的方式和canal server方式类似，也是利用zookeeper的抢占EPHEMERAL节点的方式进行控制。</p>
<blockquote>
<p>HA的实现，客户端是ClientRunningMonitor，服务端是ServerRunningMonitor。</p>
</blockquote>
<p>关于Canal Client HA的验证，可以参考：<a href="http://blog.csdn.net/xiaolinzi007/article/details/52933909" target="_blank" rel="noopener">http://blog.csdn.net/xiaolinzi007/article/details/52933909</a></p>
<ul>
<li>在IDEA中同时启动多个客户端，执行一条SQL语句，其中一个客户端会打印日志，另一个不会打印。</li>
<li>停止打印日志的那个客户端（在停止这个客户端之前，日志一直发动到这个客户端，不是负载均衡）。</li>
<li>再次执行SQL语句，另外一个客户端会打印日志。</li>
</ul>
<p>Client1的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [3] ,count : [3] , memsize : [198] , Time : 2017-10-12 17:59:59</span><br><span class="line">* Start : [mysql-bin.000004:1656:1507802398000(2017-10-12 17:59:58)] </span><br><span class="line">* End : [mysql-bin.000004:1831:1507802398000(2017-10-12 17:59:58)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1656] , executeTime : 1507802398000 , delay : 1188ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 768</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:1782] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507802398000 , delay : 1199ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqh    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:1831] , executeTime : 1507802398000 , delay : 1236ms</span><br><span class="line">## stop the canal client## canal client is down.</span><br></pre></td></tr></table></figure>
<p>停止Client1后，Client2的日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">****************************************************</span><br><span class="line">* Batch Id: [4] ,count : [3] , memsize : [198] , Time : 2017-10-12 18:02:15</span><br><span class="line">* Start : [mysql-bin.000004:1906:1507802534000(2017-10-12 18:02:14)] </span><br><span class="line">* End : [mysql-bin.000004:2081:1507802534000(2017-10-12 18:02:14)] </span><br><span class="line">****************************************************</span><br><span class="line"></span><br><span class="line">================&gt; binlog[mysql-bin.000004:1906] , executeTime : 1507802534000 , delay : 1807ms</span><br><span class="line"> BEGIN ----&gt; Thread id: 768</span><br><span class="line">----------------&gt; binlog[mysql-bin.000004:2032] , name[canal_test,test] , eventType : UPDATE , executeTime : 1507802534000 , delay : 1819ms</span><br><span class="line">uid : 1    type=int(4)</span><br><span class="line">name : zqhx    type=varchar(10)    update=true</span><br><span class="line">----------------</span><br><span class="line"> END ----&gt; transaction id: 0</span><br><span class="line">================&gt; binlog[mysql-bin.000004:2081] , executeTime : 1507802534000 , delay : 1855ms</span><br></pre></td></tr></table></figure>
<p>观察ZK节点中instance对应的client节点，在Client切换时，会进行变更。<br>比如下面的客户端从56806端口切换到了56842端口。<br>把所有客户端都关闭后，1001下没有running。表示instance没有客户端消费binlog了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">启动两个客户端，第一个客户端（56806）正在运行</span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 29] get /otter/canal/destinations/example/1001/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.241.44:56806&quot;,&quot;clientId&quot;:1001&#125;</span><br><span class="line"></span><br><span class="line">停止第一个客户端，删除节点</span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 30] get /otter/canal/destinations/example/1001/running</span><br><span class="line">Node does not exist: /otter/canal/destinations/example/1001/running</span><br><span class="line"></span><br><span class="line">第二个客户端（56842）成为主</span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 31] get /otter/canal/destinations/example/1001/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.241.44:56842&quot;,&quot;clientId&quot;:1001&#125;</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.52:2181(CONNECTED) 32] ls /otter/canal/destinations/example/1001</span><br><span class="line">[cursor]</span><br></pre></td></tr></table></figure>
<p>具体实现相关类有：ClientRunningMonitor/ClientRunningListener/ClientRunningData。</p>
<p>client running相关控制，主要为解决client自身的failover机制。<br>canal client允许同时启动多个canal client，<br>通过running机制，可保证只有一个client在工作，其他client做为冷备.<br>当运行中的client挂了，running会控制让冷备中的client转为工作模式，<br>这样就可以确保canal client也不会是单点. 保证整个系统的高可用性.</p>
<p>下图左边是客户端的HA实现，右边是服务端的HA实现</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171012184033228" alt="ha"></p>
<h2 id="Develop_Canal_Client"><a href="https://github.com/alibaba/canal/wiki/ClientAPI">Develop Canal Client</a></h2><p>先理解下面的类图结构：</p>
<ul>
<li>CanalConnector接口，定义了连接、订阅、获取、应答、回滚等方法</li>
<li>SimpleCanalConnector实现，单机版本</li>
<li>ClusterCanalConnector实现，HA版本</li>
</ul>
<p><img src="https://camo.githubusercontent.com/8cc684cf92e22d738d57b002c356afba96bcc4f5/687474703a2f2f646c322e69746579652e636f6d2f75706c6f61642f6174746163686d656e742f303039302f363435332f39326233343335302d323566632d333162332d626361362d3865326131653763356532322e6a7067" alt="client"></p>
<h3 id="subscribe_change">subscribe change</h3><p>重新看下CanalServerWithEmbedded的订阅方法。我们知道客户端在连接服务端的某个destination之后，会紧接着调用subscribe()方法。</p>
<p>客户端连接服务端时，必须指定destination名称，因为一个服务端可能有多个destination。<br>比如服务端启动了两个Instance，它们的destination名称分别是example1和example2。<br>假设有两个客户端A和B，A连接example1，B连接example2（在代码中手动指定的，不是自动选择）。<br>服务端的canalInstances字典为：{example1=&gt;Instance1，example2-&gt;Instance2}。<br>那么ClientA的destination等于example1，对应的服务端实例为Instance1。<br>ClientB的destination等于example2，对应的服务端实例为Instance3。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171012230738279" alt="clients"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * 客户端订阅，重复订阅时会更新对应的filter信息</span><br><span class="line"> */</span><br><span class="line">public void subscribe(ClientIdentity clientIdentity) throws CanalServerException &#123;</span><br><span class="line">    CanalInstance canalInstance = canalInstances.get(clientIdentity.getDestination());</span><br><span class="line">    if (!canalInstance.getMetaManager().isStart()) &#123;</span><br><span class="line">        canalInstance.getMetaManager().start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    canalInstance.getMetaManager().subscribe(clientIdentity); // 执行一下meta订阅</span><br><span class="line"></span><br><span class="line">    // 根据Client从MetaManager中获取最近一次的Cursor</span><br><span class="line">    Position position = canalInstance.getMetaManager().getCursor(clientIdentity);</span><br><span class="line">    if (position == null) &#123; // 如果没有</span><br><span class="line">        position = canalInstance.getEventStore().getFirstPosition();// 获取一下store中的第一条</span><br><span class="line">        if (position != null) &#123;</span><br><span class="line">            canalInstance.getMetaManager().updateCursor(clientIdentity, position); // 更新一下cursor</span><br><span class="line">        &#125;</span><br><span class="line">        logger.info(&quot;subscribe successfully, &#123;&#125; with first position:&#123;&#125; &quot;, clientIdentity, position);</span><br><span class="line">    &#125; else &#123; // 有就直接使用</span><br><span class="line">        logger.info(&quot;subscribe successfully, use last cursor position:&#123;&#125; &quot;, clientIdentity, position);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // 通知下订阅关系变化</span><br><span class="line">    canalInstance.subscribeChange(clientIdentity);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里面关于订阅方法有两个地方，CanalInstance本身调用了subscribeChange，它关联的MetaManager也调用了subscribe方法。</p>
<p>一个CanalServer可以有多个CanalInstance，每个Instance都会有一个MetaManager。<br>而一个Instance对应一个Client。那么，这么说来，一个MetaManager也就只会有一个Client了。<br>但是从下面的数据结构来看的话，一个MetaManager貌似可以有多个Destination。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">public class MemoryMetaManager extends AbstractCanalLifeCycle implements CanalMetaManager &#123;</span><br><span class="line">    protected Map&lt;String, List&lt;ClientIdentity&gt;&gt;              destinations;</span><br><span class="line">    protected Map&lt;ClientIdentity, MemoryClientIdentityBatch&gt; batches;</span><br><span class="line">    protected Map&lt;ClientIdentity, Position&gt;                  cursors;</span><br><span class="line"></span><br><span class="line">    public synchronized void subscribe(ClientIdentity clientIdentity) throws CanalMetaManagerException &#123;</span><br><span class="line">        List&lt;ClientIdentity&gt; clientIdentitys = destinations.get(clientIdentity.getDestination());</span><br><span class="line">        if (clientIdentitys.contains(clientIdentity)) &#123;</span><br><span class="line">            clientIdentitys.remove(clientIdentity);</span><br><span class="line">        &#125;</span><br><span class="line">        clientIdentitys.add(clientIdentity);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>猜测：多个Client可以连接到同一个Instance（虽然只会有一个Instance起作用），所以一个MetaManager可以管理多个Client。<br>NO！Client的HA与MetaManager记录的Client是不一样的。HA表示同一时间只有一个Client起作用，那么MetaManager不可能同时记录两个Client。</p>
</blockquote>
<blockquote>
<p>官方ClientAPI文档上：ClientIdentity是canal client和server交互之间的身份标识，目前clientId写死为1001.<br><strong><font color="red" size="5">目前canal server上的一个instance只能有一个client消费</font></strong>，<br>clientId的设计是为1个instance多client消费模式而预留的，暂时不需要理会。</p>
</blockquote>
<p>也就是说：一个Instance还是有可能有多个Client连接上来的，只是目前只允许一个而已！！！</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171012234337736" alt="subscribes"></p>
<p>这里的数据结构为什么这么设计，还需要参考<em>AbstractMetaManagerTest</em>的<em>doSubscribeTest</em>方法来理解。</p>
<p>对于相同的destination，可以订阅不同的client。下面的示例分别订阅了[client1,client2]和[client1,client3]。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">public void doSubscribeTest(CanalMetaManager metaManager) &#123;</span><br><span class="line">    ClientIdentity client1 = new ClientIdentity(destination, (short) 1);</span><br><span class="line">    metaManager.subscribe(client1);</span><br><span class="line">    metaManager.subscribe(client1); // 重复调用：删除旧的client1，并继续增加新的client1</span><br><span class="line">    ClientIdentity client2 = new ClientIdentity(destination, (short) 2);</span><br><span class="line">    metaManager.subscribe(client2);</span><br><span class="line"></span><br><span class="line">    List&lt;ClientIdentity&gt; clients = metaManager.listAllSubscribeInfo(destination);</span><br><span class="line">    Assert.assertEquals(Arrays.asList(client1, client2), clients);</span><br><span class="line"></span><br><span class="line">    metaManager.unsubscribe(client2);</span><br><span class="line">    ClientIdentity client3 = new ClientIdentity(destination, (short) 3);</span><br><span class="line">    metaManager.subscribe(client3);</span><br><span class="line"></span><br><span class="line">    clients = metaManager.listAllSubscribeInfo(destination);</span><br><span class="line">    Assert.assertEquals(Arrays.asList(client1, client3), clients);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>有不懂的地方，可以看看测试用例，验证自己的想法是否正确。</p>
</blockquote>
<p><strong>CanalServerWithEmbedded</strong>的订阅方法最后还会调用<strong>AbstractCanalInstance</strong>的<code>subscribeChange</code>方法。<br>这里会设置表名的filter，以及黑名单。配置项在instance.properties中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># table regex</span><br><span class="line">canal.instance.filter.regex = .*\\..*</span><br><span class="line"># table black regex</span><br><span class="line">canal.instance.filter.black.regex =</span><br></pre></td></tr></table></figure>
<p>filter表示客户端要通过Canal Server获取MySQL哪些表的binlog，上面配置项表示获取所有表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">public class AbstractCanalInstance extends AbstractCanalLifeCycle implements CanalInstance &#123;</span><br><span class="line">    protected Long                                   canalId;                                                      // 和manager交互唯一标示</span><br><span class="line">    protected String                                 destination;                                                  // 队列名字</span><br><span class="line">    protected CanalEventStore&lt;Event&gt;                 eventStore;                                                   // 有序队列</span><br><span class="line"></span><br><span class="line">    protected CanalEventParser                       eventParser;                                                  // 解析对应的数据信息</span><br><span class="line">    protected CanalEventSink&lt;List&lt;CanalEntry.Entry&gt;&gt; eventSink;                                                    // 链接parse和store的桥接器</span><br><span class="line">    protected CanalMetaManager                       metaManager;                                                  // 消费信息管理器</span><br><span class="line">    protected CanalAlarmHandler                      alarmHandler;                                                 // alarm报警机制</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public boolean subscribeChange(ClientIdentity identity) &#123;</span><br><span class="line">        if (StringUtils.isNotEmpty(identity.getFilter())) &#123;</span><br><span class="line">            logger.info(&quot;subscribe filter change to &quot; + identity.getFilter());</span><br><span class="line">            AviaterRegexFilter aviaterFilter = new AviaterRegexFilter(identity.getFilter());</span><br><span class="line"></span><br><span class="line">            boolean isGroup = (eventParser instanceof GroupEventParser);</span><br><span class="line">            if (isGroup) &#123;</span><br><span class="line">                // 处理group的模式</span><br><span class="line">                List&lt;CanalEventParser&gt; eventParsers = ((GroupEventParser) eventParser).getEventParsers();</span><br><span class="line">                for (CanalEventParser singleEventParser : eventParsers) &#123;// 需要遍历启动</span><br><span class="line">                    ((AbstractEventParser) singleEventParser).setEventFilter(aviaterFilter);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                ((AbstractEventParser) eventParser).setEventFilter(aviaterFilter);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // filter的处理规则</span><br><span class="line">        // a. parser处理数据过滤处理</span><br><span class="line">        // b. sink处理数据的路由&amp;分发,一份parse数据经过sink后可以分发为多份，每份的数据可以根据自己的过滤规则不同而有不同的数据</span><br><span class="line">        // 后续内存版的一对多分发，可以考虑</span><br><span class="line">        return true;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>对应在EventParser中，存在两个Filter的引用。比如上面eventParser.setEventFilter()方法会设置AbstractEventParser的eventFilter。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public abstract class AbstractEventParser&lt;EVENT&gt; extends AbstractCanalLifeCycle implements CanalEventParser&lt;EVENT&gt; &#123;</span><br><span class="line">    protected CanalLogPositionManager                logPositionManager         = null;</span><br><span class="line">    protected CanalEventSink&lt;List&lt;CanalEntry.Entry&gt;&gt; eventSink                  = null;</span><br><span class="line">    protected CanalEventFilter                       eventFilter                = null;</span><br><span class="line">    protected CanalEventFilter                       eventBlackFilter           = null;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="EventParser_Implement">EventParser Implement</h3><p>AbstractEventParser的start()方法是解析binlog的主要方法。<br>在启动transactionBuffer和BinLogParser后，<br>会启动一个后台的工作线程<strong>parseThread</strong>一直运行：  </p>
<p>注意：下面的几个步骤是嵌套在一个while死循环里，最后会进行sleep。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// 开始执行replication</span><br><span class="line">// 1. 构造Erosa连接</span><br><span class="line">erosaConnection = buildErosaConnection();</span><br><span class="line"></span><br><span class="line">// 2. 启动一个心跳线程</span><br><span class="line">startHeartBeat(erosaConnection);</span><br><span class="line"></span><br><span class="line">// 3. 执行dump前的准备工作</span><br><span class="line">preDump(erosaConnection);</span><br><span class="line"></span><br><span class="line">// 4. 连接MySQL数据库</span><br><span class="line">erosaConnection.connect(); </span><br><span class="line"></span><br><span class="line">// 5. 获取最后的位置信息</span><br><span class="line">EntryPosition startPosition = findStartPosition(erosaConnection);</span><br><span class="line">logger.info(&quot;find start position : &#123;&#125;&quot;, startPosition.toString());</span><br><span class="line">// 重新链接，因为在找position过程中可能有状态，需要断开后重建</span><br><span class="line">erosaConnection.reconnect();</span><br><span class="line"></span><br><span class="line">// 定义回调函数，当解析成功后，sink()方法会暂存到缓冲区transactionBuffer中。缓冲区的数据会通过心跳线程放入EventSink</span><br><span class="line">final SinkFunction sinkHandler = new SinkFunction&lt;EVENT&gt;() &#123;</span><br><span class="line">    private LogPosition lastPosition;</span><br><span class="line"></span><br><span class="line">    public void sink(EVENT event) &#123;</span><br><span class="line">        CanalEntry.Entry entry = parseAndProfilingIfNecessary(event);</span><br><span class="line">        if (entry != null) &#123;</span><br><span class="line">            transactionBuffer.add(entry);</span><br><span class="line">            this.lastPosition = buildLastPosition(entry);  // 记录一下对应的positions</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 6. 开始dump数据</span><br><span class="line">if (StringUtils.isEmpty(startPosition.getJournalName()) &amp;&amp; startPosition.getTimestamp() != null) &#123;</span><br><span class="line">    erosaConnection.dump(startPosition.getTimestamp(), sinkHandler);</span><br><span class="line">&#125; else &#123;</span><br><span class="line">    erosaConnection.dump(startPosition.getJournalName(), startPosition.getPosition(), sinkHandler);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里的erosaConnection指的是Canal Server到MySQL的连接。<br>而前面我们说的客户端（CanalClient）连接CanalConnector指的是CanalClient到CanalServer的连接。</p>
<p><strong><font color="red" size="3">CanalServer到MySQL的连接是要获取binlog的dump数据包。而CanalClient到CanalServer有多种请求（GET/ACK等）。</font></strong></p>
<p>我们不会具体分析<em>dump</em>的流程，不过粗略看下erosaConnection的MySQL实现<strong>MysqlConnection</strong>是如何在获取到事件后调用回调函数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public void dump(String binlogfilename, Long binlogPosition, SinkFunction func) throws IOException &#123;</span><br><span class="line">    updateSettings();</span><br><span class="line">    sendBinlogDump(binlogfilename, binlogPosition);</span><br><span class="line">    // connector指的是CanalServer到MySQL Master服务器的连接，创建一个拉取线程拉取MySQL的binlog</span><br><span class="line">    DirectLogFetcher fetcher = new DirectLogFetcher(connector.getReceiveBufferSize());</span><br><span class="line">    fetcher.start(connector.getChannel());</span><br><span class="line">    LogDecoder decoder = new LogDecoder(LogEvent.UNKNOWN_EVENT, LogEvent.ENUM_END_EVENT);</span><br><span class="line">    LogContext context = new LogContext();</span><br><span class="line">    while (fetcher.fetch()) &#123; // 由于设置了缓冲区的大小，每次dump都只会拉取一批数据</span><br><span class="line">        LogEvent event = null;</span><br><span class="line">        event = decoder.decode(fetcher, context);</span><br><span class="line">        if (!func.sink(event)) break; // 调用回调方法</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>服务端有一个心跳线程，它的目的是消费<em>transactionBuffer</em>，并写入到<strong>EventSink</strong>中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">protected boolean consumeTheEventAndProfilingIfNecessary(List&lt;CanalEntry.Entry&gt; entrys) &#123;</span><br><span class="line">    boolean result = eventSink.sink(entrys, </span><br><span class="line">        (runningInfo == null) ? null : runningInfo.getAddress(), destination);</span><br><span class="line">    return result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>EventSink</strong>最终会将数据写入到<strong>EventStore</strong>中，即<em>Put</em>到<strong>RingBuffer</strong>中。回顾下这张图：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171011211529169" alt="ops"></p>
<h2 id="CanalController">CanalController</h2><p>前面分析了这么多，一直没分析Canal服务是怎么起来的，其实很简单，<br>执行脚本startup.sh本质上通过CanalLauncher会启动CanalController。</p>
<h2 id="eunomia">eunomia</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: 192.168.6.55:2181(CONNECTED) 3] ls /otter/canal/destinations</span><br><span class="line">[octopus_demeter, example_bak, namelist_test, xiaopang2, namelist2, xiaopang3, namelist1, example, xiaopang]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.55:2181(CONNECTED) 4] ls /otter/canal/destinations/xiaopang</span><br><span class="line">[eunomia, cluster, 1001, running]</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.55:2181(CONNECTED) 5] ls /otter/canal/destinations/xiaopang/eunomia</span><br><span class="line">[_c_2a900d4e-75fb-4445-b30c-04e1bdb2e5d9-lock-0001381746, runnning, _c_ea33db37-9193-4c75-9e61-85e59e123109-lock-0001381738]</span><br><span class="line"></span><br><span class="line">// Eunomia Server？还是Canal Client？</span><br><span class="line">[zk: 192.168.6.55:2181(CONNECTED) 7] get /otter/canal/destinations/xiaopang/eunomia/runnning</span><br><span class="line">10.57.17.100</span><br><span class="line"></span><br><span class="line">[zk: 192.168.6.55:2181(CONNECTED) 18] get /otter/canal/destinations/xiaopang/1001/running</span><br><span class="line">&#123;&quot;active&quot;:true,&quot;address&quot;:&quot;10.57.17.100:60661&quot;,&quot;clientId&quot;:1001&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/alibaba/canal&quot;&gt;canal&lt;/a&gt;: 阿里巴巴mysql数据库binlog的增量订阅&amp;amp;消费组件&lt;br&gt;
    
    </summary>
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/categories/midd/"/>
    
    
      <category term="midd" scheme="http://github.com/zqhxuyuan/tags/midd/"/>
    
  </entry>
  
  <entry>
    <title>Spark DataSources Implementation</title>
    <link href="http://github.com/zqhxuyuan/2017/09/15/2017-09-15-Spark-DataSources/"/>
    <id>http://github.com/zqhxuyuan/2017/09/15/2017-09-15-Spark-DataSources/</id>
    <published>2017-09-14T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.260Z</updated>
    
    <content type="html"><![CDATA[<p>Spark数据源扩展与实践(40行代码实现一个自定义的DataSource)<br><a id="more"></a></p>
<h2 id="简单示例">简单示例</h2><p>Spark的DataSource API可以方便地扩展。如果没有使用META-INFO这种ServiceLocator机制，则自定义的数据源名称必须是DefaultSource.<br>并且必须实现RelationProvider接口。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class DefaultSource extends RelationProvider &#123;</span><br><span class="line">  override def createRelation(sqlContext: SQLContext,</span><br><span class="line">                              parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">    ???</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通常自定义数据源都有不同的配置文件，所以我们也要实现自己的BaseRelation</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class DefaultSource extends RelationProvider&#123;</span><br><span class="line">  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">    EmptyRelation()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class EmptyRelation() extends BaseRelation &#123;</span><br><span class="line">  override def sqlContext: SQLContext = ???</span><br><span class="line">  override def schema: StructType = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>主要的起始还是BaseRelation的实现类，但是这里怎么获取schema和SQLContext呢。由于DefaultSource的createRelation方法中已经有SQLContext。所以我们可以改成</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class DefaultSource extends RelationProvider&#123;</span><br><span class="line">  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">    EmptyRelation()(sqlContext)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class EmptyRelation()(@transient val sc: SQLContext) extends BaseRelation &#123;</span><br><span class="line">  override def sqlContext: SQLContext = sc</span><br><span class="line"></span><br><span class="line">  override def schema: StructType = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么Schema怎么确定呢？通常它需要从DefaultSource的createRelation方法的parameters确定。<br>所以通常我们会给自定义的BaseRelation加上一个参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class DefaultSource extends RelationProvider&#123;</span><br><span class="line">  override def createRelation(sqlContext: SQLContext, parameters: Map[String, String]): BaseRelation = &#123;</span><br><span class="line">    EmptyRelation(parameters)(sqlContext)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class EmptyRelation(parameters: Map[String, String])(@transient val sc: SQLContext) extends BaseRelation &#123;</span><br><span class="line">  override def sqlContext: SQLContext = sc</span><br><span class="line"></span><br><span class="line">  override def schema: StructType = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个schema的具体实现必须依赖于如何读取数据源。所以EmptyRelation还需要实现另外一个接口：TableScan</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">case class EmptyRelation(parameters: Map[String, String])</span><br><span class="line">                        (@transient val sc: SQLContext) </span><br><span class="line">  extends BaseRelation with TableScan&#123;</span><br><span class="line">  override def sqlContext: SQLContext = sc</span><br><span class="line"></span><br><span class="line">  override def schema: StructType = ???</span><br><span class="line"></span><br><span class="line">  override def buildScan(): RDD[Row] = ???</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>现在有两个方法需要我们自己实现。buildScan表示如何读取数据源，并生成<code>RDD[ROW]</code>。<br>下面以一个简单的示例入门：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">case class EmptyRelation(parameters: Map[String, String])</span><br><span class="line">                        (@transient val sc: SQLContext) </span><br><span class="line">  extends BaseRelation with TableScan&#123;</span><br><span class="line">  override def sqlContext: SQLContext = sc</span><br><span class="line"></span><br><span class="line">  override def schema: StructType = &#123;</span><br><span class="line">    StructType(List(</span><br><span class="line">      StructField(&quot;id&quot;, IntegerType), </span><br><span class="line">      StructField(&quot;name&quot;, StringType),</span><br><span class="line">      StructField(&quot;age&quot;, IntegerType)</span><br><span class="line">    ))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def buildScan(): RDD[Row] = &#123;</span><br><span class="line">    val rdd = sqlContext.sparkContext.parallelize(</span><br><span class="line">      List(</span><br><span class="line">        (1, &quot;A&quot;, 20),</span><br><span class="line">        (2, &quot;B&quot;, 25)</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    rdd.map(row =&gt; Row.fromSeq(Seq(row._1, row._2, row._3)))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来就可以运行测试例子了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">object TestExample &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    val spark = SparkSession.builder().master(&quot;local&quot;).getOrCreate()</span><br><span class="line">    val df = spark.read.format(&quot;com.zqh.spark.connectors.test.empty&quot;).load()</span><br><span class="line">    df.printSchema()</span><br><span class="line">    df.show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>什么，只有40行代码，就实现了自定义的DataSource!!!</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line"> |-- id: integer (nullable = true)</span><br><span class="line"> |-- name: string (nullable = true)</span><br><span class="line"> |-- age: integer (nullable = true)</span><br><span class="line"></span><br><span class="line">+---+----+---+</span><br><span class="line">| id|name|age|</span><br><span class="line">+---+----+---+</span><br><span class="line">|  1|   A| 20|</span><br><span class="line">+---+----+---+</span><br></pre></td></tr></table></figure>
<p>上面示例EmptyRelation中，schema方法和buildScan方法有如下特点：</p>
<ul>
<li>schema定义了三个字段，则buildScan中每一行Row都必须有三个元素</li>
<li>RDD的每一行Row是数据，而schema对应了数据的元数据，schema可以任意指定</li>
</ul>
<p>总结下自定义数据源相关的类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">RelationProvider                  BaseRelation    TableScan</span><br><span class="line">       /|\                            /|\            /|\                   spark</span><br><span class="line">        |                              |              |        ------------------</span><br><span class="line">        |                              |              |                    user</span><br><span class="line">        |                           schema()     buildScan()    </span><br><span class="line">DefaultSource                          |              |</span><br><span class="line">        |                              |              |</span><br><span class="line">        |                              |              |</span><br><span class="line">        ·                              |              |</span><br><span class="line">createRelation()  --------------------&gt; EmptyRelation</span><br></pre></td></tr></table></figure>
<h2 id="JDBC_DataSource">JDBC DataSource</h2><p>开启mysql的查询日志，对应的日志文件是<code>/usr/local/var/mysql/zqhmac.log</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; set GLOBAL general_log = on;</span><br><span class="line">Query OK, 0 rows affected (0.08 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; show VARIABLES like &apos;%general_log%&apos;;</span><br><span class="line">+------------------+---------------------------------+</span><br><span class="line">| Variable_name    | Value                           |</span><br><span class="line">+------------------+---------------------------------+</span><br><span class="line">| general_log      | ON                              |</span><br><span class="line">| general_log_file | /usr/local/var/mysql/zqhmac.log |</span><br><span class="line">+------------------+---------------------------------+</span><br></pre></td></tr></table></figure>
<p>spark读取jdbc有多种方式：</p>
<h3 id="1-_全量读取，只有一个分区">1. 全量读取，只有一个分区</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val url = &quot;jdbc:mysql://localhost/test&quot;</span><br><span class="line">val table = &quot;test&quot;</span><br><span class="line">val properties = new java.util.Properties</span><br><span class="line">properties.put(&quot;user&quot;, &quot;root&quot;)</span><br><span class="line">properties.put(&quot;password&quot;, &quot;root&quot;)</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url, table, properties)</span><br><span class="line"></span><br><span class="line">df.rdd.partitions.size # 1</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2008 Query SELECT `id`,`name`,`total` FROM test</span><br></pre></td></tr></table></figure>
<p>Spark UI上可以看到只有一个Executor和一个Task：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019193301335" alt="jdbc spark"></p>
<p>如果数据量太大，就会报错OOM：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20171019193323001" alt="oom"></p>
<h3 id="2-_指定上下界，自动分片">2. 指定上下界，自动分片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val columnName = &quot;id&quot;</span><br><span class="line">val lowerBound = 1</span><br><span class="line">val upperBound = 1000</span><br><span class="line">val numPartitions = 5</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url,table,columnName,lowerBound,upperBound,numPartitions,properties)</span><br><span class="line"></span><br><span class="line">df.rdd.partitions.size # 指定的分区数量</span><br></pre></td></tr></table></figure>
<p>指定上下界有个限制条件是分区字段必须是整数类型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def jdbc(</span><br><span class="line">    url: String,</span><br><span class="line">    table: String,</span><br><span class="line">    columnName: String,</span><br><span class="line">    lowerBound: Long,</span><br><span class="line">    upperBound: Long,</span><br><span class="line">    numPartitions: Int,</span><br><span class="line">    connectionProperties: Properties): DataFrame = &#123;</span><br><span class="line">  // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.</span><br><span class="line">  this.extraOptions ++= Map(</span><br><span class="line">    JDBCOptions.JDBC_PARTITION_COLUMN -&gt; columnName,</span><br><span class="line">    JDBCOptions.JDBC_LOWER_BOUND -&gt; lowerBound.toString,</span><br><span class="line">    JDBCOptions.JDBC_UPPER_BOUND -&gt; upperBound.toString,</span><br><span class="line">    JDBCOptions.JDBC_NUM_PARTITIONS -&gt; numPartitions.toString)</span><br><span class="line">  jdbc(url, table, connectionProperties)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>spark的做法是根据上下界，分区个数，自动切分。这种场景主要针对数据库的主键是自增字段（当然是整数了）。</p>
<p>因为自增的数字分布很均匀，所以给定上下界和分区的数量，每个分区拉取的数据也是很均匀的。</p>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2010 Query SELECT `id`,`name`,`total` FROM test WHERE id &lt; 201 or id is null</span><br><span class="line">2011 Query SELECT `id`,`name`,`total` FROM test WHERE id &gt;= 201 AND id &lt; 401</span><br><span class="line">2012 Query SELECT `id`,`name`,`total` FROM test WHERE id &gt;= 401 AND id &lt; 601</span><br><span class="line">2013 Query SELECT `id`,`name`,`total` FROM test WHERE id &gt;= 601 AND id &lt; 801</span><br><span class="line">2014 Query SELECT `id`,`name`,`total` FROM test WHERE id &gt;= 801</span><br></pre></td></tr></table></figure>
<h3 id="3-_手动构造predicates">3. 手动构造predicates</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">val predicates = Array(</span><br><span class="line">  &quot;id&gt;=0 and id&lt;10&quot;,</span><br><span class="line">  &quot;id&gt;=10 and id&lt;100&quot;,</span><br><span class="line">  &quot;id&gt;=100 and id&lt;1000&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url, table, predicates, properties)</span><br><span class="line"></span><br><span class="line">df.rdd.partitions.size # 3，predicates数组有几个，对应几个分区</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2016 Query SELECT `id`,`name`,`total` FROM test WHERE id&gt;=0 and id&lt;10</span><br><span class="line">2017 Query SELECT `id`,`name`,`total` FROM test WHERE id&gt;=10 and id&lt;100</span><br><span class="line">2018 Query SELECT `id`,`name`,`total` FROM test WHERE id&gt;=100 and id&lt;1000</span><br></pre></td></tr></table></figure>
<p>如果数据分布不均匀，可以采用这种方式，而且这种方式不限于主键、整数类型，可以是任意类型，任意字段。</p>
<p>比如我们的测试mysql表数据如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from test;</span><br><span class="line">+-----+------+-------+</span><br><span class="line">| id  | name | total |</span><br><span class="line">+-----+------+-------+</span><br><span class="line">|   1 | A    |     1 |</span><br><span class="line">|   2 | B    |     2 |</span><br><span class="line">|   3 | C    |     3 |</span><br><span class="line">|  11 | A    |    12 |</span><br><span class="line">|  12 | B    |    12 |</span><br><span class="line">|  13 | C    |    12 |</span><br><span class="line">| 100 | 1    |     0 |</span><br><span class="line">| 101 | 2    |     1 |</span><br><span class="line">| 102 | 2    |     1 |</span><br><span class="line">+-----+------+-------+</span><br></pre></td></tr></table></figure>
<p>现在要根据name列进行手动指定查询方式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val predicates = Array(</span><br><span class="line">  &quot;name = &apos;A&apos;&quot;,</span><br><span class="line">  &quot;name = &apos;B&apos;&quot;,</span><br><span class="line">  &quot;name = &apos;C&apos;&quot;,</span><br><span class="line">  &quot;name in(&apos;1&apos;,&apos;2&apos;)&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url, table, predicates, properties)</span><br><span class="line"></span><br><span class="line">df.show</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2020 Query SELECT `id`,`name`,`total` FROM test WHERE name = &apos;A&apos;</span><br><span class="line">2022 Query SELECT `id`,`name`,`total` FROM test WHERE name = &apos;C&apos;</span><br><span class="line">2023 Query SELECT `id`,`name`,`total` FROM test WHERE name = &apos;B&apos;</span><br><span class="line">2021 Query SELECT `id`,`name`,`total` FROM test WHERE name in(&apos;1&apos;,&apos;2&apos;)</span><br></pre></td></tr></table></figure>
<p>由于是自定义查询条件，所以我们可以使用任何方式，比如limit方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val predicates = Array(</span><br><span class="line">  &quot;1=1 order by name limit 3 offset 0&quot;,</span><br><span class="line">  &quot;1=1 order by name limit 3 offset 3&quot;,</span><br><span class="line">  &quot;1=1 order by name limit 3 offset 6&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url, table, predicates, properties)</span><br><span class="line">df.count</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2025 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 3</span><br><span class="line">2026 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 6</span><br><span class="line">2027 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 0</span><br></pre></td></tr></table></figure>
<p>动态指定排序字段和个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val orderByColumn = &quot;name&quot;</span><br><span class="line">val limitCount = 3</span><br><span class="line">val predicates = Array(</span><br><span class="line">  s&quot;1=1 order by $orderByColumn limit $limitCount offset 0&quot;,</span><br><span class="line">  s&quot;1=1 order by $orderByColumn limit $limitCount offset $&#123;limitCount&#125;&quot;,</span><br><span class="line">  s&quot;1=1 order by $orderByColumn limit $limitCount offset $&#123;limitCount*2&#125;&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">val df = spark.read.jdbc(url, table, predicates, properties)</span><br><span class="line">df.count</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2030 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 3</span><br><span class="line">2029 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 0</span><br><span class="line">2031 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 6</span><br></pre></td></tr></table></figure>
<p>当然上面的predicates还是不够智能，正确的做法是先查询总数，然后根据limitCount构造predicates数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val orderByColumn = &quot;name&quot;</span><br><span class="line">val limitCount = 3</span><br><span class="line">//val totalCount = spark.read.jdbc(url, table, properties).count  // 日志：SELECT 1 FROM test</span><br><span class="line">val countDF = spark.read.jdbc(url, s&quot;(select count(*) from $table) tmp&quot;, properties) // SELECT * FROM (select count(*) from test) tmp WHERE 1=0</span><br><span class="line">val totalCount = countDF.take(1)(0).getAs[Long](0) // SELECT `count(*)` FROM (select count(*) from test) tmp</span><br><span class="line"></span><br><span class="line">val split = totalCount / limitCount</span><br><span class="line">val predicates = for(i &lt;- 0l to split) yield s&quot;1=1 order by $orderByColumn limit $limitCount offset $&#123;limitCount * i&#125;&quot;</span><br><span class="line">val df = spark.read.jdbc(url, table, predicates.toArray, properties)</span><br><span class="line">df.count</span><br></pre></td></tr></table></figure>
<p>后台日志：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2050 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 0</span><br><span class="line">2051 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 6</span><br><span class="line">2052 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 3</span><br><span class="line">2053 Query SELECT 1 FROM test WHERE 1=1 order by name limit 3 offset 9</span><br></pre></td></tr></table></figure>
<h2 id="JDBC实现">JDBC实现</h2><p>spark.read.jdbc进入DataFrameReader，真正执行在load()方法中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def load(paths: String*): DataFrame = &#123;</span><br><span class="line">  sparkSession.baseRelationToDataFrame(</span><br><span class="line">    DataSource.apply(</span><br><span class="line">      sparkSession,</span><br><span class="line">      paths = paths,</span><br><span class="line">      userSpecifiedSchema = userSpecifiedSchema,</span><br><span class="line">      className = source,</span><br><span class="line">      options = extraOptions.toMap).resolveRelation())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>JDBC格式对应的Provider就定义在DataSource中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">object DataSource extends Logging &#123;</span><br><span class="line">  private val backwardCompatibilityMap: Map[String, String] = &#123;</span><br><span class="line">    val jdbc = classOf[JdbcRelationProvider].getCanonicalName</span><br><span class="line">    val json = classOf[JsonFileFormat].getCanonicalName</span><br><span class="line">    val csv = classOf[CSVFileFormat].getCanonicalName</span><br><span class="line">    Map(</span><br><span class="line">      &quot;org.apache.spark.sql.jdbc&quot; -&gt; jdbc,</span><br><span class="line">      &quot;org.apache.spark.sql.json&quot; -&gt; json,</span><br><span class="line">      &quot;com.databricks.spark.csv&quot; -&gt; csv</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>jdbc数据源的定义类是：JdbcRelationProvider</p>
<h2 id="JDBC扩展">JDBC扩展</h2><p>参考: <a href="http://blog.csdn.net/cjuexuan/article/details/52333970" target="_blank" rel="noopener">http://blog.csdn.net/cjuexuan/article/details/52333970</a></p>
<p>category是唯一键，存在则更新num，不存在则插入category,num。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO ip_category_count</span><br><span class="line">(category,num,createTime)</span><br><span class="line">VALUES(?,?,CURRENT_TIMESTAMP)</span><br><span class="line">ON DUPLICATE KEY UPDATE</span><br><span class="line">num=?,updateTime=CURRENT_TIMESTAMP</span><br></pre></td></tr></table></figure>
<p>对应的Statemen写法， set时从1开始，get时从0开始：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ps.setInt(1, row.getInt(0))</span><br><span class="line">ps.setLong(2, row.getLong(1))</span><br><span class="line">ps.setLong(3, row.getLong(1))</span><br><span class="line">ps.executeUpdate()</span><br></pre></td></tr></table></figure>
<p>假设有下面的SQL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO test_1 (`id`,`year`,count`) VALUES (?,?,?)</span><br><span class="line">ON DUPLICATE KEY UPDATE `id`=?,`year`=?,`count`=?</span><br></pre></td></tr></table></figure>
<p>对应的写法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ps.setInt(1, row.getInt(0))</span><br><span class="line">ps.setString(2, row.getLong(1))</span><br><span class="line">ps.setLong(3, row.getLong(2))</span><br><span class="line">-------------------------------</span><br><span class="line">ps.setInt(4, row.getInt(0))</span><br><span class="line">ps.setString(5, row.getLong(1))</span><br><span class="line">ps.setLong(6, row.getLong(2))</span><br></pre></td></tr></table></figure>
<p>总结出来的规则：<code>stmt.setInt(pos + 1, row.getInt(pos - offset))</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. i&lt;midField,  position=i, offset=0        =&gt; stmt.setInt(i + 1, row.getInt(i - 0))</span><br><span class="line">2. i&gt;=midField, position=i, offset=midField =&gt; stmt.setInt(i + 1, row.getInt(i - midField))</span><br></pre></td></tr></table></figure>
<p>以3个字段为例，当<code>i&lt;midField</code>时：</p>
<ul>
<li>i=0: stmt.setInt(0 + 1, row.getInt(0 - 0)), stmt.setInt(1, row.getInt(0))</li>
<li>i=1: stmt.setInt(1 + 1, row.getInt(1 - 0)), stmt.setInt(2, row.getInt(1))</li>
<li>i=2: stmt.setInt(2 + 1, row.getInt(2 - 0)), stmt.setInt(3, row.getInt(2))</li>
</ul>
<p>当<code>i&gt;=midField</code>时：</p>
<ul>
<li>i=3: stmt.setInt(3 + 1, row.getInt(3 - 3)), stmt.setInt(3, row.getInt(0))</li>
<li>i=4: stmt.setInt(4 + 1, row.getInt(4 - 3)), stmt.setInt(4, row.getInt(1))</li>
<li>i=5: stmt.setInt(5 + 1, row.getInt(5 - 3)), stmt.setInt(5, row.getInt(2))</li>
</ul>
<p>setter方法的第一个参数：index of setter，第二个参数：index of row。<br>比如对于i小于midField而言，get的位置等于索引减去0；i大于midField而言，get的位置等于索引减去3。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">row[1,2,3]</span><br><span class="line">setter(0)    =》 set(0+1, get(0-0))    =》 set(1, get(0))</span><br><span class="line">setter(1)    =》 set(1+1, get(1-0))    =》 set(2, get(1))</span><br><span class="line">setter(2)    =》 set(2+1, get(2-0))    =》 set(3, get(2))</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">setter(3)    =》 set(3+1, get(3-3))    =》 set(4, get(0))</span><br><span class="line">setter(4)    =》 set(4+1, get(4-3))    =》 set(5, get(1))</span><br><span class="line">setter(5)    =》 set(5+1, get(5-3))    =》 set(6, get(2))</span><br></pre></td></tr></table></figure>
<p>代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> length = rddSchema.fields.length</span><br><span class="line"><span class="keyword">val</span> numFields = <span class="keyword">if</span> (isUpdateMode) length * <span class="number">2</span> <span class="keyword">else</span> length <span class="comment">// real num Field length</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> midField = numFields / <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> (i &lt; numFields) &#123;</span><br><span class="line">  <span class="comment">//if duplicate ,'?' size = 2 * row.field.length</span></span><br><span class="line">  <span class="keyword">if</span> (isUpdateMode) &#123; <span class="comment">// 更新模式</span></span><br><span class="line">    i &lt; midField <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">// check midField &gt; i ,if midFiled &gt;i ,rowIndex is setterIndex - (setterIndex/2) + 1</span></span><br><span class="line">      <span class="keyword">case</span> <span class="literal">true</span> ⇒ <span class="comment">// insert部分</span></span><br><span class="line">        <span class="keyword">if</span> (row.isNullAt(i)) &#123;</span><br><span class="line">          stmt.setNull(i + <span class="number">1</span>, nullTypes(i))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          setters(i).apply(stmt, row, i, <span class="number">0</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      <span class="keyword">case</span> <span class="literal">false</span> ⇒ <span class="comment">// update部分</span></span><br><span class="line">        <span class="keyword">if</span> (row.isNullAt(i - midField)) &#123;</span><br><span class="line">          stmt.setNull(i + <span class="number">1</span>, nullTypes(i - midField))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          setters(i).apply(stmt, row, i, midField)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;  <span class="comment">// 直接插入</span></span><br><span class="line">    <span class="keyword">if</span> (row.isNullAt(i)) &#123;</span><br><span class="line">      stmt.setNull(i + <span class="number">1</span>, nullTypes(i))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      setters(i).apply(stmt, row, i, <span class="number">0</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  i = i + <span class="number">1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>总结下对应关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">setter[i]:  | 0 | 1 | 2 | 3 | 4 | 5 |</span><br><span class="line">position:   | 0 | 1 | 2 | 3 | 4 | 5 |</span><br><span class="line">offset:     | 0 | 0 | 0 | 3 | 3 | 3 |</span><br><span class="line">setXXX:     | 1 | 2 | 3 | 4 | 5 | 6 |   i+1</span><br><span class="line">getXXX:     | 0 | 1 | 2 | 0 | 1 | 2 |   position-offset</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark数据源扩展与实践(40行代码实现一个自定义的DataSource)&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/categories/spark/"/>
    
    
      <category term="hadoop" scheme="http://github.com/zqhxuyuan/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>StreamingPro</title>
    <link href="http://github.com/zqhxuyuan/2017/09/04/2017-09-04-StreamingPro/"/>
    <id>http://github.com/zqhxuyuan/2017/09/04/2017-09-04-StreamingPro/</id>
    <published>2017-09-03T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.259Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/allwefantasy/streamingpro/">https://github.com/allwefantasy/streamingpro/</a><br><a id="more"></a></p>
<p>单个Job的配置示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"you-first-streaming-job"</span>: &#123;</span><br><span class="line">    <span class="attr">"desc"</span>: <span class="string">"just a example"</span>,</span><br><span class="line">    <span class="attr">"strategy"</span>: <span class="string">"spark"</span>,</span><br><span class="line">    <span class="attr">"algorithm"</span>: [],</span><br><span class="line">    <span class="attr">"ref"</span>: [</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"compositor"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.sources"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"format"</span>: <span class="string">"socket"</span>,</span><br><span class="line">            <span class="attr">"outputTable"</span>: <span class="string">"test"</span>,</span><br><span class="line">            <span class="attr">"port"</span>: <span class="string">"9999"</span>,</span><br><span class="line">            <span class="attr">"host"</span>: <span class="string">"localhost"</span>,</span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"-"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.sql"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"sql"</span>: <span class="string">"select avg(value) avgAge from test"</span>,</span><br><span class="line">            <span class="attr">"outputTableName"</span>: <span class="string">"test3"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.sql"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"sql"</span>: <span class="string">"select count(value) as nameCount from test"</span>,</span><br><span class="line">            <span class="attr">"outputTableName"</span>: <span class="string">"test1"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.sql"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"sql"</span>: <span class="string">"select sum(value) ageSum from test"</span>,</span><br><span class="line">            <span class="attr">"outputTableName"</span>: <span class="string">"test2"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.sql"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"sql"</span>: <span class="string">"select * from test1 union select * from test2 union select * from test3"</span>,</span><br><span class="line">            <span class="attr">"outputTableName"</span>: <span class="string">"test4"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="attr">"name"</span>: <span class="string">"stream.outputs"</span>,</span><br><span class="line">        <span class="attr">"params"</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">"name"</span>: <span class="string">"jack"</span>,</span><br><span class="line">            <span class="attr">"format"</span>: <span class="string">"console"</span>,</span><br><span class="line">            <span class="attr">"path"</span>: <span class="string">"-"</span>,</span><br><span class="line">            <span class="attr">"inputTableName"</span>: <span class="string">"test4"</span>,</span><br><span class="line">            <span class="attr">"mode"</span>: <span class="string">"Overwrite"</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">"configParams"</span>: &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>多个Job的配置示例：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">   <span class="attr">"you-first-streaming-job"</span>: &#123;</span><br><span class="line">     <span class="attr">"desc"</span>: <span class="string">"just a example"</span>,</span><br><span class="line">     <span class="attr">"strategy"</span>: <span class="string">"spark"</span>,</span><br><span class="line">     <span class="attr">"algorithm"</span>: [],</span><br><span class="line">     <span class="attr">"ref"</span>: [</span><br><span class="line">     ],</span><br><span class="line">     <span class="attr">"compositor"</span>: [</span><br><span class="line">     ],</span><br><span class="line">     <span class="attr">"configParams"</span>: &#123;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;,</span><br><span class="line">   <span class="attr">"you-second-streaming-job"</span>: &#123;</span><br><span class="line">        <span class="attr">"desc"</span>: <span class="string">"just a example"</span>,</span><br><span class="line">        <span class="attr">"strategy"</span>: <span class="string">"spark"</span>,</span><br><span class="line">        <span class="attr">"algorithm"</span>: [],</span><br><span class="line">        <span class="attr">"ref"</span>: [</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"compositor"</span>: [</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">"configParams"</span>: &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>StreamingPro支持Spark、SparkStreaming、SparkStruncture、Flink。入口类都是统一的<code>StreamingApp</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> params = <span class="keyword">new</span> <span class="type">ParamsUtil</span>(args)</span><br><span class="line">    require(params.hasParam(<span class="string">"streaming.name"</span>), <span class="string">"Application name should be set"</span>)</span><br><span class="line">    <span class="type">PlatformManager</span>.getOrCreate.run(params)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过streaming.platform可以指定不同的运行平台。当然，不同的运行引擎的jar包也不同。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">SHome=/Users/allwefantasy/streamingpro</span><br><span class="line"></span><br><span class="line">./bin/spark-submit   --class streaming.core.StreamingApp \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">--name <span class="built_in">test</span> \</span><br><span class="line"><span class="variable">$SHome</span>/streamingpro-spark-2.0-0.4.15-SNAPSHOT.jar    \</span><br><span class="line">-streaming.name <span class="built_in">test</span>    \</span><br><span class="line">-streaming.platform spark_streaming \</span><br><span class="line">-streaming.job.file.path file://<span class="variable">$SHome</span>/spark-streaming.json</span><br><span class="line"></span><br><span class="line">bin/flink run -c streaming.core.StreamingApp \ </span><br><span class="line">/Users/allwefantasy/streamingpro/streamingpro.flink-0.4.14-SNAPSHOT-online-1.2.0.jar \</span><br><span class="line">-streaming.name god \</span><br><span class="line">-streaming.platform flink_streaming \</span><br><span class="line">-streaming.job.file.path file:///Users/allwefantasy/streamingpro/flink.json</span><br></pre></td></tr></table></figure>
<p>jar包会被用来加载不同的Runtime。Runtime运行的映射关系定义在<code>PlatformManager</code>的<code>platformNameMapping</code>变量中。<br>Runtime是一个接口，最主要的是startRuntime方法和params方法。后面我们把Runtime叫做<strong>执行引擎</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">StreamingRuntime</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span></span>: <span class="type">StreamingRuntime</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">destroyRuntime</span></span>(stopGraceful: <span class="type">Boolean</span>, stopContext: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Boolean</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">streamingRuntimeInfo</span></span>: <span class="type">StreamingRuntimeInfo</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resetRuntimeOperator</span></span>(runtimeOperator: <span class="type">RuntimeOperator</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configureStreamingRuntimeInfo</span></span>(streamingRuntimeInfo: <span class="type">StreamingRuntimeInfo</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span></span></span><br><span class="line"><span class="function">  <span class="title">def</span> <span class="title">startThriftServer</span></span></span><br><span class="line"><span class="function">  <span class="title">def</span> <span class="title">startHttpServer</span></span></span><br><span class="line"><span class="function">  <span class="title">def</span> <span class="title">params</span></span>: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamingPro本质上还是通过spark-submit运行。框架的整体运行流程在<code>PlatformManager</code>的<code>run</code>方法中。主要的步骤有：</p>
<ol>
<li>设置配置信息</li>
<li>根据反射机制，创建并获取运行时环境</li>
<li>获取dispatcher以及所有的strategies</li>
<li>启动REST服务、Thrift服务、注册ZK（可选）</li>
<li>启动执行引擎，并等待作业完成</li>
</ol>
<blockquote>
<p>关于Dispatcher、Strategy的概念，参考作者的ServiceframeworkDispatcher项目。<br>反射创建执行引擎，调用的是对应Object类的getOrCreate方法，并传入params参数，最后实例化为StreamingRuntime。</p>
</blockquote>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">platformNameMapping</span> </span>= <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  <span class="type">SPAKR_S_S</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStructuredStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPAKR_STRUCTURED_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStructuredStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">FLINK_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.FlinkStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPAKR_STREAMING</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkStreamingRuntime"</span>,</span><br><span class="line">  <span class="type">SPARK</span> -&gt; <span class="string">"streaming.core.strategy.platform.SparkRuntime"</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>注意：StreamingPro的Runtime只是Spark作业的执行引擎，具体根据配置文件加载策略是ServiceframeworkDispatcher的工作。<br>假设我们定义了下面的一个配置文件，由于采用了shortName，需要定义一个ShortNameMapping</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"convert-multi-csv-to-json"</span>: &#123;</span><br><span class="line">    <span class="string">"desc"</span>: <span class="string">"测试"</span>,</span><br><span class="line">    <span class="string">"strategy"</span>: <span class="string">"spark"</span>,</span><br><span class="line">    <span class="string">"algorithm"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"testProcessor"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"ref"</span>: [],</span><br><span class="line">    <span class="string">"compositor"</span>: [</span><br><span class="line">      &#123;</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"testCompositor"</span></span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"configParams"</span>: &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>DefaultShortNameMapping的定义如下。这样配置文件中的spark就和ServiceframeworkDispatcher的加载过程对应起来了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DefaultShortNameMapping</span> <span class="keyword">extends</span> <span class="title">ShortNameMapping</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> compositorNameMap: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">    <span class="string">"spark"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.DefaultStrategy"</span>,</span><br><span class="line">    <span class="string">"testProcessor"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.TestProcessor"</span>,</span><br><span class="line">    <span class="string">"testCompositor"</span> -&gt; <span class="string">"serviceframework.dispatcher.test.TestCompositor"</span></span><br><span class="line">  )</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">forName</span></span>(shortName: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (compositorNameMap.contains(shortName)) compositorNameMap(shortName)</span><br><span class="line">    <span class="keyword">else</span> shortName</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ServiceframeworkDispatcher的核心是StrategyDispatcher，这个类在创建的时候，会读取配置文件。<br>然后解析配置文件中的strategy、algorithm(processor)、ref、compositor、configParams等配置项，并构造对应的对象。<br>ServiceframeworkDispatcher是一个模块组合框架，它主要定义了Compositor、Processor、Strategy三个接口。</p>
<p>Strategy接口包含了processor、ref、compositor，以及初始化和result方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Strategy</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">ServiceInj</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">processor</span></span>:<span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">ref</span></span>:<span class="type">JList</span>[<span class="type">Strategy</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">compositor</span></span>:<span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>:<span class="type">String</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(name:<span class="type">String</span>,alg:<span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]],ref:<span class="type">JList</span>[<span class="type">Strategy</span>[<span class="type">T</span>]],com:<span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]],params:<span class="type">JMap</span>[<span class="type">Any</span>,<span class="type">Any</span>])</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">result</span></span>(params:<span class="type">JMap</span>[<span class="type">Any</span>,<span class="type">Any</span>]):<span class="type">JList</span>[<span class="type">T</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">configParams</span></span>:util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span> </span>= &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Strategy策略的初始化需要算法、引用、组合器，以及配置信息，对应的方法是StrategyDispatcher的createStrategy方法。</p>
<p>注意下面的initialize方法，createAlgorithms和createCompositors初始化时<br>会读取params配置，这是一个嵌套了Map的列表：<code>JList[JMap[String, Any]]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createStrategy</span></span>(name: <span class="type">String</span>, desc: <span class="type">JMap</span>[_, _]): <span class="type">Option</span>[<span class="type">Strategy</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (_strategies.contains(name)) <span class="keyword">return</span> <span class="type">None</span>;</span><br><span class="line">  <span class="comment">// 实例化策略，如果有shortName，则先获取fullName，并通过Class.forName实例化具体的策略类</span></span><br><span class="line">  <span class="keyword">val</span> strategy = <span class="type">Class</span>.forName(shortNameMapping.forName(desc.get(<span class="string">"strategy"</span>).asInstanceOf[<span class="type">String</span>])).newInstance().asInstanceOf[<span class="type">Strategy</span>[<span class="type">T</span>]]</span><br><span class="line">  <span class="comment">// 读取配置信息，并实例化为Map[Any,Any]</span></span><br><span class="line">  <span class="keyword">val</span> configParams: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>] = <span class="keyword">if</span> (desc.containsKey(<span class="string">"configParams"</span>)) desc.get(<span class="string">"configParams"</span>).asInstanceOf[<span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]] <span class="keyword">else</span> <span class="keyword">new</span> java.util.<span class="type">HashMap</span>()</span><br><span class="line">  <span class="comment">// 初始化策略，需要创建算法、引用、组合器</span></span><br><span class="line">  strategy.initialize(name, createAlgorithms(desc), createRefs(desc), createCompositors(desc), configParams)</span><br><span class="line">  _strategies.put(name, strategy)</span><br><span class="line">  <span class="type">Option</span>(strategy)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建算法。一个策略由0个或者多个算法提供结果</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createAlgorithms</span></span>(jobJMap: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">Processor</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!jobJMap.contains(<span class="string">"algorithm"</span>) &amp;&amp; !jobJMap.contains(<span class="string">"processor"</span>)) <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">Processor</span>[<span class="type">T</span>]]()</span><br><span class="line">  <span class="keyword">val</span> processors = <span class="keyword">if</span> (jobJMap.contains(<span class="string">"algorithm"</span>)) jobJMap(<span class="string">"algorithm"</span>) <span class="keyword">else</span> jobJMap(<span class="string">"processor"</span>)</span><br><span class="line">  processors.asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]].map &#123;</span><br><span class="line">    alg =&gt;</span><br><span class="line">      <span class="keyword">val</span> name = shortName2FullName(alg)</span><br><span class="line">      <span class="keyword">val</span> processor = <span class="type">Class</span>.forName(name).newInstance().asInstanceOf[<span class="type">Processor</span>[<span class="type">T</span>]]</span><br><span class="line">      <span class="keyword">val</span> params: <span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]] = <span class="keyword">if</span> (alg.contains(<span class="string">"params"</span>)) alg(<span class="string">"params"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]] <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]()</span><br><span class="line">      processor.initialize(name, params)</span><br><span class="line">      processor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建组合器，可以多个，按顺序调用。有点类似过滤器链。第一个过滤器会接受算法或者策略的结果。后续的组合器就只能处理上一阶段的组合器吐出的结果</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createCompositors</span></span>(jobJMap: <span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">Compositor</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!jobJMap.contains(<span class="string">"compositor"</span>)) <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">AList</span>()</span><br><span class="line">  <span class="keyword">val</span> compositors = jobJMap.get(<span class="string">"compositor"</span>)</span><br><span class="line">  compositors.asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]].map &#123;</span><br><span class="line">    f =&gt;</span><br><span class="line">      <span class="keyword">val</span> compositor = <span class="type">Class</span>.forName(shortName2FullName(f)).newInstance().asInstanceOf[<span class="type">Compositor</span>[<span class="type">T</span>]]</span><br><span class="line">      <span class="keyword">val</span> params: <span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]] = <span class="keyword">if</span> (f.contains(<span class="string">"params"</span>)) f.get(<span class="string">"params"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]] <span class="keyword">else</span> <span class="keyword">new</span> <span class="type">AList</span>[<span class="type">JMap</span>[<span class="type">String</span>, <span class="type">Any</span>]]()</span><br><span class="line">      compositor.initialize(f.get(<span class="string">"typeFilter"</span>).asInstanceOf[<span class="type">JList</span>[<span class="type">String</span>]], params)</span><br><span class="line">      compositor</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ServiceframeworkDispatcher的核心是StrategyDispatcher，而StrategyDispatcher的核心是其dispatch方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dispatch</span></span>(params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]): <span class="type">JList</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  findStrategies(clientType) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(strategies) =&gt;</span><br><span class="line">      strategies.flatMap &#123; f =&gt; f.result(params) &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不同执行引擎的启动方法实现不同：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkRuntime</span>(<span class="params">_params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span>) <span class="keyword">extends</span> <span class="title">StreamingRuntime</span> <span class="keyword">with</span> <span class="title">PlatformManagerListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span></span>: <span class="type">StreamingRuntime</span> = <span class="keyword">this</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> sparkSession: <span class="type">SparkSession</span> = createRuntime</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRuntime</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//...创建SparkSession，这里会根据参数判断是否支持Hive、Carbondata</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  params.put(<span class="string">"_session_"</span>, sparkSession) <span class="comment">//将SparkSession放入params中</span></span><br><span class="line">  registerUDF  </span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">params</span></span>: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>] = _params</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingRuntime</span>(<span class="params">_params: <span class="type">JMap</span>[<span class="type">Any</span>, <span class="type">Any</span>]</span>) <span class="keyword">extends</span> <span class="title">StreamingRuntime</span> <span class="keyword">with</span> <span class="title">PlatformManagerListener</span> </span>&#123; self =&gt;</span><br><span class="line">  <span class="keyword">var</span> streamingContext: <span class="type">StreamingContext</span> = createRuntime</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createRuntime</span> </span>= &#123;</span><br><span class="line">    <span class="comment">//创建StreamingContext，并将SparkSession放入params中</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">startRuntime</span> </span>= &#123;</span><br><span class="line">    streamingContext.start()</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">awaitTermination</span> </span>= streamingContext.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>但真正执行StreamingPro主流程在streamingpro-commons下的SparkStreamingStrategy类。<br>注意：如果是spark-1.6，则streamingpro-spark下也有一个SparkStreamingStrategy类。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkStreamingStrategy</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">Strategy</span>[<span class="type">T</span>] <span class="keyword">with</span> <span class="title">DebugTrait</span> <span class="keyword">with</span> <span class="title">JobStrategy</span> </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> _ref: util.<span class="type">List</span>[<span class="type">Strategy</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _compositor: util.<span class="type">List</span>[<span class="type">Compositor</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _processor: util.<span class="type">List</span>[<span class="type">Processor</span>[<span class="type">T</span>]] = _</span><br><span class="line">  <span class="keyword">var</span> _configParams: util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>] = _</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">result</span></span>(params: util.<span class="type">Map</span>[<span class="type">Any</span>, <span class="type">Any</span>]): util.<span class="type">List</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    ref.foreach &#123; r =&gt; r.result(params) &#125; <span class="comment">// 先执行ref</span></span><br><span class="line">    <span class="keyword">if</span> (compositor != <span class="literal">null</span> &amp;&amp; compositor.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 第一个Compositor, 产生第一个中间结果</span></span><br><span class="line">      <span class="keyword">var</span> middleR = compositor.get(<span class="number">0</span>).result(processor, ref, <span class="literal">null</span>, params)</span><br><span class="line">      <span class="comment">// 将新的中间结果运用到下一个Compositor</span></span><br><span class="line">      <span class="comment">// 第一个Compositor的结果运用到第二个的输入, 第二个Compositor的结果运用到第三个Compositor的输入...</span></span><br><span class="line">      <span class="comment">// 所以不同Compositor是链式执行的</span></span><br><span class="line">      <span class="keyword">for</span> (i &lt;- <span class="number">1</span> until compositor.size()) &#123;</span><br><span class="line">        middleR = compositor.get(i).result(processor, ref, middleR, params)</span><br><span class="line">      &#125;</span><br><span class="line">      middleR</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">new</span> util.<span class="type">ArrayList</span>[<span class="type">T</span>]()</span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意：配置文件中每个Job都有一个<code>strategy</code>级别的<code>configParams</code>，<code>ref</code>也会使用这个全局的<code>configParams</code>。<br>它是一个<code>Map[String, Any]</code>的结构。每个Compositor和Processor内部也有一个<code>params</code>配置，这是一个数组。</p>
<blockquote>
<p>实际上，全局的<code>configParams</code>参数会被用在Strategy、Ref/Processor和Compositor的result()方法的最后一个参数。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&quot;compositor&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;name&quot;: &quot;testCompositor&quot;,</span><br><span class="line">    &quot;params&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;sql&quot;: &quot;select avg(value) avgAge from test&quot;,</span><br><span class="line">        &quot;outputTableName&quot;: &quot;test3&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;sql&quot;: &quot;select sum(value) sumAge from test&quot;,</span><br><span class="line">        &quot;outputTableName&quot;: &quot;test4&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">],</span><br></pre></td></tr></table></figure>
<p>接下来以读取多个数据源的Compositor实现类为例：</p>
<ul>
<li><code>_configParams</code>是在创建Compositor时初始化调用的，这是一个<code>List[Map[String, Any]]</code>的结构，对应了<code>params</code>列表配置</li>
<li>如果需要替换，则会先处理配置信息</li>
<li>接着，从params中获取SparkSession（还记得之前创建Runtime时放入Map中吗？），</li>
<li>然后，执行sparkSession.read.format(xx).options(Map).load(path)</li>
<li>最后，通过df.createOrReplaceTempView创建Spark SQL的临时表，名称为<code>outputTable</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class MultiSQLSourceCompositor[T] extends Compositor[T] with CompositorHelper &#123;</span><br><span class="line">  private var _configParams: util.List[util.Map[Any, Any]] = _</span><br><span class="line"></span><br><span class="line">  override def initialize(typeFilters: util.List[String], configParams: util.List[util.Map[Any, Any]]): Unit = &#123;</span><br><span class="line">    this._configParams = configParams</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def result(alg: util.List[Processor[T]], ref: util.List[Strategy[T]], middleResult: util.List[T], params: util.Map[Any, Any]): util.List[T] = &#123;</span><br><span class="line"></span><br><span class="line">    _configParams.foreach &#123; sourceConfig =&gt;</span><br><span class="line">      val name = sourceConfig.getOrElse(&quot;name&quot;, &quot;&quot;).toString</span><br><span class="line"></span><br><span class="line">      val _cfg = sourceConfig.map(f =&gt; (f._1.toString, f._2.toString)).map &#123; f =&gt;</span><br><span class="line">        (f._1, params.getOrElse(s&quot;streaming.sql.source.$&#123;name&#125;.$&#123;f._1&#125;&quot;, f._2).toString)</span><br><span class="line">      &#125;.toMap</span><br><span class="line"></span><br><span class="line">      val sourcePath = _cfg(&quot;path&quot;)</span><br><span class="line">      val df = sparkSession(params).read.format(sourceConfig(&quot;format&quot;).toString).options(</span><br><span class="line">        (_cfg - &quot;format&quot; - &quot;path&quot; - &quot;outputTable&quot;).map(f =&gt; (f._1.toString, f._2.toString))).load(sourcePath)</span><br><span class="line">      df.createOrReplaceTempView(_cfg.getOrElse(&quot;outputTable&quot;, _cfg.getOrElse(&quot;outputTableName&quot;, &quot;&quot;)))</span><br><span class="line">    &#125;</span><br><span class="line">    List()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>为了支持配置的动态替换，<code>_cfg</code>参数会做一些处理，比如上面的<code>s&quot;streaming.sql.source.${name}.${f._1}&quot;</code>如果需要被替换，则会被替换为<code>f._2</code>。<br>下表列举了StreamingPro支持的几种替换方式。</p>
<table>
<thead>
<tr>
<th>配置参数</th>
<th>配置示例</th>
<th>动态传参数</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>streaming.sql.source.[name].[参数]</code></td>
<td>“path”: “file:///tmp/sample_article.txt”</td>
<td>-streaming.sql.source.firstSource.path  file:///tmp/wow.txt</td>
</tr>
<tr>
<td><code>streaming.sql.out.[name].[参数]</code></td>
<td>“path”: “file:///tmp/sample_article.txt”</td>
<td>-streaming.sql.source.firstSink.path  file:///tmp/wow_20170101.txt</td>
</tr>
<tr>
<td><code>streaming.sql.params.[param-name]</code></td>
<td>“sql”: “select * from test where hp_time=:today”</td>
<td>-streaming.sql.params.today “20170101”</td>
</tr>
</tbody>
</table>
<p>假设有两个数据输入源和一个输出目标的配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;batch.sources&quot;,</span><br><span class="line">  &quot;params&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;firstSource&quot;,</span><br><span class="line">      &quot;path&quot;: &quot;file:///tmp/sample_article.txt&quot;,</span><br><span class="line">      &quot;format&quot;: &quot;com.databricks.spark.csv&quot;,</span><br><span class="line">      &quot;outputTable&quot;: &quot;article&quot;,</span><br><span class="line">      &quot;header&quot;:true</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;:&quot;secondSource&quot;,</span><br><span class="line">        &quot;path&quot;: &quot;file:///tmp/sample_article2.txt&quot;,</span><br><span class="line">        &quot;format&quot;: &quot;com.databricks.spark.csv&quot;,</span><br><span class="line">        &quot;outputTable&quot;: &quot;article2&quot;,</span><br><span class="line">        &quot;header&quot;:true</span><br><span class="line">      &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;batch.outputs&quot;,</span><br><span class="line">  &quot;params&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;name&quot;:&quot;firstSink&quot;,</span><br><span class="line">      &quot;path&quot;: &quot;file:///tmp/sample_article.txt&quot;,</span><br><span class="line">      &quot;format&quot;: &quot;com.databricks.spark.csv&quot;,</span><br><span class="line">      &quot;outputTable&quot;: &quot;article&quot;,</span><br><span class="line">      &quot;header&quot;:true</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Source的功能是：读取输入源形成DataFrame，然后创建临时表。其他组件比如SQL也是类似的。至此StreamingPro的大致流程就分析完了。 </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/allwefantasy/streamingpro/&quot;&gt;https://github.com/allwefantasy/streamingpro/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>CarbonData</title>
    <link href="http://github.com/zqhxuyuan/2017/07/13/CarbonData-In-Action/"/>
    <id>http://github.com/zqhxuyuan/2017/07/13/CarbonData-In-Action/</id>
    <published>2017-07-12T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.269Z</updated>
    
    <content type="html"><![CDATA[<p>Apache CarbonData</p>
<a id="more"></a>
<h2 id="Apache_CarbonData">Apache CarbonData</h2><p>版本：carbondata-1.1.0，spark-2.1.1，hadoop-2.6.0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mvn -DskipTests -Pspark-2.1 -Dspark.version=2.1.1 -Dhadoop.version=2.6.0 clean package</span><br><span class="line"></span><br><span class="line">$ ll assembly/target/scala-2.11</span><br><span class="line">8.9M  7 12 16:14 carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar</span><br></pre></td></tr></table></figure>
<p>本地模式测试，创建CarbonSession的第一个参数为本地文件系统</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars ~/Github/carbondata-parent-1.1.0/assembly/target/scala-2.11/carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.CarbonSession._</span><br><span class="line">val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession(&quot;/tmp/carbon&quot;)</span><br><span class="line">carbon.sql(&quot;CREATE TABLE IF NOT EXISTS test_table(id string,name string,city string,age Int)STORED BY &apos;carbondata&apos;&quot;)</span><br><span class="line">carbon.sql(&quot;LOAD DATA INPATH &apos;/Users/zhengqh/Downloads/spark-2.1.1-bin-hadoop2.7/sample.csv&apos; INTO TABLE test_table&quot;)</span><br><span class="line">carbon.sql(&quot;SELECT city, avg(age), sum(age) FROM test_table GROUP BY city&quot;).show()</span><br></pre></td></tr></table></figure>
<p>本地文件系统的文件夹包括Fact（表数据）、Metadata(表结构)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">➜  carbondata-parent-1.1.0 tree /tmp/carbon</span><br><span class="line">/tmp/carbon</span><br><span class="line">├── default</span><br><span class="line">│   └── test_table</span><br><span class="line">│       ├── Fact</span><br><span class="line">│       │   └── Part0</span><br><span class="line">│       │       └── Segment_0</span><br><span class="line">│       │           ├── 0_batchno0-0-1499845043969.carbonindex</span><br><span class="line">│       │           └── part-0-0_batchno0-0-1499845043969.carbondata</span><br><span class="line">│       └── Metadata</span><br><span class="line">│           ├── 3d8bd318-a620-419b-b0fd-c276936375e2.dict</span><br><span class="line">│           ├── 3d8bd318-a620-419b-b0fd-c276936375e2.dictmeta</span><br><span class="line">│           ├── 3d8bd318-a620-419b-b0fd-c276936375e2_27.sortindex</span><br><span class="line">│           ├── f2f45986-6fb6-42af-b991-513ee43aad01.dict</span><br><span class="line">│           ├── f2f45986-6fb6-42af-b991-513ee43aad01.dictmeta</span><br><span class="line">│           ├── f2f45986-6fb6-42af-b991-513ee43aad01_18.sortindex</span><br><span class="line">│           ├── f93ce55d-b82a-4eca-9076-e21dcd819218.dict</span><br><span class="line">│           ├── f93ce55d-b82a-4eca-9076-e21dcd819218.dictmeta</span><br><span class="line">│           ├── f93ce55d-b82a-4eca-9076-e21dcd819218_30.sortindex</span><br><span class="line">│           ├── schema</span><br><span class="line">│           └── tablestatus</span><br><span class="line">└── modifiedTime.mdt</span><br></pre></td></tr></table></figure>
<p>yarn模式按照官网部署<a href="http://carbondata.apache.org/installation-guide.html" target="_blank" rel="noopener">http://carbondata.apache.org/installation-guide.html</a></p>
<blockquote>
<p>注意：使用yarn模式，不需要把carbondata通过scp分发到各个节点，只需要在Driver端有就可以。另外，当前版本不依赖kettle</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cd spark-2.1.1*</span><br><span class="line">mkdir carbonlib</span><br><span class="line">cp ~/carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar carbonlib</span><br><span class="line">cp ~/carbon.properties conf</span><br><span class="line"></span><br><span class="line">tar -zcvf carbondata.tar.gz carbonlib/</span><br><span class="line">mv carbondata.tar.gz carbonlib/</span><br><span class="line"></span><br><span class="line">$ vi conf/spark-defaults.conf</span><br><span class="line">spark.executor.extraJavaOptions -Dcarbon.properties.filepath=/usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/conf/carbon.properties</span><br><span class="line">spark.driver.extraJavaOptions   -Dcarbon.properties.filepath=/usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/conf/carbon.properties</span><br><span class="line">spark.driver.extraClassPath     /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/carbonlib/*</span><br><span class="line">spark.executor.extraClassPath   /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/carbonlib/*</span><br><span class="line">spark.yarn.dist.files           /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/conf/carbon.properties</span><br><span class="line">spark.yarn.dist.archives        /usr/install/spark-2.1.1-bin-2.6.0-cdh5.9.0/carbonlib/carbondata.tar.gz</span><br></pre></td></tr></table></figure>
<p>启动spark-shell还需要加上<code>--jars</code>。注意创建CarbonSession时第一个参数必须加上hdfs前缀，否则会报错找不到文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ bin/spark-shell --jars /home/admin/carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS test_table1(id string,name string,city string,age Int)&quot;)</span><br><span class="line">sql(&quot;insert into table test_table1 values(&apos;1&apos;,&apos;david&apos;,&apos;shenzhen&apos;,31)&quot;)</span><br><span class="line">sql(&quot;insert into table test_table1 values(&apos;2&apos;,&apos;eason&apos;,&apos;shenzhen&apos;,20)&quot;)</span><br><span class="line">sql(&quot;insert into table test_table1 values(&apos;3&apos;,&apos;jarry&apos;,&apos;wuhan&apos;,35)&quot;)</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.CarbonSession._</span><br><span class="line">val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession(&quot;hdfs://tdhdfs/user/tongdun/carbon&quot;,&quot;/home/admin/carbon&quot;)</span><br><span class="line"></span><br><span class="line">carbon.sql(&quot;CREATE TABLE IF NOT EXISTS test_table2(id string,name string,city string,age Int)STORED BY &apos;carbondata&apos;&quot;)</span><br><span class="line">carbon.sql(&quot;INSERT INTO test_table2 SELECT * FROM test_table1&quot;) // insert #1</span><br><span class="line">carbon.sql(&quot;select * from test_table2&quot;).show</span><br><span class="line">carbon.sql(&quot;INSERT INTO test_table2 SELECT * FROM test_table1&quot;) // insert again</span><br><span class="line">carbon.sql(&quot;select * from test_table2&quot;).show</span><br><span class="line"></span><br><span class="line">carbon.sql(&quot;INSERT overwrite table test_table2 SELECT * FROM test_table1&quot;) // overwrite</span><br></pre></td></tr></table></figure>
<p>carbondata运行在HDFS时，它的事实数据与元数据保存在HDFS上。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170713222930582" alt="carbon"></p>
<p>将hdfs表数据导入到carbondata建立的表后，执行一些查询语句，观察ui。</p>
<blockquote>
<p>注意：导入数据时，carbondata分为两个步骤：全局字典（GlobalDictionary）和CarbonDataRDD。<br>其中全局字典会在Metadata下生产索引文件，CarbonDataRDD会在Fact下生成数据文件。</p>
</blockquote>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170713222957886" alt="carbon1"></p>
<h3 id="CarbonData数据导入与查询">CarbonData数据导入与查询</h3><p>建立crosspartner carbondata表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line">import org.apache.spark.sql.CarbonSession._</span><br><span class="line">val carbon = SparkSession.builder().config(sc.getConf).getOrCreateCarbonSession(&quot;hdfs://tdhdfs/user/tongdun/carbon&quot;,&quot;/home/admin/carbon&quot;)</span><br><span class="line"></span><br><span class="line">carbon.sql(&quot;drop table cross_partner_carbon&quot;)</span><br><span class="line">carbon.sql(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS cross_partner_carbon(</span><br><span class="line">  partnerCode string,</span><br><span class="line">  eventType string,</span><br><span class="line">  idNumber string,</span><br><span class="line">  accountMobile string,</span><br><span class="line">  accountEmail string,</span><br><span class="line">  accountPhone string,</span><br><span class="line">  deviceId string,</span><br><span class="line">  cardNumber string,</span><br><span class="line">  contact1Mobile string,</span><br><span class="line">  contact2Mobile string,</span><br><span class="line">  contact3Mobile string,</span><br><span class="line">  contact4Mobile string,</span><br><span class="line">  contact5Mobile string,</span><br><span class="line">  contact1IdNumber string,</span><br><span class="line">  contact2IdNumber string,</span><br><span class="line">  contact3IdNumber string,</span><br><span class="line">  contact4IdNumber string,</span><br><span class="line">  contact5IdNumber string,</span><br><span class="line">  sequenceId string</span><br><span class="line">)</span><br><span class="line">STORED BY &apos;carbondata&apos;</span><br><span class="line">TBLPROPERTIES (&apos;DICTIONARY_EXCLUDE&apos;=&apos;sequenceId&apos;)</span><br><span class="line">&quot;&quot;&quot;)</span><br></pre></td></tr></table></figure>
<p>再生成carbondata表：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql(&quot;insert into cross_partner_carbon select * from crosspartner&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select count(*) from cross2partner_dt&quot;).show</span><br><span class="line">carbon.sql(&quot;select count(*) from cross_partner_carbon_dm&quot;).show</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from cross2partner_dt&quot;).show</span><br><span class="line">carbon.sql(&quot;select * from cross_partner_carbon_dm&quot;).show</span><br><span class="line"></span><br><span class="line">val idnumber=&quot;&quot;</span><br><span class="line">spark.sql(s&quot;select sequenceId from cross2partner_dt where partnerCode=&apos;007fenqi&apos; and eventType=&apos;Loan&apos; and idNumber=&apos;$idnumber&apos;&quot;).show</span><br><span class="line">carbon.sql(s&quot;select sequenceId from cross_partner_carbon_dm where partnerCode=&apos;007fenqi&apos; and eventType=&apos;Loan&apos; and idNumber=&apos;$idnumber&apos;&quot;).show</span><br></pre></td></tr></table></figure>
<p>比较crosspartner_hdfs的过滤与carbondata的查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql(&quot;select sequenceId from cross_partner_carbon where partnerCode=&apos;qufenqi&apos; and eventType=&apos;Loan&apos; and idNumber=&apos;&apos;&quot;).show</span><br></pre></td></tr></table></figure>
<h3 id="实验结果">实验结果</h3><p>创建carbondata表时，如果默认所有字段都加上索引，导入数据时Executor会报错OOM。<br>如果去掉所有字段的索引，导入数据很快，但是查询速度就满了。</p>
<p>比较磁盘空间的大小，没有索引下，Parquet和Carbondata差不多</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170721100001741" alt="1"></p>
<h3 id="问题">问题</h3><h4 id="1-_Hive表与CarbonData表">1. Hive表与CarbonData表</h4><p>activity事件数据,只取借贷和放贷的数据，并保存成临时表crosspartner_hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;&quot;CREATE TABLE crosspartner_hdfs(</span><br><span class="line">  partnerCode string,</span><br><span class="line">  eventType string,</span><br><span class="line">  idNumber string,</span><br><span class="line">  accountMobile string,</span><br><span class="line">  accountEmail string,</span><br><span class="line">  accountPhone string,</span><br><span class="line">  deviceId string,</span><br><span class="line">  cardNumber string,</span><br><span class="line">  contact1Mobile string,</span><br><span class="line">  contact2Mobile string,</span><br><span class="line">  contact3Mobile string,</span><br><span class="line">  contact4Mobile string,</span><br><span class="line">  contact5Mobile string,</span><br><span class="line">  contact1IdNumber string,</span><br><span class="line">  contact2IdNumber string,</span><br><span class="line">  contact3IdNumber string,</span><br><span class="line">  contact4IdNumber string,</span><br><span class="line">  contact5IdNumber string,</span><br><span class="line">  sequenceId string</span><br><span class="line">) partitioned by(ds string)</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;&quot;&quot;insert into table crosspartner_hdfs partition(ds=&apos;201706&apos;)</span><br><span class="line">select </span><br><span class="line">  activity_map.partnerCode as partnerCode,</span><br><span class="line">  activity_map.eventType as eventType,</span><br><span class="line">  activity_map.idNumber as idNumber,</span><br><span class="line">  activity_map.accountMobile as accountMobile,</span><br><span class="line">  activity_map.accountEmail as accountEmail,</span><br><span class="line">  activity_map.accountPhone as accountPhone,</span><br><span class="line">  activity_map.deviceId as deviceId,</span><br><span class="line">  activity_map.cardNumber as cardNumber,</span><br><span class="line">  activity_map.contact1Mobile as contact1Mobile,</span><br><span class="line">  activity_map.contact2Mobile as contact2Mobile,</span><br><span class="line">  activity_map.contact3Mobile as contact3Mobile,</span><br><span class="line">  activity_map.contact4Mobile as contact4Mobile,</span><br><span class="line">  activity_map.contact5Mobile as contact5Mobile,</span><br><span class="line">  activity_map.contact1IdNumber as contact1IdNumber,</span><br><span class="line">  activity_map.contact2IdNumber as contact2IdNumber,</span><br><span class="line">  activity_map.contact3IdNumber as contact3IdNumber,</span><br><span class="line">  activity_map.contact4IdNumber as contact4IdNumber,</span><br><span class="line">  activity_map.contact5IdNumber as contact5IdNumber,</span><br><span class="line">  activity_map.sequenceId as sequenceId</span><br><span class="line">from activity </span><br><span class="line">where year=2017 and month=6</span><br><span class="line">and activity_map.eventType in(&apos;Loan&apos;,&apos;Lending&apos;)</span><br><span class="line">&quot;&quot;&quot;)</span><br></pre></td></tr></table></figure>
<p>上面如果建表时没有指定存储为parquet,最后是part-xxx。<br>而且即使指定了parquet,insert sql也不能指定分区数量。  </p>
<p><strong>下面改用parquet文件夹加上手动分区的形式:cross_partner_hdfs</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">year</span></span>(ymd: <span class="type">String</span>) = ymd.substring(<span class="number">0</span>,<span class="number">4</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">month</span></span>(ymd: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> month=ymd.substring(<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">  <span class="keyword">if</span>(month.startsWith(<span class="string">"0"</span>)) month=ymd.substring(<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">  month</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">day</span></span>(ymd: <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="keyword">var</span> d=ymd.substring(<span class="number">6</span>,<span class="number">8</span>)</span><br><span class="line">  <span class="keyword">if</span>(d.startsWith(<span class="string">"0"</span>)) d=ymd.substring(<span class="number">7</span>,<span class="number">8</span>)</span><br><span class="line">  d</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//写成parquet文件夹</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossData</span></span>(beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMdd"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      println(ymd)</span><br><span class="line">      <span class="keyword">var</span> y=year(ymd)</span><br><span class="line">      <span class="keyword">var</span> m=month(ymd)</span><br><span class="line">      <span class="keyword">var</span> d=day(ymd)</span><br><span class="line">      spark.sql(<span class="string">s""</span><span class="string">"</span></span><br><span class="line"><span class="string">        select </span></span><br><span class="line"><span class="string">          activity_map.partnerCode as partnerCode,</span></span><br><span class="line"><span class="string">          activity_map.eventType as eventType,</span></span><br><span class="line"><span class="string">          activity_map.idNumber as idNumber,</span></span><br><span class="line"><span class="string">          activity_map.accountMobile as accountMobile,</span></span><br><span class="line"><span class="string">          activity_map.accountEmail as accountEmail,</span></span><br><span class="line"><span class="string">          activity_map.accountPhone as accountPhone,</span></span><br><span class="line"><span class="string">          activity_map.deviceId as deviceId,</span></span><br><span class="line"><span class="string">          activity_map.cardNumber as cardNumber,</span></span><br><span class="line"><span class="string">          activity_map.contact1Mobile as contact1Mobile,</span></span><br><span class="line"><span class="string">          activity_map.contact2Mobile as contact2Mobile,</span></span><br><span class="line"><span class="string">          activity_map.contact3Mobile as contact3Mobile,</span></span><br><span class="line"><span class="string">          activity_map.contact4Mobile as contact4Mobile,</span></span><br><span class="line"><span class="string">          activity_map.contact5Mobile as contact5Mobile,</span></span><br><span class="line"><span class="string">          activity_map.contact1IdNumber as contact1IdNumber,</span></span><br><span class="line"><span class="string">          activity_map.contact2IdNumber as contact2IdNumber,</span></span><br><span class="line"><span class="string">          activity_map.contact3IdNumber as contact3IdNumber,</span></span><br><span class="line"><span class="string">          activity_map.contact4IdNumber as contact4IdNumber,</span></span><br><span class="line"><span class="string">          activity_map.contact5IdNumber as contact5IdNumber,</span></span><br><span class="line"><span class="string">          activity_map.sequenceId as sequenceId</span></span><br><span class="line"><span class="string">        from activity </span></span><br><span class="line"><span class="string">        where year=$y and month=$m and day=$d </span></span><br><span class="line"><span class="string">        and activity_map.eventType in('Loan','Lending')</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span>).repartition(<span class="number">1</span>).write.mode(<span class="string">"overwrite"</span>).parquet(<span class="string">s"/user/hive/warehouse/cross_partner_hdfs/ds=<span class="subst">$ymd</span>"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData(<span class="string">"20170101"</span>,<span class="string">"20170630"</span>)</span><br><span class="line"></span><br><span class="line">genCrossData(<span class="string">"20170621"</span>,<span class="string">"20170630"</span>)</span><br></pre></td></tr></table></figure>
<p>查询parquet，建立临时表，使用SparkSQL查询</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.parquet(&quot;/user/hive/warehouse/cross_partner_hdfs/*&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;cross_partner_hdfs&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select * from cross_partner_hdfs&quot;).show</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select sequenceId from cross_partner_hdfs where partnerCode=&apos;qufenqi&apos; and eventType=&apos;Loan&apos; and idNumber=&apos;&apos;&quot;).show</span><br></pre></td></tr></table></figure>
<p>使用临时表的数据插入到carbondata table</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val df=spark.read.parquet(&quot;/user/hive/warehouse/cross_partner_hdfs/*&quot;)</span><br><span class="line">df.createOrReplaceTempView(&quot;cross_partner_hdfs&quot;)</span><br><span class="line"></span><br><span class="line">carbon.sql(&quot;insert into cross_partner_carbon select * from cross_partner_hdfs&quot;)</span><br></pre></td></tr></table></figure>
<p>carbondata不认识用df注册的临时表：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170714085833872" alt="10"></p>
<p>创建hive表时指定parquet格式，并从parquet文件夹的数据直接生成表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;&quot;CREATE TABLE crosspartner(</span><br><span class="line">  partnerCode string,</span><br><span class="line">  eventType string,</span><br><span class="line">  idNumber string,</span><br><span class="line">  accountMobile string,</span><br><span class="line">  accountEmail string,</span><br><span class="line">  accountPhone string,</span><br><span class="line">  deviceId string,</span><br><span class="line">  cardNumber string,</span><br><span class="line">  contact1Mobile string,</span><br><span class="line">  contact2Mobile string,</span><br><span class="line">  contact3Mobile string,</span><br><span class="line">  contact4Mobile string,</span><br><span class="line">  contact5Mobile string,</span><br><span class="line">  contact1IdNumber string,</span><br><span class="line">  contact2IdNumber string,</span><br><span class="line">  contact3IdNumber string,</span><br><span class="line">  contact4IdNumber string,</span><br><span class="line">  contact5IdNumber string,</span><br><span class="line">  sequenceId string</span><br><span class="line">) partitioned by(ds string) stored as parquet</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">import java.text.SimpleDateFormat</span><br><span class="line">import java.util.&#123;Calendar,Date&#125;</span><br><span class="line">def genCrossData(beg: String, end: String) = &#123;</span><br><span class="line">    var cal = Calendar.getInstance()</span><br><span class="line">    var datef=new SimpleDateFormat(&quot;yyyyMMdd&quot;)</span><br><span class="line">    var beginTime=datef.parse(beg)</span><br><span class="line">    var endTime=datef.parse(end)</span><br><span class="line">    while(beginTime.compareTo(endTime)&lt;=0)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      var ymd=datef.format(beginTime)</span><br><span class="line">      var df = spark.read.parquet(s&quot;/user/hive/warehouse/cross_partner_hdfs/ds=$ymd&quot;)</span><br><span class="line">      df.repartition(1).write.mode(&quot;overwrite&quot;).parquet(s&quot;/user/hive/warehouse/crosspartner/ds=$ymd&quot;)</span><br><span class="line">      spark.sql(s&quot;alter table crosspartner add partition(ds=&apos;$ymd&apos;)&quot;)</span><br><span class="line">      cal.add(Calendar.DATE,1);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData(&quot;20170101&quot;,&quot;20170630&quot;)</span><br></pre></td></tr></table></figure>
<p>或者直接用parquet文件创建外部表：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">create external table cross2partner_dt(</span></span><br><span class="line"><span class="string">  partnerCode string,</span></span><br><span class="line"><span class="string">  eventType string,</span></span><br><span class="line"><span class="string">  idNumber string,</span></span><br><span class="line"><span class="string">  accountMobile string,</span></span><br><span class="line"><span class="string">  accountEmail string,</span></span><br><span class="line"><span class="string">  accountPhone string,</span></span><br><span class="line"><span class="string">  deviceId string,</span></span><br><span class="line"><span class="string">  cardNumber string,</span></span><br><span class="line"><span class="string">  contact1Mobile string,</span></span><br><span class="line"><span class="string">  contact2Mobile string,</span></span><br><span class="line"><span class="string">  contact3Mobile string,</span></span><br><span class="line"><span class="string">  contact4Mobile string,</span></span><br><span class="line"><span class="string">  contact5Mobile string,</span></span><br><span class="line"><span class="string">  contact1IdNumber string,</span></span><br><span class="line"><span class="string">  contact2IdNumber string,</span></span><br><span class="line"><span class="string">  contact3IdNumber string,</span></span><br><span class="line"><span class="string">  contact4IdNumber string,</span></span><br><span class="line"><span class="string">  contact5IdNumber string,</span></span><br><span class="line"><span class="string">  sequenceId string    </span></span><br><span class="line"><span class="string">) </span></span><br><span class="line"><span class="string">partitioned by (ds string)</span></span><br><span class="line"><span class="string">stored as parquet</span></span><br><span class="line"><span class="string">location '/user/hive/warehouse/cross_partner_hdfs'</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>)</span><br><span class="line">spark.sql(<span class="string">s"alter table cross2partner_dt add partition(ds='20170101')"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossData</span></span>(beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMMdd"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      spark.sql(<span class="string">s"alter table cross2partner_dt add partition(ds='<span class="subst">$ymd</span>')"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossData(<span class="string">"20170102"</span>,<span class="string">"20170630"</span>)</span><br></pre></td></tr></table></figure>
<p>一次性将所有数据插入carbondata太慢了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql(s&quot;insert into cross_partner_carbon select * from crosspartner where ds like &apos;$ymd%&apos;&quot;)</span><br></pre></td></tr></table></figure>
<p><strong>改用按月/天插入carbondata表</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.&#123;<span class="type">Calendar</span>,<span class="type">Date</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">genCrossCarbonData</span></span>(beg: <span class="type">String</span>, end: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="keyword">var</span> cal = <span class="type">Calendar</span>.getInstance()</span><br><span class="line">    <span class="keyword">var</span> datef=<span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">"yyyyMM"</span>)</span><br><span class="line">    <span class="keyword">var</span> beginTime=datef.parse(beg)</span><br><span class="line">    <span class="keyword">var</span> endTime=datef.parse(end)</span><br><span class="line">    <span class="keyword">while</span>(beginTime.compareTo(endTime)&lt;=<span class="number">0</span>)&#123;</span><br><span class="line">      cal.setTime(beginTime);</span><br><span class="line">      <span class="keyword">var</span> ymd=datef.format(beginTime)</span><br><span class="line">      println(ymd) </span><br><span class="line">      carbon.sql(<span class="string">s"insert into cross_partner_carbon select * from cross2partner_dt where ds like '<span class="subst">$ymd</span>%'"</span>)</span><br><span class="line">      cal.add(<span class="type">Calendar</span>.<span class="type">DATE</span>,<span class="number">1</span>);</span><br><span class="line">      beginTime=cal.getTime();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">genCrossCarbonData(<span class="string">"201701"</span>,<span class="string">"201706"</span>)</span><br></pre></td></tr></table></figure>
<p>导入数据时还是会报错：</p>
<p>增加内存：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark/shell \</span><br><span class="line">--conf spark.executor.instances=15 \</span><br><span class="line">--conf spark.executor.cores=2 \</span><br><span class="line">--conf spark.executor.memory=8g \</span><br><span class="line">--conf spark.driver.memory=8g \</span><br></pre></td></tr></table></figure>
<h4 id="2-_carbondata其他设置">2. carbondata其他设置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">carbon.sql(&quot;&quot;&quot;CREATE TABLE IF NOT EXISTS crosspartner1(</span><br><span class="line">...</span><br><span class="line">STORED BY &apos;carbondata&apos;</span><br><span class="line">TBLPROPERTIES (&apos;DICTIONARY_EXCLUDE&apos;=&apos;sequenceId,idNumber,accountMobile,accountEmail,accountPhone,deviceId,cardNumber,contact1Mobile,contact2Mobile,contact3Mobile,contact4Mobile,contact5Mobile,contact1IdNumber,contact2IdNumber,contact3IdNumber,contact4IdNumber,contact5IdNumber&apos;)</span><br><span class="line">&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">carbon.sql(&quot;insert into crosspartner1 select * from cross_partner_hdfs&quot;)</span><br></pre></td></tr></table></figure>
<h4 id="3-_carbon_thrift_server">3. carbon thrift server</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--conf spark.sql.hive.thriftServer.singleSession=true \</span><br><span class="line">--hiveconf hive.server2.thrift.port=10002 \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=&quot;192.168.39.25&quot; \</span><br><span class="line">--class org.apache.carbondata.spark.thriftserver.CarbonThriftServer \</span><br><span class="line">carbonlib/carbondata_2.11-1.1.1-shade-hadoop2.6.0.jar \</span><br><span class="line">hdfs://tdhdfs/user/tongdun/carbon</span><br><span class="line">hdfs://tdhdfs/user/hive/warehouse/carbon.store</span><br><span class="line">hdfs://tdhdfs/user/tongdun/carbondata/CarbonStore</span><br></pre></td></tr></table></figure>
<h4 id="4-_spark-2-2-0">4. spark-2.2.0</h4><p>carbondata-1.1.1目前不支持spark2.2。如果加上profile，更改spark版本为2.2.0，编译不通过</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$ mvn -DskipTests -Pspark-2.2 -Dspark.version=2.2.0 -Dhadoop.version=2.6.0 clean package</span><br><span class="line"></span><br><span class="line">[WARNING] /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/carbondata/spark/rdd/UpdateCoalescedRDD.scala:23: warning: imported `RDD&apos; is permanently hidden by definition of class RDD in package rdd</span><br><span class="line">[INFO] import org.apache.spark.rdd.&#123;CoalescedRDDPartition, DataLoadPartitionCoalescer, RDD&#125;</span><br><span class="line">[INFO]                                                                                 ^</span><br><span class="line">[WARNING] /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/carbondata/spark/util/CarbonScalaUtil.scala:125: warning: non-variable type argument Any in type pattern scala.collection.Map[Any,Any] is unchecked since it is eliminated by erasure</span><br><span class="line">[INFO]         case m: scala.collection.Map[Any, Any] =&gt;</span><br><span class="line">[INFO]                                  ^</span><br><span class="line">[ERROR] /Users/zhengqh/Github/carbondata-parent-1.1.1/integration/spark-common/src/main/scala/org/apache/spark/sql/optimizer/CarbonDecoderOptimizerHelper.scala:87: error: value child is not a member of org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable</span><br><span class="line">[INFO]       case i: InsertIntoTable =&gt; process(i.child, nodeList)</span><br><span class="line">[INFO]                                            ^</span><br><span class="line">[WARNING] 11 warnings found</span><br><span class="line">[ERROR] one error found</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Reactor Summary:</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] Apache CarbonData :: Parent ........................ SUCCESS [  5.140 s]</span><br><span class="line">[INFO] Apache CarbonData :: Common ........................ SUCCESS [ 10.114 s]</span><br><span class="line">[INFO] Apache CarbonData :: Core .......................... SUCCESS [ 29.232 s]</span><br><span class="line">[INFO] Apache CarbonData :: Processing .................... SUCCESS [  9.828 s]</span><br><span class="line">[INFO] Apache CarbonData :: Hadoop ........................ SUCCESS [  5.719 s]</span><br><span class="line">[INFO] Apache CarbonData :: Spark Common .................. FAILURE [01:10 min]</span><br><span class="line">[INFO] Apache CarbonData :: Spark Common Test ............. SKIPPED</span><br><span class="line">[INFO] Apache CarbonData :: Assembly ...................... SKIPPED</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 ........................ SKIPPED</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 Examples ............... SKIPPED</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 02:10 min</span><br><span class="line">[INFO] Finished at: 2017-08-03T14:39:55+08:00</span><br><span class="line">[INFO] Final Memory: 72M/786M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[ERROR] Failed to execute goal org.scala-tools:maven-scala-plugin:2.15.2:compile (default) on project carbondata-spark-common: wrap: org.apache.commons.exec.ExecuteException: Process exited with an error: 1(Exit value: 1) -&gt; [Help 1]</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] After correcting the problems, you can resume the build with the command</span><br><span class="line">[ERROR]   mvn &lt;goals&gt; -rf :carbondata-spark-common</span><br></pre></td></tr></table></figure>
<p>如果使用spark2.1.1编译的二进制包，放到spark2.2.0下，也会报错：</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170803143703134" alt="car"></p>
<p>spark-1.6.2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">case class InsertIntoTable(</span><br><span class="line">    table: LogicalPlan,</span><br><span class="line">    partition: Map[String, Option[String]],</span><br><span class="line">    child: LogicalPlan,</span><br><span class="line">    overwrite: Boolean,</span><br><span class="line">    ifNotExists: Boolean)</span><br><span class="line">  extends LogicalPlan &#123;</span><br><span class="line"></span><br><span class="line">  override def children: Seq[LogicalPlan] = child :: Nil</span><br><span class="line">  override def output: Seq[Attribute] = Seq.empty</span><br><span class="line"></span><br><span class="line">  assert(overwrite || !ifNotExists)</span><br><span class="line">  override lazy val resolved: Boolean = childrenResolved &amp;&amp; child.output.zip(table.output).forall &#123;</span><br><span class="line">    case (childAttr, tableAttr) =&gt;</span><br><span class="line">      DataType.equalsIgnoreCompatibleNullability(childAttr.dataType, tableAttr.dataType)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>spark-2.2.0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">case class InsertIntoTable(</span><br><span class="line">    table: LogicalPlan,</span><br><span class="line">    partition: Map[String, Option[String]],</span><br><span class="line">    query: LogicalPlan,</span><br><span class="line">    overwrite: Boolean,</span><br><span class="line">    ifPartitionNotExists: Boolean)</span><br><span class="line">  extends LogicalPlan &#123;</span><br><span class="line">  // We don&apos;t want `table` in children as sometimes we don&apos;t want to transform it.</span><br><span class="line">  override def children: Seq[LogicalPlan] = query :: Nil</span><br><span class="line">  override def output: Seq[Attribute] = Seq.empty</span><br><span class="line">  override lazy val resolved: Boolean = false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更改为i.query后，重新编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[INFO] Apache CarbonData :: Assembly ...................... FAILURE [  2.180 s]</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 ........................ SKIPPED</span><br><span class="line">[INFO] Apache CarbonData :: Spark2 Examples ............... SKIPPED</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD FAILURE</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 01:57 min</span><br><span class="line">[INFO] Finished at: 2017-08-03T15:33:59+08:00</span><br><span class="line">[INFO] Final Memory: 83M/728M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[ERROR] Failed to execute goal on project carbondata-assembly: Could not resolve dependencies for project org.apache.carbondata:carbondata-assembly:pom:1.1.1: Could not find artifact org.apache.carbondata:carbondata-spark:jar:1.1.1 in central (http://repo1.maven.org/maven2) -&gt; [Help 1]</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.</span><br><span class="line">[ERROR] Re-run Maven using the -X switch to enable full debug logging.</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] For more information about the errors and possible solutions, please read the following articles:</span><br><span class="line">[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException</span><br><span class="line">[ERROR]</span><br><span class="line">[ERROR] After correcting the problems, you can resume the build with the command</span><br><span class="line">[ERROR]   mvn &lt;goals&gt; -rf :carbondata-assembly</span><br></pre></td></tr></table></figure>
<p>默认1.6版本的assembly无法下载1.1.1的pom,将默认版本改为(添加)2.2.0</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;profile&gt;</span><br><span class="line">      &lt;id&gt;spark-2.2&lt;/id&gt;</span><br><span class="line">        &lt;activation&gt;</span><br><span class="line">            &lt;activeByDefault&gt;true&lt;/activeByDefault&gt;</span><br><span class="line">        &lt;/activation&gt;</span><br><span class="line">      &lt;dependencies&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">          &lt;groupId&gt;org.apache.carbondata&lt;/groupId&gt;</span><br><span class="line">          &lt;artifactId&gt;carbondata-spark2&lt;/artifactId&gt;</span><br><span class="line">          &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt;</span><br><span class="line">        &lt;/dependency&gt;</span><br><span class="line">      &lt;/dependencies&gt;</span><br><span class="line">    &lt;/profile&gt;</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache CarbonData&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>SnappyData</title>
    <link href="http://github.com/zqhxuyuan/2017/07/13/SnappyData-In-Action/"/>
    <id>http://github.com/zqhxuyuan/2017/07/13/SnappyData-In-Action/</id>
    <published>2017-07-12T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.383Z</updated>
    
    <content type="html"><![CDATA[<p>SnappyData®</p>
<a id="more"></a>
<h2 id="SnappyData">SnappyData</h2><h3 id="开发模式">开发模式</h3><p>由于下载的snappydata已经带了spark，所以不需要使用–packges</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cd snappydata-0.9-bin</span><br><span class="line">$ bin/spark-shell --driver-memory=4g \</span><br><span class="line">  --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir \</span><br><span class="line">  --conf spark.snappydata.store.log-file=quickstartdatadir/quickstart.log \</span><br><span class="line">  --driver-java-options=&quot;-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g&quot;</span><br><span class="line">Spark context Web UI available at http://192.168.6.52:4042</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
<p>执行CRUD操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)</span><br><span class="line">import snappy.implicits._</span><br><span class="line">import org.apache.spark.sql.types._</span><br><span class="line">import org.apache.spark.sql.Row</span><br><span class="line"></span><br><span class="line">val ds = Seq((1,&quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)).toDS()</span><br><span class="line">val tableSchema = StructType(Array(StructField(&quot;CustKey&quot;, IntegerType, false),StructField(&quot;CustName&quot;, StringType, false)))</span><br><span class="line"></span><br><span class="line">snappy.createTable(tableName = &quot;colTable&quot;, provider = &quot;column&quot;, schema = tableSchema, options = Map.empty[String, String], allowExisting = false)</span><br><span class="line">snappy.createTable(tableName = &quot;rowTable&quot;, provider = &quot;row&quot;, schema = tableSchema, options = Map.empty[String, String], allowExisting = false)</span><br><span class="line"></span><br><span class="line">ds.write.insertInto(&quot;colTable&quot;)</span><br><span class="line">ds.write.insertInto(&quot;rowTable&quot;)</span><br><span class="line"></span><br><span class="line">snappy.insert(&quot;colTable&quot;, Row(10, &quot;f&quot;))</span><br><span class="line">snappy.insert(&quot;rowTable&quot;, Row(4, &quot;d&quot;))</span><br><span class="line"></span><br><span class="line">snappy.table(&quot;colTable&quot;).count</span><br><span class="line">snappy.table(&quot;colTable&quot;).orderBy(&quot;CustKey&quot;).show</span><br><span class="line">snappy.table(&quot;rowTable&quot;).count</span><br><span class="line">snappy.table(&quot;rowTable&quot;).orderBy(&quot;CUSTKEY&quot;).show</span><br><span class="line"></span><br><span class="line">// update and delete on row table. current version did&apos;t support update and delete on column table.</span><br><span class="line"></span><br><span class="line">// update rowTable set custname=&apos;d&apos; where custkey=1</span><br><span class="line">snappy.update(tableName = &quot;rowTable&quot;, filterExpr = &quot;CUSTKEY=1&quot;, newColumnValues = Row(&quot;d&quot;), updateColumns = &quot;CUSTNAME&quot;)</span><br><span class="line">snappy.table(&quot;rowTable&quot;).orderBy(&quot;CUSTKEY&quot;).show</span><br><span class="line">// delete rowTable where custkey=1</span><br><span class="line">snappy.delete(tableName = &quot;rowTable&quot;, filterExpr = &quot;CUSTKEY=1&quot;)</span><br></pre></td></tr></table></figure>
<p>打开<a href="http://192.168.6.52:4042/dashboard/" target="_blank" rel="noopener">http://192.168.6.52:4042/dashboard/</a>，查看web-ui的dashboard页面</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170714115456658" alt="snappy"></p>
<p>查看quickstartdir,索引采用GF(GemFire)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ tree quickstartdatadir/</span><br><span class="line">quickstartdatadir/</span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE_1.crf</span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE_1.drf</span><br><span class="line">├── BACKUPGFXD-DEFAULT-DISKSTORE.if</span><br><span class="line">├── datadictionary</span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE_1.crf</span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE_1.drf</span><br><span class="line">│   ├── BACKUPGFXD-DD-DISKSTORE.if</span><br><span class="line">│   └── DRLK_IFGFXD-DD-DISKSTORE.lk</span><br><span class="line">├── DRLK_IFGFXD-DEFAULT-DISKSTORE.lk</span><br><span class="line">├── gemfirexdtemp_1015622261.d</span><br><span class="line">└── quickstart.log</span><br></pre></td></tr></table></figure>
<p>简单的性能测试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def benchmark(name: String, times: Int = 10, warmups: Int = 6)(f: =&gt; Unit) &#123;</span><br><span class="line">  for (i &lt;- 1 to warmups) &#123;</span><br><span class="line">    f</span><br><span class="line">  &#125;</span><br><span class="line">  val startTime = System.nanoTime</span><br><span class="line">  for (i &lt;- 1 to times) &#123;</span><br><span class="line">    f</span><br><span class="line">  &#125;</span><br><span class="line">  val endTime = System.nanoTime</span><br><span class="line">  println(s&quot;Average time taken in $name for $times runs: &quot; +</span><br><span class="line">    (endTime - startTime).toDouble / (times * 1000000.0) + &quot; millis&quot;)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">val snappy = new org.apache.spark.sql.SnappySession(spark.sparkContext)</span><br><span class="line">val testDF = snappy.range(100000000).selectExpr(&quot;id&quot;, &quot;concat(&apos;sym&apos;, cast((id % 100) as varchar(10))) as sym&quot;)</span><br><span class="line">snappy.sql(&quot;drop table if exists snappyTable&quot;)</span><br><span class="line">snappy.sql(&quot;create table snappyTable (id bigint not null, sym varchar(10) not null) using column&quot;)</span><br><span class="line">benchmark(&quot;Snappy insert perf&quot;, 1, 0) &#123;testDF.write.insertInto(&quot;snappyTable&quot;) &#125;</span><br><span class="line">benchmark(&quot;Snappy perf&quot;) &#123;snappy.sql(&quot;select sym, avg(id) from snappyTable group by sym&quot;).collect()&#125;</span><br></pre></td></tr></table></figure>
<h3 id="单机模式">单机模式</h3><p>左图为本地模式，右图为伪分布式模式：分别启动locator（左下）、server（DataServer，右上）、<br>leader（左上），quickstartdir为右下（share-nothing store）.</p>
<p><img src="http://snappydatainc.github.io/snappydata/Images/SnappyLocalMode.png" alt="snappy"><br><img src="http://snappydatainc.github.io/snappydata/Images/SnappyEmbeddedMode.png" alt="snappy"></p>
<p>伪分布式模式的三个组件都在本机启动，使用不同的文件夹。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ cd snappydata-0.9-bin</span><br><span class="line">$ mkdir -p node-a/locator1 node-b/server1 node-c/lead1</span><br><span class="line"></span><br><span class="line">$ bin/snappy locator start -dir=node-a/locator1</span><br><span class="line"></span><br><span class="line">Starting SnappyData Locator using peer discovery on: 0.0.0.0[10334]</span><br><span class="line">Starting Thrift server for SnappyData at address localhost/127.0.0.1[1527]</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/node-a/locator1/snappylocator.log</span><br><span class="line">SnappyData Locator pid: 27651 status: running</span><br><span class="line"></span><br><span class="line">$ bin/snappy server start -dir=node-b/server1 -locators=dp0652:10334</span><br><span class="line"></span><br><span class="line">Starting SnappyData Server using locators for peer discovery: dp0652:10334</span><br><span class="line">Starting Thrift server for SnappyData at address localhost/127.0.0.1[1528]</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/node-b/server1/snappyserver.log</span><br><span class="line">SnappyData Server pid: 29595 status: running</span><br><span class="line">  Distributed system now has 2 members.</span><br><span class="line">  Other members: dp0652(27651:locator)&lt;v0&gt;:32709</span><br><span class="line"></span><br><span class="line">$ bin/snappy leader start -dir=node-c/lead1 -locators=dp0652:10334</span><br><span class="line"></span><br><span class="line">Starting SnappyData Leader using locators for peer discovery: localhost:10334</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/node-c/lead1/snappyleader.log</span><br><span class="line">SnappyData Leader pid: 29860 status: running</span><br><span class="line">  Distributed system now has 3 members.</span><br><span class="line">  Other members: dp0652(27651:locator)&lt;v0&gt;:32709, dp0652(29595:datastore)&lt;v7&gt;:9553</span><br></pre></td></tr></table></figure>
<p>如果要修改地址，可以用xx=xx的方式，<br>比如(修改locator的地址)[<a href="https://snappydatainc.github.io/snappydata/reference/configuration_parameters/start-locator/]" target="_blank" rel="noopener">https://snappydatainc.github.io/snappydata/reference/configuration_parameters/start-locator/]</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/snappy locator start -dir=node-a/locator1 -start-locator=192.168.6.52[1529]</span><br></pre></td></tr></table></figure>
<p>关闭各个组件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/snappy locator stop -dir=node-a/locator1</span><br><span class="line">bin/snappy server stop -dir=node-b/server1</span><br><span class="line">bin/snappy leader stop -dir=node-c/lead1</span><br></pre></td></tr></table></figure>
<p>执行spark-shell，并指定snappydata的连接地址为<code>localhost:1527</code>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --driver-memory=4g \</span><br><span class="line">  --conf spark.snappydata.connection=localhost:1527 \</span><br><span class="line">  --conf spark.snappydata.store.sys-disk-dir=quickstartdatadir2 \</span><br><span class="line">  --conf spark.snappydata.store.log-file=quickstartdatadir2/quickstart.log \</span><br><span class="line">  --driver-java-options=&quot;-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSClassUnloadingEnabled -XX:MaxNewSize=1g&quot;</span><br></pre></td></tr></table></figure>
<p>如果打开<a href="http://192.168.6.52:4042" target="_blank" rel="noopener">http://192.168.6.52:4042</a>，有spark app的页面，但是没有dashboard的页面。<br>打开<a href="http://192.168.6.52:5050/dashboard/" target="_blank" rel="noopener">http://192.168.6.52:5050/dashboard/</a>，可以查看snappydata的web ui。</p>
<blockquote>
<p>5050类似于spark standalone的8082 web-ui，4040类似于spark app的ui。</p>
</blockquote>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170714120810617" alt="snappy2"></p>
<h3 id="一键启动三个组件">一键启动三个组件</h3><p>上面三个启动脚本可以用一个脚本执行,这种情况默认的文件夹在work下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sbin/snappy-start-all.sh</span><br><span class="line">sbin/snappy-stop-all.sh</span><br><span class="line">sbin/snappy-status-all.sh</span><br></pre></td></tr></table></figure>
<p>snappy-start-all.sh会在本地启动一个locator,一个server,一个leader.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ sbin/snappy-start-all.sh</span><br><span class="line">Starting SnappyData Locator using peer discovery on: localhost[10334], other locators: localhost[10334]</span><br><span class="line">Starting Thrift server for SnappyData at address localhost/127.0.0.1[1527]</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/localhost-locator-1/snappylocator.log</span><br><span class="line">SnappyData Locator pid: 7949 status: running</span><br><span class="line"></span><br><span class="line">Starting SnappyData Server using locators for peer discovery: localhost[10334]</span><br><span class="line">Starting Thrift server for SnappyData at address localhost/127.0.0.1[1528]</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/localhost-server-1/snappyserver.log</span><br><span class="line">SnappyData Server pid: 8176 status: running</span><br><span class="line">  Distributed system now has 2 members.</span><br><span class="line">  Other members: localhost(7949:locator)&lt;v0&gt;:37846</span><br><span class="line"></span><br><span class="line">Starting SnappyData Leader using locators for peer discovery: localhost[10334]</span><br><span class="line">Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/localhost-lead-1/snappyleader.log</span><br><span class="line">SnappyData Leader pid: 8488 status: running</span><br><span class="line">  Distributed system now has 3 members.</span><br><span class="line">  Other members: localhost(7949:locator)&lt;v0&gt;:37846, dp0652(8176:datastore)&lt;v1&gt;:24462</span><br></pre></td></tr></table></figure>
<p>查看默认work下的目录</p>
<ul>
<li>lead：类似于Spark的Driver，文件夹是spark-jobserver，放了作业和jar包</li>
<li>locator：</li>
<li>server：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ tree work/</span><br><span class="line">work/</span><br><span class="line">├── localhost-lead-1</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE.if</span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE.lk</span><br><span class="line">│   ├── snappyleader.gfs</span><br><span class="line">│   ├── snappyleader.log</span><br><span class="line">│   ├── snappyleader.pid</span><br><span class="line">│   ├── spark-jobserver</span><br><span class="line">│   │   ├── filedao</span><br><span class="line">│   │   │   └── data</span><br><span class="line">│   │   │       ├── configs.data</span><br><span class="line">│   │   │       ├── jars.data</span><br><span class="line">│   │   │       └── jobs.data</span><br><span class="line">│   │   └── upload</span><br><span class="line">│   │       └── files.data</span><br><span class="line">│   └── start_snappyleader.log</span><br><span class="line">├── localhost-locator-1</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1.crf</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1.drf</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE.if</span><br><span class="line">│   ├── datadictionary</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1.crf</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1.drf</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE.if</span><br><span class="line">│   │   └── DRLK_IFGFXD-DD-DISKSTORE.lk</span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE.lk</span><br><span class="line">│   ├── locator10334state.dat</span><br><span class="line">│   ├── locator10334views.log</span><br><span class="line">│   ├── snappylocator.gfs</span><br><span class="line">│   ├── snappylocator.log</span><br><span class="line">│   ├── snappylocator.pid</span><br><span class="line">│   └── start_snappylocator.log</span><br><span class="line">├── localhost-server-1</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1.crf</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE_1.drf</span><br><span class="line">│   ├── BACKUPGFXD-DEFAULT-DISKSTORE.if</span><br><span class="line">│   ├── datadictionary</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1.crf</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE_1.drf</span><br><span class="line">│   │   ├── BACKUPGFXD-DD-DISKSTORE.if</span><br><span class="line">│   │   └── DRLK_IFGFXD-DD-DISKSTORE.lk</span><br><span class="line">│   ├── DRLK_IFGFXD-DEFAULT-DISKSTORE.lk</span><br><span class="line">│   ├── snappyserver.gfs</span><br><span class="line">│   ├── snappyserver.log</span><br><span class="line">│   ├── snappyserver.pid</span><br><span class="line">│   └── start_snappyserver.log</span><br><span class="line">└── members.txt</span><br></pre></td></tr></table></figure>
<h2 id="client">client</h2><p>先停止snappydata，然后修改远程机器conf下的servers, locators, leads.<br>将localhost改为主机地址:192.168.6.52，再重启snappydata。</p>
<p>注意：默认启动时，使用的是localhost，work下的文件夹页是localhost开头。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0652 snappydata-0.9-bin]$ sbin/snappy-start-all.sh</span><br><span class="line">192.168.6.52: Starting SnappyData Locator using peer discovery on: 192.168.6.52[10334], other locators: 192.168.6.52:10334</span><br><span class="line">192.168.6.52: Starting Thrift server for SnappyData at address /192.168.6.52[1527]</span><br><span class="line">192.168.6.52: Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/192.168.6.52-locator-1/snappylocator.log</span><br><span class="line">192.168.6.52: SnappyData Locator pid: 45151 status: running</span><br><span class="line">192.168.6.52: Starting SnappyData Server using locators for peer discovery: 192.168.6.52:10334</span><br><span class="line">192.168.6.52: Starting Thrift server for SnappyData at address /192.168.6.52[1528]</span><br><span class="line">192.168.6.52: Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/192.168.6.52-server-1/snappyserver.log</span><br><span class="line">192.168.6.52: SnappyData Server pid: 45860 status: running</span><br><span class="line">192.168.6.52:   Distributed system now has 2 members.</span><br><span class="line">192.168.6.52:   Other members: dp0652(45151:locator)&lt;v0&gt;:48205</span><br><span class="line">192.168.6.52: Starting SnappyData Leader using locators for peer discovery: 192.168.6.52:10334</span><br><span class="line">192.168.6.52: Logs generated in /home/qihuang.zheng/snappydata-0.9-bin/work/192.168.6.52-lead-1/snappyleader.log</span><br><span class="line">192.168.6.52: SnappyData Leader pid: 46726 status: running</span><br><span class="line">192.168.6.52:   Distributed system now has 3 members.</span><br><span class="line">192.168.6.52:   Other members: dp0652(45860:datastore)&lt;v1&gt;:8287, dp0652(45151:locator)&lt;v0&gt;:48205</span><br></pre></td></tr></table></figure>
<p>查看进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">45860 io.snappydata.tools.ServerLauncher server -critical-heap-percentage=90 -eviction-heap-percentage=81 locators=192.168.6.52:10334 log-file=snappyserver.log -client-bind-address=192.168.6.52</span><br><span class="line">46726 io.snappydata.tools.LeaderLauncher server locators=192.168.6.52:10334 log-file=snappyleader.log -run-netserver=false</span><br><span class="line">45151 io.snappydata.tools.LocatorLauncher server locators=192.168.6.52:10334 start-locator=192.168.6.52:10334 log-file=snappylocator.log -client-bind-address=192.168.6.52 -peer-discovery-address=192.168.6.52 jmx-manager=true</span><br></pre></td></tr></table></figure>
<p>本机下载snappydata的二进制包，并启动snappy脚本，通过thrift/jdbc连接远程的snappydata cluster</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">➜  snappydata-0.9-bin bin/snappy</span><br><span class="line">SnappyData 版本 0.9</span><br><span class="line">snappy&gt; connect client &apos;192.168.6.52:1527&apos;;</span><br><span class="line">九月 14, 2017 3:43:43 下午 java.util.logging.LogManager$RootLogger log</span><br><span class="line">信息: Starting client on &apos;10.57.4.219&apos; with ID=&apos;7059|2017/09/14 15:43:43.185 CST&apos;</span><br><span class="line">Using CONNECTION0</span><br><span class="line">snappy&gt; show connections ;</span><br><span class="line">CONNECTION0* -  jdbc:snappydata:thrift://192.168.6.52[1527]</span><br><span class="line">* = 当前连接</span><br><span class="line">snappy&gt; show tables;</span><br><span class="line">TABLE_SCHEM          |TABLE_NAME                    |TABLE_TYPE  |REMARKS</span><br><span class="line">--------------------------------------------------------------------------------------</span><br><span class="line">SYS                  |ASYNCEVENTLISTENERS           |SYSTEM TABLE|</span><br><span class="line">SYS                  |GATEWAYRECEIVERS              |SYSTEM TABLE|</span><br><span class="line">SYS                  |GATEWAYSENDERS                |SYSTEM TABLE|</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;SnappyData®&lt;/p&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Deep into Apache Gearpump</title>
    <link href="http://github.com/zqhxuyuan/2017/06/25/2017-06-24-Gearpump/"/>
    <id>http://github.com/zqhxuyuan/2017/06/25/2017-06-24-Gearpump/</id>
    <published>2017-06-24T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.257Z</updated>
    
    <content type="html"><![CDATA[<p>Deep into Apache Gearpump<br><a id="more"></a></p>
<p><strong>Prefix</strong>: I’ve heard Gearpump nearly one or two years ago, but never take a deep look inside. Until recently I’m almost done writing my chinese book about kafka internal implimentation, and decide to add some kafka relation opensouce system to my book’s appendix, such as spark streaming,storm,flink, and gearpump! So I finaly have a chance to deep into Gearpump.</p>
<h2 id="Introduce">Introduce</h2><p>According to offical documentation: “Gearpump is a 100% Akka based platform. We model big data streaming within the Akka actor hierarchy”. Below It’s Gearpump Actor Hierarchy architecture. PS: If you don’t know Actor right now, It’s fine, just think that’s another RPC layer or message transformer.</p>
<p><img src="http://gearpump.apache.org/releases/latest/img/actor_hierarchy.png" alt="geararch"></p>
<p>Everything in the diagram is an actor; they fall into two categories, Cluster Actors and Application Actors.</p>
<p><strong>Cluster Actors</strong></p>
<p><em>Worker</em>: Maps to a physical worker machine. It is responsible for <span style="border-bottom:1px dashed red;">managing resources</span> and report metrics on that machine.</p>
<p><em>Master</em>: Heart of the cluster, which <span style="border-bottom:1px dashed red;">manages workers, resources, and applications</span>. The main function is delegated to three child actors, App Manager, Worker Manager, and Resource Scheduler.</p>
<p><strong>Application Actors</strong></p>
<p><em>AppMaster</em>: Responsible to <span style="border-bottom:1px dashed red;">schedule the tasks to workers</span> and manage the state of the application. Different applications have different AppMaster instances and are isolated.</p>
<p><em>Executor</em>: Child of AppMaster, represents a JVM process. Its job is to <span style="border-bottom:1px dashed red;">manage the life cycle of tasks</span> and recover the tasks in case of failure.</p>
<p><em>Task</em>: Child of Executor, does the real job. Every task actor has a global unique address. One task actor can send data to any other task actors. This gives us great flexibility of how the computation DAG is distributed.</p>
<blockquote>
<p>All actors in the graph are weaved together with actor supervision, and actor watching and every error is handled properly via supervisors. In a master, a risky job is isolated and delegated to child actors, so it’s more robust. In the application, an extra intermediate layer “Executor” is created so that we can do fine-grained and fast recovery in case of task failure. A master watches the lifecycle of AppMaster and worker to handle the failures, but the life cycle of Worker and AppMaster are not bound to a Master Actor by supervision, so that Master node can fail independently. Several Master Actors form an Akka cluster, the Master state is exchanged using the Gossip protocol in a conflict-free consistent way so that there is no single point of failure. With this hierarchy design, we are able to achieve high availability.</p>
</blockquote>
<p>Next It’s a good entrance to knowing some <a href="http://gearpump.apache.org/releases/latest/introduction/basic-concepts/index.html" target="_blank" rel="noopener">basic concepts</a>. It’s very necessary, you should first take a detail/serious look at if you want to know how gearpump works.</p>
<p><strong>Master &amp; Worker</strong></p>
<blockquote>
<p>Gearpump follow <strong>master slave architecture</strong>. Every cluster contains one or more Master node, and several worker nodes. Worker node is responsible to manage local resources on single machine, and Master node is responsible to manage global resources of the whole cluster.</p>
</blockquote>
<p>If you have already know hadoop/spark such bigdata system, you should familiar those terminology. Here is the first comparison about gearpump and other system.</p>
<table>
<thead>
<tr>
<th>bigdata system</th>
<th>Master</th>
<th>Slave</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hadoop HDFS</td>
<td>NameNode</td>
<td>DataNode</td>
</tr>
<tr>
<td>Hadoop YARN</td>
<td>ReourceManager</td>
<td>NodeManager</td>
</tr>
<tr>
<td>Spark</td>
<td>ClusterManagement</td>
<td>Worker</td>
</tr>
<tr>
<td>Storm</td>
<td>Nimbus</td>
<td>Supervisor</td>
</tr>
<tr>
<td>Gearpump</td>
<td>Master</td>
<td>Worker</td>
</tr>
</tbody>
</table>
<p><strong>Application &amp; AppMaster &amp; Executor</strong></p>
<blockquote>
<p><strong>Application</strong> is what we want to parallel and run on the cluster. There are different application types, for example MapReduce application and streaming application are different application types. Gearpump natively supports Streaming Application types, it also contains several templates to help user to create custom application types, like distributedShell.</p>
</blockquote>
<blockquote>
<p>In runtime, every application instance is represented by a single <strong>AppMaster</strong> and a list of <strong>Executors</strong>. AppMaster represents the command and controls center of the Application instance. It communicates with user, master, worker, and executor to get the job done. Each executor is a parallel unit for distributed application. Typically AppMaster and Executor will be started as JVM processes on worker nodes.</p>
</blockquote>
<p>Now we have talking all important components in gearpump. Notice here we did’t mentioned Task as appeared in previous actor hierarchy. Also notice that Application is not an actor but an Java main class. Next take a look at Application Submission Flow in gearpump.</p>
<blockquote>
<p>When user submits an application to Master, Master will first find an available worker to start the AppMaster. After AppMaster is started, AppMaster will request Master for more resources (worker) to start executors. The Executor now is only an empty container. After the executors are started, the AppMaster will then distribute real computation tasks to the executor and run them in parallel way.</p>
</blockquote>
<blockquote>
<p>To submit an application, a Gearpump client specifies a computation defined within a DAG and submits this to an active master. The SubmitApplication message is sent to the Master who then forwards this to an AppManager. </p>
</blockquote>
<p><img src="http://gearpump.apache.org/releases/latest/img/submit.png" alt="submit app"> </p>
<blockquote>
<p>The AppManager locates an available worker and launches an AppMaster in a sub-process JVM of the worker. The AppMaster will then negotiate with the Master for Resource allocation in order to distribute the DAG as defined within the Application. The allocated workers will then launch Executors (new JVMs).</p>
</blockquote>
<p><img src="http://gearpump.apache.org/releases/latest/img/submit2.png" alt="launch"></p>
<p>Here I summary basic steps of submit application. notice the step number below are’t corresponding to the official pictures above.</p>
<ol>
<li>User(client) submits an streaming application to gearpump Master;</li>
<li>Master forward <code>SubmitApplication</code> request to AppManager;</li>
<li>Master will first find an available worker to start the AppMaster;</li>
<li>AppMaster started(as Executor) on one of worker which master specified, until now, AppManager on Master can send  <code>SubmitApplicationResult</code> to client;</li>
<li>AppMaster send <code>RequestResource</code> to master, the purpose of this step is ask resources to run/launch Tasks which doing real job. After all, AppMaster is not responsible to running job, but instead let Tasks doing the job. Notice the lifecycle of both AppMaster and Tasks all resides in Executors. So If you want to start AppMaster or Task, you first must start Executor, then let Executor start AppMaster and Task;</li>
<li>Once AppMaster receive <code>ResouceAllocated</code> response, it’ll send <code>LaunchExecutor</code> to workers which Master pointing out where to go. For ex, the ResouceAllocated response says by Master to AppMaster: you can run executors on workers #1 and #2. Then AppMaster will send LaunchExecutor request to this two workers;</li>
<li>The Workers receive LaunchExecutor request from AppMaster, it then spawn an Executor as a java process. The reason why spawn a new process here is that the Executor and Worker thread should separate, which means the working process of Executor and Worker shouldn’t affect each other;</li>
<li>Just like Worker register to Master for reporting resources, the Executor also register to AppMaster by sending <code>RegisterExecutor</code> request. If someone regist to other-one, that means someone wants to be managed/controlled by other-one. for example, students regist to school, company regist to Mainland China, employee regist to company and so on;</li>
<li>The AppMaster receive <code>RegisterExecutor</code> request from Executor on Worker, it then ask Executor to start Task;</li>
<li>As AppMaster may getting more than one resouce at step6, and each Executor all register to AppMaster, so AppMaster can start multi task on this registerd Executor;</li>
<li>Each Task reside in Executor has DAG information defined within Application, so every Task can doing real job.</li>
</ol>
<p>The workflow above was extraordinary like yarn application below. I take the picture and description from <a href="https://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/" target="_blank" rel="noopener">this excellent hortonworks blog</a>.</p>
<ol>
<li>A client program <em>submits</em> the application, including the necessary specifications to launch the application-specific <strong>ApplicationMaster</strong> itself.</li>
<li>The <strong>ResourceManager</strong> assumes the responsibility to negotiate a specified container in which to <em>start</em> the ApplicationMaster and then <em>launches</em> the ApplicationMaster.</li>
<li>The ApplicationMaster, on boot-up, <em>registers</em> with the ResourceManager – the registration allows the client program to query the ResourceManager for details, which allow it to  directly communicate with its own ApplicationMaster.</li>
<li>During normal operation the ApplicationMaster negotiates appropriate resource containers via the <em>resource-request </em>protocol.</li>
<li>On successful <em>container allocations</em>, the ApplicationMaster <em>launches</em> the container by providing the container launch specification to the NodeManager. The launch specification, typically, includes the necessary information to allow the container to communicate with the ApplicationMaster itself.</li>
<li>The application code executing within the container then provides necessary information (progress, status etc.) to its ApplicationMaster via an application-specific protocol.</li>
<li>During the application execution, the client that submitted the program communicates directly with the ApplicationMaster to get status, progress updates etc. via an application-specific protocol.</li>
<li>Once the application is complete, and all necessary work has been finished, the ApplicationMaster deregisters with the ResourceManager and shuts down, allowing its own container to be repurposed.</li>
</ol>
<p><img src="https://2xbbhjxc6wk3v21p62t8n4d4-wpengine.netdna-ssl.com/wp-content/uploads/2012/08/yarnflow.png" alt="yarn"></p>
<p>The picture above start two client application to yarn cluster, the ApplicationMaster reside on node2 of red one start three containers on node1 and node3, the ApplicationMaster reside on node1 of blue one only start one container. </p>
<p>In yarn, ResouceManager take responsible to launch ApplicationManager on one of container, and launching Tasks on containers is the responsibility of ApplicationManager. But as you know, the ApplicationManager did’t know cluster resources, so he ask ResouceManager to give him the information of where to start tasks. Now we summary some conclusions:</p>
<ol>
<li>ResouceManager launch ApplicationManager on one of NodeManager.</li>
<li>ApplicationManager launch Tasks on multi NodeManagers.</li>
<li>NodeManagers report resouce to ResouceManager.</li>
<li>Containers report task execution progress to ApplicationManager.  </li>
<li>ResouceManager manager ApplicationManager, and ApplicationManager manager tasks. If all tasks monitored by ApplicationManager was finished, then Application registered to ReousceManager was completed.</li>
</ol>
<p>Step into gearpump, there are similiarity idea inspired from yarn. We could take yarn’s container as gearpump’s Executor, and yarn’s NodeManager as gearpump’s Worker. Because Containers reside in NodeManager at yarn world, and Executors reside in Worker at gearpump world.</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170624165428321" alt="yarn-gp"></p>
<p>We could also consider yarn’s ResouceManager as gearpump’s AppManager. Note that AppManager is different from AppMaster, which the former is at Master side, and the latter is at Worker side.</p>
<p>The Master in Gearpump have three main components: AppManager,Scheduler,Worker Manager. In reality, there are non WorkerManager class around gearpump source code,but Master indeed has a map which mapping Worker ActorRef to WorkerId. </p>
<p>After oveview gearpump architecture, Let’s begin explore gearpump inside now.</p>
<h2 id="Part-1:_Application">Part-1: Application</h2><p>First given a WordCount example, We sumbit an StreamApplication through ClientContext. Inside the application() method, we create three <code>Processor</code> and connect by <code>~</code> to construct a DAG graph.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> <span class="keyword">extends</span> <span class="title">AkkaApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">application</span></span>(config: <span class="type">ParseResult</span>, system: <span class="type">ActorSystem</span>): </span><br><span class="line">      <span class="type">StreamApplication</span> = &#123;</span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> actorSystem = system</span><br><span class="line">    <span class="keyword">val</span> split = <span class="keyword">new</span> <span class="type">Split</span></span><br><span class="line">    <span class="keyword">val</span> sourceProcessor = <span class="type">DataSourceProcessor</span>(split, <span class="number">2</span>, <span class="string">"Split"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = <span class="type">Processor</span>[<span class="type">Sum</span>](<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">val</span> computation = sourceProcessor ~ <span class="type">HashPartitioner</span> ~&gt; sum</span><br><span class="line">    <span class="keyword">val</span> app = <span class="type">StreamApplication</span>(<span class="string">"wordCount"</span>, <span class="type">Graph</span>(computation))</span><br><span class="line">    app</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(akkaConf: <span class="type">Config</span>, args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> context: <span class="type">ClientContext</span> = <span class="type">ClientContext</span>(akkaConf)</span><br><span class="line">    <span class="keyword">val</span> app = application(config, context.system)</span><br><span class="line">    context.submit(app)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>StreamApplication is one of gearpump supported application type, there’re other applications such as MapReduce could run in gearpump. Each Application type has special appMaster class, StreamApplication’s appMaster is AppMaster. There’re some other ApplicationMaster actor implementation embeded: DistShellAppMaster,DistServiceAppMaster,and AppMaster.</p>
<p>Note Application is a scala App, but ApplicationMaster is an Actor. So what’s different between an App and and Actor? Well, App normaly has a main method doing what you want, but actor doing much more complicate thing.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">userConfig</span></span>(<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">UserConfig</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMaster</span></span>: <span class="type">Class</span>[_ &lt;: <span class="type">ApplicationMaster</span>]</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ApplicationMaster</span> <span class="keyword">extends</span> <span class="title">Actor</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">StreamApplication</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">appMaster</span></span>: <span class="type">Class</span>[_ &lt;: <span class="type">ApplicationMaster</span>] = classOf[<span class="type">AppMaster</span>] </span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMaster</span>(<span class="params">appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span></span>) </span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ApplicationMaster</span> </span>&#123;...&#125;</span><br></pre></td></tr></table></figure>
<p>ClientContext is a user facing util to submit/manage an application. The AppDescription describe application metadata such as appMaster name(here is AppMaster).</p>
<p>In the Akka world, Actor is the king. Client send SubmitApplication request to Master Actor, and expect get SubmitApplicationResult response from Master. Messages are sent to an Actor through one of the following methods.</p>
<ul>
<li><code>!</code> means “fire-and-forget”, e.g. send a message asynchronously and return immediately. Also known as tell.</li>
<li><code>?</code> sends a message asynchronously and returns a Future representing a possible reply. Also known as ask. That’s the way client submit application doing here.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClientContext</span>(<span class="params">config: <span class="type">Config</span>, sys: <span class="type">ActorSystem</span>, _master: <span class="type">ActorRef</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(app: <span class="type">Application</span>, jar: <span class="type">String</span>, executorNum: <span class="type">Int</span>)= &#123;</span><br><span class="line">    <span class="keyword">val</span> appName = ...</span><br><span class="line">    <span class="keyword">val</span> submissionConfig = ...</span><br><span class="line">    <span class="keyword">val</span> appDescription = <span class="type">AppDescription</span>(appName,app.appMaster.getName,...)</span><br><span class="line">    <span class="keyword">val</span> appJar = <span class="type">Option</span>(jar).map(loadFile)</span><br><span class="line">    submitApplication(<span class="type">SubmitApplication</span>(appDescription, appJar))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitApplication</span></span>(submitApplication: <span class="type">SubmitApplication</span>)=&#123;</span><br><span class="line">    <span class="keyword">val</span> result = <span class="type">ActorUtil</span>.askActor[<span class="type">SubmitApplicationResult</span>](</span><br><span class="line">        master, submitApplication, masterClientTimeout)</span><br><span class="line">    <span class="keyword">val</span> application = result.appId <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Success</span>(appId) =&gt;</span><br><span class="line">        <span class="type">Console</span>.println(<span class="string">s"Submit app succeed. The app id is <span class="subst">$appId</span>"</span>)</span><br><span class="line">        <span class="keyword">new</span> <span class="type">RunningApplication</span>(appId, master, masterClientTimeout)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Failure</span>(ex) =&gt; <span class="keyword">throw</span> ex</span><br><span class="line">    &#125;</span><br><span class="line">    application</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now Let’s see how Master deal with SubmitApplication. Before this, you should know that client only submit application when Master has started. Also note that when start Master, we also start some Workers to form a gearpump cluster. Only then the cluster is stabled, client then can submit application. We can see that when startup Master, in preStart() method, Master created an AppManager and Scheduler by invoking <code>context.actorOf(...)</code>. That means before client submit application, AppManager and Scheduler already exists in Master, and they both preparing to work.</p>
<p>We’re also seeing a <code>receiveHandler()</code> method return Receive object, and was invoked by <code>waitForNextWorkerId()</code> method. What <code>context.become()</code> and <code>orElse</code> meaning? well, normaly you define one receive method, but here you have seen there’re multi receive method, so become() method of ActorContext is used for switchover between different receive method.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">Master</span> <span class="keyword">extends</span> <span class="title">Actor</span> <span class="keyword">with</span> <span class="title">Stash</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> kvService = context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">InMemoryKVService</span>()))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> appManager: <span class="type">ActorRef</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> scheduler: <span class="type">ActorRef</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> workers = <span class="keyword">new</span> immutable.<span class="type">HashMap</span>[<span class="type">ActorRef</span>, <span class="type">WorkerId</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment">// when start up Master, send GetKV to kvService immediatery</span></span><br><span class="line">  kvService ! <span class="type">GetKV</span>(<span class="type">MASTER_GROUP</span>, <span class="type">WORKER_ID</span>) </span><br><span class="line">  context.become(waitForNextWorkerId) <span class="comment">// wait for getting result </span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForNextWorkerId</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">GetKVSuccess</span>(_, result) =&gt; <span class="comment">// receive GetKV response</span></span><br><span class="line">      context.become(receiveHandler) <span class="comment">// switchover to receiveHandler</span></span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; stash() <span class="comment">// why do we stash here?</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveHandler</span></span>: <span class="type">Receive</span> = workerMsgHandler orElse</span><br><span class="line">    appMasterMsgHandler orElse <span class="comment">// AppMaster to Master</span></span><br><span class="line">    onMasterListChange orElse <span class="comment">// Master change</span></span><br><span class="line">    clientMsgHandler orElse <span class="comment">// Client to Master. you'll see submit app here</span></span><br><span class="line">    kvServiceMsgHandler orElse <span class="type">ActorUtil</span>.defaultMsgHandler(self)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">preStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    appManager = context.actorOf(</span><br><span class="line">        <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">AppManager</span>(kvService, <span class="type">AppMasterLauncher</span>)),</span><br><span class="line">        classOf[<span class="type">AppManager</span>].getSimpleName)</span><br><span class="line">    scheduler = context.actorOf(<span class="type">Props</span>(schedulerClass))</span><br><span class="line">    context.system.eventStream.subscribe(self,classOf[<span class="type">DisassociatedEvent</span>])</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Now you have overview the main function in Master, lets see how clientMsgHandler receive method response to client’s submit application request. I have omit other unimportance request only left submit and restart application. The Master delegate/forward reqeust to AppManager.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def clientMsgHandler: Receive = &#123;</span><br><span class="line">  case app: SubmitApplication =&gt; appManager.forward(app)</span><br><span class="line">  case app: RestartApplication =&gt; appManager.forward(app)</span><br><span class="line">  case register: RegisterAppResultListener =&gt; appManager forward register</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AppManager is dedicated child of Master to manager all applications. The AppManager behaviour similar as Master.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">AppManager</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    kvService: <span class="type">ActorRef</span>, launcher: <span class="type">AppMasterLauncherFactory</span></span>) </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Actor</span> <span class="keyword">with</span> <span class="title">Stash</span> <span class="keyword">with</span> <span class="title">TimeOutScheduler</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  kvService ! <span class="type">GetKV</span>(<span class="type">MASTER_GROUP</span>, <span class="type">MASTER_STATE</span>)</span><br><span class="line">  context.become(waitForMasterState)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForMasterState</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">GetKVSuccess</span>(_, result) =&gt;</span><br><span class="line">      context.become(receiveHandler)</span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; stash()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveHandler</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    clientMsgHandler orElse <span class="comment">// Client to AppManager</span></span><br><span class="line">      appMasterMessage orElse <span class="comment">// AppMaster to AppManager</span></span><br><span class="line">      selfMsgHandler orElse</span><br><span class="line">      workerMessage orElse <span class="comment">// Worker to AppManager</span></span><br><span class="line">      appDataStoreService orElse terminationWatch</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">clientMsgHandler</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SubmitApplication</span>(app, jar, username) =&gt;</span><br><span class="line">      <span class="keyword">val</span> client = sender()</span><br><span class="line">      context.actorOf(launcher.props(</span><br><span class="line">        nextAppId, <span class="number">-1</span>, app, jar, username, context.parent, client))</span><br><span class="line">      <span class="comment">// ommit something like save application metadata to kv store</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Master create AppManager by invoke <code>context.actorOf(Props(...))</code>, here AppManager create AppMasterLauncher Actor by <code>context.actorOf(launcher.props(..))</code>. <strong><code>AppMasterLauncher</code> is a child Actor of <code>AppManager</code>, it is responsible to launch the <code>AppMaster</code> on the cluster.</strong> </p>
<p>When AppManager receive SubmitApplication from client, it create AppMasterLauncher, and send RequestResource to master then wait for ResourceAllocation. </p>
<p>When AppMasterLauncher receive ResourceAllocated response from master, it will Try to launch a executor for AppMaster on worker specified by ResourceAllocated response.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMasterLauncher</span>(<span class="params">...,master: <span class="type">ActorRef</span>, client: <span class="type">ActorRef</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="type">LOG</span>.info(<span class="string">s"Ask Master resource to start AppMaster <span class="subst">$appId</span>..."</span>)</span><br><span class="line">  master ! <span class="type">RequestResource</span>(appId, <span class="type">ResourceRequest</span>(<span class="type">Resource</span>(<span class="number">1</span>))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = waitForResourceAllocation</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForResourceAllocation</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ResourceAllocated</span>(allocations) =&gt;</span><br><span class="line">      <span class="keyword">val</span> <span class="type">ResourceAllocation</span>(resource, worker, workerId) = allocations(<span class="number">0</span>)</span><br><span class="line">      <span class="keyword">val</span> workerInfo = <span class="type">WorkerInfo</span>(workerId, worker)</span><br><span class="line">      <span class="keyword">val</span> appMasterContext = <span class="type">AppMasterContext</span>(...)</span><br><span class="line">      <span class="comment">// Try to launch a executor for AppMaster on worker for app</span></span><br><span class="line">      <span class="keyword">val</span> name = <span class="type">ActorUtil</span>.actorNameForExecutor(appId, executorId)</span><br><span class="line">      <span class="keyword">val</span> selfPath = <span class="type">ActorUtil</span>.getFullPath(context.system, self.path)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> executorJVM = <span class="type">ExecutorJVMConfig</span>(</span><br><span class="line">        classOf[<span class="type">ActorSystemBooter</span>].getName, <span class="type">Array</span>(name, selfPath), jar,</span><br><span class="line">        username, appMasterAkkaConfig)</span><br><span class="line"></span><br><span class="line">      worker ! <span class="type">LaunchExecutor</span>(appId, executorId, resource, executorJVM)</span><br><span class="line">      context.become(waitForActorSystemToStart(worker, appMasterContext, resource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Let’s see how Worker deal with LaunchExecutor reqeust from AppMasterLauncher.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[cluster] <span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params">masterProxy: <span class="type">ActorRef</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span></span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">service</span></span>: <span class="type">Receive</span> = appMasterMsgHandler orElse clientMessageHandler </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMasterMsgHandler</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> launch: <span class="type">LaunchExecutor</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> executor = context.actorOf(<span class="type">Props</span>(classOf[<span class="type">ExecutorWatcher</span>], </span><br><span class="line">        launch, masterInfo, ioPool, jarStoreClient, executorProcLauncher))</span><br><span class="line">      context.watch(executor)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The ExecutorWatcher create a java process and the main class <code>ActorSystemBooter</code> is coming from ExecutorJVMConfig which defined in AppMasterLauncher.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExecutorWatcher</span>(<span class="params">launch: <span class="type">LaunchExecutor</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">    procLauncher: <span class="type">ExecutorProcessLauncher</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> ctx = launch.executorJvmConfig</span><br><span class="line">  procLauncher.createProcess(ctx.mainClass, ctx.arguments)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ExecutorWatcher is an Actor, ActorSystemBooter is an pure scala app. But inside ActorSystemBooter’s main method, it create another actor: Daemon.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ActorSystemBooter</span>(<span class="params">config: <span class="type">Config</span></span>) </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">boot</span></span>(name: <span class="type">String</span>, reportBackActor: <span class="type">String</span>): <span class="type">ActorSystem</span> = &#123;</span><br><span class="line">    system.actorOf(<span class="type">Props</span>(classOf[<span class="type">Daemon</span>], name, reportBackActor), <span class="string">"daemon"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ActorSystemBooter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(config: <span class="type">Config</span>): <span class="type">ActorSystemBooter</span> = <span class="keyword">new</span> <span class="type">ActorSystemBooter</span>(config)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> name = args(<span class="number">0</span>) <span class="comment">// The parameter was passed when construnct </span></span><br><span class="line">    <span class="keyword">val</span> reportBack = args(<span class="number">1</span>) <span class="comment">// ExecutorJVMConfig at AppMasterLauncher</span></span><br><span class="line">    apply(config).boot(name, reportBack)</span><br><span class="line">  &#125; </span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Daemon</span>(<span class="params">val name: <span class="type">String</span>, reportBack: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">    <span class="keyword">val</span> reportBackActor = context.actorSelection(reportBack)</span><br><span class="line">    reportBackActor ! <span class="type">RegisterActorSystem</span>(</span><br><span class="line">        <span class="type">ActorUtil</span>.getSystemAddress(context.system).toString)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Those many Actor headache me, and the invoke chain nest and nest again. So I draw a picture to help me understand what happend all the way around. To make my picture looks vividly, I use gear to indicate an Actor, you can see except ActorSystemBooter, all others are Actor. The underline character means request. Let me outlines some import steps.</p>
<ol>
<li>AppManager create AppMasterLauncher which then send RequestResource to Master</li>
<li>After AppMasterLauncher receive ResourceAllocated, it send LauncherExecutor request to Worker</li>
<li>Worker create an ExecutorWatcher and create a java Daemon process which send RegisterActorSystem request back to Master</li>
</ol>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170624205039225" alt="9"></p>
<p>Now the AppMasterLauncher is going to deal with RegisterActorSystem request. If you backward to check AppMasterLauncher, you can find that: after AppMasterLauncher send LaunchExecutor, it is waiting for ActorSystem to start.</p>
<p>After Daemon actor in Worker send <code>RegisterActorSystem</code> request to <code>AppMasterLauncher</code>, the AppMasterLauncher finally have chance to receive RegisterActorSystem event, first it send <code>ActorSystemRegistered</code> request to Daemon, and then send another request <code>CreateActor</code> to Daemon again.</p>
<ol>
<li>Daemon on Worker send RegisterActorSystem request to AppMasterLauncher</li>
<li>AppMasterLauncher on Master send ActorSystemRegistered to Daemon on Worker</li>
<li>AppMasterLauncher on Master send CreateActor to Daemon on Worker</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMasterLauncher</span>(<span class="params">...,master: <span class="type">ActorRef</span>, client: <span class="type">ActorRef</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForResourceAllocation</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    worker ! <span class="type">LaunchExecutor</span>(appId, executorId, resource, executorJVM)</span><br><span class="line">    context.become(</span><br><span class="line">        waitForActorSystemToStart(worker, appMasterContext, resource))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForActorSystemToStart</span></span>(worker: <span class="type">ActorRef</span>, appContext: <span class="type">AppMasterContext</span>,</span><br><span class="line">      resource: <span class="type">Resource</span>): <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterActorSystem</span>(systemPath) =&gt;</span><br><span class="line">      sender ! <span class="type">ActorSystemRegistered</span>(worker)</span><br><span class="line">      <span class="comment">// There're many masters construct Master HA in case of fault</span></span><br><span class="line">      <span class="keyword">val</span> masterAddress = systemConfig.getStringList(<span class="type">GEARPUMP_CLUSTER_MASTERS</span>)</span><br><span class="line">        .asScala.map(<span class="type">HostPort</span>(_)).map(<span class="type">ActorUtil</span>.getMasterActorPath)</span><br><span class="line"></span><br><span class="line">      sender ! <span class="type">CreateActor</span>(</span><br><span class="line">        <span class="type">AppMasterRuntimeEnvironment</span>.props(masterAddress, app, appContext))</span><br><span class="line">      context.become(waitForAppMasterToStart(worker, appMasterTimeout))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForAppMasterToStart</span></span>(worker: <span class="type">ActorRef</span>, cancel: <span class="type">Cancellable</span>)= &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ActorCreated</span>(appMaster, _) =&gt;</span><br><span class="line">      cancel.cancel()</span><br><span class="line">      sender ! <span class="type">BindLifeCycle</span>(appMaster)</span><br><span class="line">      <span class="type">LOG</span>.info(<span class="string">s"AppMaster is created, mission complete..."</span>)</span><br><span class="line">      replyToClient(<span class="type">SubmitApplicationResult</span>(<span class="type">Success</span>(appId)))</span><br><span class="line">      context.stop(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Seems AppMasterLauncher and Daemon are playing ping-pong, and they both back and forth many times. Finally after Daemon create another Actor which we’ll talk about later, it then send ActorCreated back to AppMasterLauncher. </p>
<ol>
<li>Daemon on Worker send ActorCreated reqeust to AppMasterLauncher on Master</li>
<li>AppMasterLauncher send BindLifeCycle request back to Daemon on Worker</li>
<li>and then send SubmitApplicationResult back to Client</li>
<li>Daemon on Worker receive BindLifeCycle request from AppMasterLauncher and watch the actor. this actor being watched by Daemon is AppMaster.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Daemon</span>(<span class="params">val name: <span class="type">String</span>, reportBack: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Actor</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitForRegisterResult</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ActorSystemRegistered</span>(parent) =&gt;</span><br><span class="line">      timeout.cancel()</span><br><span class="line">      context.watch(parent)</span><br><span class="line">      context.become(waitCommand)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">waitCommand</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">BindLifeCycle</span>(actor) =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(<span class="string">s"ActorSystem <span class="subst">$name</span> Binding life cycle with actor: <span class="subst">$actor</span>"</span>)</span><br><span class="line">      context.watch(actor)</span><br><span class="line">    <span class="keyword">case</span> create<span class="meta">@CreateActor</span>(props: <span class="type">Props</span>, name: <span class="type">String</span>) =&gt;</span><br><span class="line">      <span class="keyword">val</span> actor = <span class="type">Try</span>(context.actorOf(props, name)) <span class="comment">// create another actor</span></span><br><span class="line">      actor <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Success</span>(actor) =&gt; sender ! <span class="type">ActorCreated</span>(actor, name)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Failure</span>(e) =&gt; sender ! <span class="type">CreateActorFailed</span>(props.clazz.getName, e)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">PoisonPill</span> =&gt;</span><br><span class="line">      context.stop(self)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Terminated</span>(actor) =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(<span class="string">s"System <span class="subst">$name</span> Watched actor is terminated <span class="subst">$actor</span>"</span>)</span><br><span class="line">      context.stop(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170624212741793" alt="9"></p>
<p>Daemon create an Actor which defined in RegisterActorSystem on AppMasterLauncher. This Actor is <code>AppMasterRuntimeEnvironment</code>, it’ll create AppMaster. </p>
<p>We know that create Actor can use <code>context.actorOf(props)</code> method, here the props is passed from AppMasterLauncher to Daemon, but not created on Daemon side. Why do we doing this way? Because only AppMasterLauncher know how to create an AppMaster. Passing the props inside CreateActor is just like passing other request. Now the mainpoint focus transfer to AppMasterRuntimeEnvironment.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AppMasterRuntimeEnvironment</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">props</span></span>(masters: <span class="type">Iterable</span>[<span class="type">ActorPath</span>], </span><br><span class="line">      app: <span class="type">AppDescription</span>, appContextInput: <span class="type">AppMasterContext</span></span><br><span class="line">      ): <span class="type">Props</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> master = (appId: <span class="type">AppId</span>, masterProxy: <span class="type">MasterActorRef</span>) =&gt;</span><br><span class="line">      <span class="type">MasterWithExecutorSystemProvider</span>.props(appId, masterProxy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> appMaster = (appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>) =&gt;</span><br><span class="line">      <span class="type">LazyStartAppMaster</span>.props(appContext, app)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> masterConnectionKeeper = (master: <span class="type">MasterActorRef</span>, registerAppMaster:</span><br><span class="line">      <span class="type">RegisterAppMaster</span>, listener: <span class="type">ListenerActorRef</span>) =&gt; <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MasterConnectionKeeper</span>(</span><br><span class="line">        registerAppMaster, master, masterStatusListener = listener))</span><br><span class="line"></span><br><span class="line">    <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">AppMasterRuntimeEnvironment</span>(appContextInput, app, masters,</span><br><span class="line">      master, appMaster, masterConnectionKeeper))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AppMasterRuntimeEnvironment will create three Actor once it’s created. It serves as runtime environment for AppMaster. When starting an AppMaster, we need to setup the connection to master(an MasterProxy which substitute to Master), and prepare other environments.</p>
<p>The MasterProxy also extend the function of Master, by providing a scheduler service for Executor System. AppMaster can ask Master for executor system directly. details like requesting resource, contacting worker to start a process, and then starting an executor system is hidden from AppMaster.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[appmaster] <span class="class"><span class="keyword">class</span> <span class="title">AppMasterRuntimeEnvironment</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    appContextInput: <span class="type">AppMasterContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    app: <span class="type">AppDescription</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masters: <span class="type">Iterable</span>[<span class="type">ActorPath</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    masterFactory: (<span class="type">AppId</span>, <span class="type">MasterActorRef</span></span>) <span class="title">=&gt;</span> <span class="title">Props</span>,</span></span><br><span class="line"><span class="class">    <span class="title">appMasterFactory</span></span>: (<span class="type">AppMasterContext</span>, <span class="type">AppDescription</span>) =&gt; <span class="type">Props</span>,</span><br><span class="line">    masterConnectionKeeperFactory: (<span class="type">MasterActorRef</span>, <span class="type">RegisterAppMaster</span>, <span class="type">ListenerActorRef</span>) =&gt; <span class="type">Props</span>) <span class="keyword">extends</span> <span class="type">Actor</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> master = context.actorOf(</span><br><span class="line">    masterFactory(appId, context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">MasterProxy</span>(masters, <span class="number">30.</span>seconds)))))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appContext = appContextInput.copy(masterProxy = master)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create appMaster proxy to receive command and forward to appmaster</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appMaster = context.actorOf(appMasterFactory(appContext, app))</span><br><span class="line">  context.watch(appMaster)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> registerAppMaster = <span class="type">RegisterAppMaster</span>(</span><br><span class="line">    appId, appMaster, appContext.workerInfo)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> masterConnectionKeeper = context.actorOf(</span><br><span class="line">    masterConnectionKeeperFactory(master, registerAppMaster, self))</span><br><span class="line">  context.watch(masterConnectionKeeper)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterConnected</span> =&gt;</span><br><span class="line">      <span class="type">LOG</span>.info(<span class="string">s"Master is connected, start AppMaster <span class="subst">$appId</span>..."</span>)</span><br><span class="line">      appMaster ! <span class="type">StartAppMaster</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterStopped</span> =&gt;</span><br><span class="line">      <span class="type">LOG</span>.error(<span class="string">s"Master is stopped, stop AppMaster <span class="subst">$appId</span>..."</span>)</span><br><span class="line">      context.stop(self)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Terminated</span>(actor) =&gt; actor <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> `appMaster` =&gt;</span><br><span class="line">        <span class="type">LOG</span>.error(<span class="string">s"AppMaster <span class="subst">$appId</span> is stopped, shutdown myself"</span>)</span><br><span class="line">        context.stop(self)</span><br><span class="line">      <span class="keyword">case</span> `masterConnectionKeeper` =&gt;</span><br><span class="line">        <span class="type">LOG</span>.error(<span class="string">s"Master connection keeper is stopped, appId: <span class="subst">$appId</span>, shutdown myself"</span>)</span><br><span class="line">        context.stop(self)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt; <span class="comment">// Skip</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>The workflow from creating <code>AppMasterRuntimeEnvironment</code> to create <code>AppMaster</code> is trigged through <code>MasterConnectionKeeper</code> by sending <code>RegisterAppMaster</code> request to <code>AppMasterLauncher</code>. Finally when <code>AppMasterRuntimeEnvironment</code> receive <code>MasterConnected</code> from <code>MasterConnectionKeeper</code>, it send <code>StartAppMaster</code> to <code>AppMaster</code>. happy now! Take long long way bring up to AppMaster.</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170624220921664" alt="9"></p>
<p>Note AppMasterRuntimeEnvironment did not send StartAppMaster directory to AppMaster but to LazyStartAppMaster. and Every message send to LazyStartAppMaster will forward to AppMaster. Why do we need a Lazy AppMaster? If you take look at LazyStartAppMaster, you’ll notice that LazyStartAppMaster is not really an AppMaster but it’s responsible to create AppMaster only when it receive StartAppMaster request from AppMasterRuntimeEnvironment. So you wont’t find StartAppMaster on AppMaster.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LazyStartAppMaster</span>(<span class="params">appId: <span class="type">Int</span>, appMasterProps: <span class="type">Props</span></span>) </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">Actor</span> <span class="keyword">with</span> <span class="title">Stash</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = <span class="literal">null</span></span><br><span class="line">  context.become(startAppMaster)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startAppMaster</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">StartAppMaster</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> appMaster = context.actorOf(appMasterProps, <span class="string">"appmaster"</span>)</span><br><span class="line">      context.watch(appMaster)</span><br><span class="line">      context.become(terminationWatch(appMaster) orElse </span><br><span class="line">        appMasterService(appMaster))</span><br><span class="line">      unstashAll()</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; stash()</span><br><span class="line"> &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">appMasterService</span></span>(appMaster: <span class="type">ActorRef</span>): <span class="type">Receive</span> = &#123;</span><br><span class="line">    <span class="keyword">case</span> msg =&gt; appMaster forward msg</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span>[appmaster] <span class="class"><span class="keyword">object</span> <span class="title">LazyStartAppMaster</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">props</span></span>(appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span>): <span class="type">Props</span> = &#123;</span><br><span class="line">    <span class="comment">// the class name of app.appMaster is AppMaster </span></span><br><span class="line">    <span class="comment">// which will create when receive StartAppMaster</span></span><br><span class="line">    <span class="keyword">val</span> appMasterProps = <span class="type">Props</span>(<span class="type">Class</span>.forName(app.appMaster), appContext, app)</span><br><span class="line">    <span class="type">Props</span>(<span class="keyword">new</span> <span class="type">LazyStartAppMaster</span>(appContext.appId, appMasterProps))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>The AppMaster is the head of a streaming application. It contains:</p>
<ol>
<li>ExecutorManager to manage all executors.</li>
<li>TaskManager to manage all tasks,</li>
<li>ClockService to track the global clock for this streaming application.</li>
<li>Scheduler to decide which a task should be scheduled to.</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AppMaster</span>(<span class="params">appContext: <span class="type">AppMasterContext</span>, app: <span class="type">AppDescription</span></span>) </span></span><br><span class="line"><span class="class">    <span class="keyword">extends</span> <span class="title">ApplicationMaster</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dagManager = context.actorOf(<span class="type">Props</span>(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">DagManager</span>(appContext.appId, userConfig, store,</span><br><span class="line">    <span class="type">Some</span>(getUpdatedDAG))))</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> taskManager: <span class="type">Option</span>[<span class="type">ActorRef</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> clockService: <span class="type">Option</span>[<span class="type">ActorRef</span>] = <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> executorManager: <span class="type">ActorRef</span> =</span><br><span class="line">    context.actorOf(<span class="type">ExecutorManager</span>.props(userConfig, appContext, app.clusterConfig, app.name),</span><br><span class="line">      <span class="type">ActorPathUtil</span>.executorManagerActorName)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (dag &lt;- getDAG) &#123;</span><br><span class="line">    clockService = <span class="type">Some</span>(context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">ClockService</span>(dag, self, store))))</span><br><span class="line">    <span class="keyword">val</span> jarScheduler = <span class="keyword">new</span> <span class="type">JarScheduler</span>(appId, app.name, systemConfig, context)</span><br><span class="line">    taskManager = <span class="type">Some</span>(context.actorOf(<span class="type">Props</span>(<span class="keyword">new</span> <span class="type">TaskManager</span>(appContext.appId, dagManager,</span><br><span class="line">      jarScheduler, executorManager, clockService.get, self, app.name))))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">Receive</span> = &#123;</span><br><span class="line">    taskMessageHandler orElse</span><br><span class="line">      executorMessageHandler orElse</span><br><span class="line">      ready orElse</span><br><span class="line">      recover orElse</span><br><span class="line">      appMasterService orElse</span><br><span class="line">      <span class="type">ActorUtil</span>.defaultMsgHandler(self)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>At now I lost my line of argument, as there’re no request send trigger inside AppMaster, so what’s the entry of AppMaster?</p>
<p>Keep in mind, once create AppMaster, it will create <code>ExecutorManager</code> and <code>TaskManager</code>. Althrough we did’t see request send directory from AppMaster, we could find if there’re something inside ExecutorManager or TaskManager.</p>
<p>Suddenly comeup so many Managers make me unprepared. But unlike <code>AppManager</code> reside in Master, <code>ExecutorManager</code> and <code>TaskManager</code> both reside in Worker! </p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170624224944769" alt="9"></p>
<h2 id="Processor,_OP,_Task">Processor, OP, Task</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Planner</span> </span>&#123;</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Converts Dag of Op to Dag of TaskDescription. TaskDescription is part of the low level Graph API.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">plan</span></span>(dag: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>])</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Graph</span>[<span class="type">Processor</span>[_ &lt;: <span class="type">Task</span>], _ &lt;: <span class="type">Partitioner</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> graph = optimize(dag)</span><br><span class="line">    graph.mapEdge &#123; (node1, edge, node2) =&gt;</span><br><span class="line">      edge <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Shuffle</span> =&gt;</span><br><span class="line">          node2 <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> op: <span class="type">GroupByOp</span>[_, _] =&gt;</span><br><span class="line">              <span class="keyword">new</span> <span class="type">GroupByPartitioner</span>(op.groupBy.groupByFn)</span><br><span class="line">            <span class="keyword">case</span> _ =&gt; <span class="keyword">new</span> <span class="type">HashPartitioner</span></span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Direct</span> =&gt;</span><br><span class="line">          <span class="keyword">new</span> <span class="type">CoLocationPartitioner</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.mapVertex(_.getProcessor)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">optimize</span></span>(dag: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>])</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> graph = dag.copy</span><br><span class="line">    <span class="keyword">val</span> nodes = graph.topologicalOrderWithCirclesIterator.toList.reverse</span><br><span class="line">    <span class="keyword">for</span> (node &lt;- nodes) &#123;</span><br><span class="line">      <span class="keyword">val</span> outGoingEdges = graph.outgoingEdgesOf(node)</span><br><span class="line">      <span class="keyword">for</span> (edge &lt;- outGoingEdges) &#123;</span><br><span class="line">        merge(graph, edge._1, edge._3)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    graph</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(graph: <span class="type">Graph</span>[<span class="type">Op</span>, <span class="type">OpEdge</span>], node1: <span class="type">Op</span>, node2: <span class="type">Op</span>)</span><br><span class="line">    (<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (graph.outDegreeOf(node1) == <span class="number">1</span> &amp;&amp;</span><br><span class="line">      graph.inDegreeOf(node2) == <span class="number">1</span> &amp;&amp;</span><br><span class="line">      <span class="comment">// For processor node, we don't allow it to merge with downstream operators</span></span><br><span class="line">      !node1.isInstanceOf[<span class="type">ProcessorOp</span>[_ &lt;: <span class="type">Task</span>]] &amp;&amp;</span><br><span class="line">      !node2.isInstanceOf[<span class="type">ProcessorOp</span>[_ &lt;: <span class="type">Task</span>]]) &#123;</span><br><span class="line">      <span class="keyword">val</span> (_, edge, _) = graph.outgoingEdgesOf(node1).head</span><br><span class="line">      <span class="keyword">if</span> (edge == <span class="type">Direct</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> chainedOp = node1.chain(node2)</span><br><span class="line">        graph.addVertex(chainedOp)</span><br><span class="line">        <span class="keyword">for</span> (incomingEdge &lt;- graph.incomingEdgesOf(node1)) &#123;</span><br><span class="line">          graph.addEdge(incomingEdge._1, incomingEdge._2, chainedOp)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (outgoingEdge &lt;- graph.outgoingEdgesOf(node2)) &#123;</span><br><span class="line">          graph.addEdge(chainedOp, outgoingEdge._2, outgoingEdge._3)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Remove the old vertex</span></span><br><span class="line">        graph.removeVertex(node1)</span><br><span class="line">        graph.removeVertex(node2)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">case class DataSourceOp(</span><br><span class="line">    dataSource: DataSource,</span><br><span class="line">    parallelism: Int = 1,</span><br><span class="line">    userConfig: UserConfig = UserConfig.empty,</span><br><span class="line">    description: String = &quot;source&quot;)</span><br><span class="line">  extends Op &#123;</span><br><span class="line"></span><br><span class="line">  override def chain(other: Op)(implicit system: ActorSystem): Op = &#123;</span><br><span class="line">    DataSourceOp(dataSource, parallelism,</span><br><span class="line">      userConfig.withValue(Constants.GEARPUMP_STREAMING_OPERATOR, other.fn),</span><br><span class="line">      description)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  override def getProcessor(implicit system: ActorSystem): Processor[_ &lt;: Task] = &#123;</span><br><span class="line">    Processor[DataSourceTask[Any, Any]](parallelism, description,</span><br><span class="line">      userConfig.withValue(GEARPUMP_STREAMING_SOURCE, dataSource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">class TaskWrapper(</span><br><span class="line">    val taskId: TaskId, val taskClass: Class[_ &lt;: Task], context: TaskContextData,</span><br><span class="line">    userConf: UserConfig) extends TaskContext with TaskInterface &#123;</span><br><span class="line"></span><br><span class="line">  private var task: Option[Task] = None</span><br><span class="line"></span><br><span class="line">  override def onStart(startTime: Instant): Unit = &#123;</span><br><span class="line">    val constructor = taskClass.getConstructor(</span><br><span class="line">        classOf[TaskContext], classOf[UserConfig])</span><br><span class="line">    task = Some(constructor.newInstance(this, userConf))</span><br><span class="line">    task.foreach(_.onStart(startTime))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Utility that helps user to create a DAG starting with [[DataSourceTask]] user should pass in a [[DataSource]]</p>
<p>Here is an example to build a DAG that reads from Kafka source followed by word count</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> source = <span class="keyword">new</span> <span class="type">KafkaSource</span>()</span><br><span class="line"><span class="keyword">val</span> sourceProcessor =  <span class="type">DataSourceProcessor</span>(source, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> split = <span class="type">Processor</span>[<span class="type">Split</span>](<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> sum = <span class="type">Processor</span>[<span class="type">Sum</span>](<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> dag = sourceProcessor ~&gt; split ~&gt; sum</span><br></pre></td></tr></table></figure>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataSourceProcessor</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(</span><br><span class="line">      dataSource: <span class="type">DataSource</span>,</span><br><span class="line">      parallelism: <span class="type">Int</span> = <span class="number">1</span>,</span><br><span class="line">      description: <span class="type">String</span> = <span class="string">""</span>,</span><br><span class="line">      taskConf: <span class="type">UserConfig</span> = <span class="type">UserConfig</span>.empty)(<span class="keyword">implicit</span> system: <span class="type">ActorSystem</span>)</span><br><span class="line">    : <span class="type">Processor</span>[<span class="type">DataSourceTask</span>[<span class="type">Any</span>, <span class="type">Any</span>]] = &#123;</span><br><span class="line">    <span class="type">Processor</span>[<span class="type">DataSourceTask</span>[<span class="type">Any</span>, <span class="type">Any</span>]](parallelism, description,</span><br><span class="line">      taskConf.withValue[<span class="type">DataSource</span>](<span class="type">Constants</span>.<span class="type">GEARPUMP_STREAMING_SOURCE</span>, dataSource))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Default Task container for [[org.apache.gearpump.streaming.source.DataSource]] that reads from DataSource in batch</p>
<p>DataSourceTask calls:</p>
<ul>
<li><code>DataSource.open()</code> in <code>onStart</code> and pass in [[org.apache.gearpump.streaming.task.TaskContext]]</li>
</ul>
<p>and application start time</p>
<ul>
<li><code>DataSource.read()</code> in each <code>onNext</code>, which reads a batch of messages</li>
<li><code>DataSource.close()</code> in <code>onStop</code></li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataSourceTask</span>[<span class="type">IN</span>, <span class="type">OUT</span>] <span class="title">private</span>[source](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    context: <span class="type">TaskContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">UserConfig</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    source: <span class="type">DataSource</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    transform: <span class="type">Transform</span>[<span class="type">IN</span>, <span class="type">OUT</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Task</span>(<span class="params">context, conf</span>) </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">this</span></span>(context: <span class="type">TaskContext</span>, conf: <span class="type">UserConfig</span>) = &#123;</span><br><span class="line">    <span class="keyword">this</span>(context, conf,</span><br><span class="line">      conf.getValue[<span class="type">DataSource</span>](<span class="type">GEARPUMP_STREAMING_SOURCE</span>)(context.system).get,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Transform</span>[<span class="type">IN</span>, <span class="type">OUT</span>](context,</span><br><span class="line">        conf.getValue[<span class="type">FunctionRunner</span>[<span class="type">IN</span>, <span class="type">OUT</span>]](<span class="type">GEARPUMP_STREAMING_OPERATOR</span>)(context.system))</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deep into Apache Gearpump&lt;br&gt;
    
    </summary>
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/categories/bigdata/"/>
    
    
      <category term="流处理" scheme="http://github.com/zqhxuyuan/tags/%E6%B5%81%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Apache Kafka JIRA</title>
    <link href="http://github.com/zqhxuyuan/2017/06/20/JIRA_KAFKA/"/>
    <id>http://github.com/zqhxuyuan/2017/06/20/JIRA_KAFKA/</id>
    <published>2017-06-19T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.330Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka JIRA<br><a id="more"></a></p>
<p><a href="https://issues.apache.org/jira/browse/KAFKA" target="_blank" rel="noopener">https://issues.apache.org/jira/browse/KAFKA</a></p>
<h2 id="2944">2944</h2><p><a href="https://github.com/apache/kafka/pull/723">https://github.com/apache/kafka/pull/723</a></p>
<p>最后来分析<code>KafkaBasedLog</code>的<code>readToLogEnd()</code>方法如何读取到日志的最末尾，具体步骤如下。</p>
<ol>
<li>定位到分区的最末尾，通过消费者的<code>seekToEnd()</code>只是声明了重置策略为<code>LATEST</code>，并没有真正定位。客户端还需要调用消费者的轮询方法，才能保证发送拉取请求，并更新消费者的当前位置；</li>
<li>比较消费者的当前位置（<code>endOffset</code>）与上一次还没定位到最末尾时的位置（<code>startOffset</code>），如果前者大于后者，客户端需要调用<code>seek()</code>方法定位到旧的位置（<code>startOffset</code>）；</li>
<li>如果步骤(2)回退到旧的位置，需要调用轮询方法消费消息，直到当前位置是分区的最末尾位置。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaBasedLog</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123; </span><br><span class="line">  <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">readToLogEnd</span><span class="params">()</span> </span>&#123; <span class="comment">// 读取到日志的最末尾</span></span><br><span class="line">    <span class="comment">// 1. 定位到分区的最末尾（logEndOffset）</span></span><br><span class="line">    Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line">    Map&lt;TopicPartition, Long&gt; offsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">      <span class="keyword">long</span> offset = consumer.position(tp); <span class="comment">// 获取当前的消费位置</span></span><br><span class="line">      offsets.put(tp, offset); <span class="comment">// 暂存起来</span></span><br><span class="line">      consumer.seekToEnd(singleton(tp)); <span class="comment">// 定位到最末尾的位置</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 2. 回退到开始位置</span></span><br><span class="line">    Map&lt;TopicPartition, Long&gt; endOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      poll(<span class="number">0</span>);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">        <span class="keyword">long</span> startOffset = offsets.get(tp); <span class="comment">// 旧的消费位置</span></span><br><span class="line">        <span class="keyword">long</span> endOffset = consumer.position(tp); <span class="comment">// 当前的偏移量等于最末尾的位置</span></span><br><span class="line">        <span class="keyword">if</span> (endOffset &gt; startOffset) &#123; </span><br><span class="line">          endOffsets.put(tp, endOffset); </span><br><span class="line">          consumer.seek(tp, startOffset);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 3. 开始读取，直到读取到分区的最末尾位置</span></span><br><span class="line">    <span class="keyword">while</span> (!endOffsets.isEmpty()) &#123;</span><br><span class="line">      poll(Integer.MAX_VALUE);</span><br><span class="line">      Iterator it = endOffsets.entrySet().iterator();</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext()) &#123;</span><br><span class="line">        Map.Entry&lt;TopicPartition, Long&gt; entry = it.next();</span><br><span class="line">        <span class="keyword">if</span> (consumer.position(entry.getKey()) &lt; entry.getValue()) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">else</span> it.remove();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>客户端调用<code>readToLogEnd()</code>之前，如果还有新的消息没有消费，当调用<code>readToLogEnd()</code>方法时，可以保证客户端会完全消费新写入的消息。如图8-31（左图）所示，偏移量从<code>3</code>到<code>6</code>是新写入的消息（比如一个连接器配置、两个任务配置、一个提交日志的配置，总共四条消息）。客户端为了读取到分区最近的位置，先定位到最近的位置（<code>7</code>）。注意这时不能立即调用轮询方法，因为如果客户端在最近的位置，调用轮询不会有任何的新消息。客户端应该再定位到上次消费的位置（<code>3</code>），然后才能调用轮询方法，直到消费者的当前位置大于等于最近位置时，就说明客户端读取到了日志的最末尾。右图中，假设客户端已经消费到了日志的最末尾，那么调用<code>readToLogEnd()</code>方法会立即返回。</p>
<p><img src="https://images.weserv.nl/?url=http://img.blog.csdn.net/20170520193000243" alt="8"></p>
<p>图8-31 读取到分区最末尾的位置</p>
<blockquote>
<p><strong>注意：</strong>上面的<code>readToLogEnd()</code>方法用到了Kafka新消费者的三个方法。（1）：<code>postion()</code>方法返回消费者当前的位置，即消费进度，这个值比客户端真正消费过的位置要大<code>1</code>。比如客户端消费了两条消息，<code>postion()</code>方法的返回值就等于<code>3</code>。（2）：<code>seekToEnd(tp)</code>方法定位到日志的最末尾，同样，这个值也是实际的偏移量加上<code>1</code>（即<code>nextOffset</code>）。比如分区实际只有六条消息，最末尾的偏移量等于<code>7</code>。（3）：<code>seekTo(tp,offset)</code>方法定位到日志的指定位置。客户端定位到指定位置后，下一步一般是要调用轮询方法，并从这个位置拉取消息。所以如果客户端已经消费了偏移量等于<code>1</code>和<code>2</code>的两条消息，定位的位置是<code>3</code>，表示要拉取第三条的消息。不能定位到<code>2</code>，那样的话，从位置<code>2</code>开始拉取消息，就重复拉取了第二条消息。</p>
</blockquote>
<h2 id="2500/2076/KIP-17">2500/2076/KIP-17</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka JIRA&lt;br&gt;
    
    </summary>
    
      <category term="kafka" scheme="http://github.com/zqhxuyuan/categories/kafka/"/>
    
    
      <category term="流处理" scheme="http://github.com/zqhxuyuan/tags/%E6%B5%81%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Spark Metrics</title>
    <link href="http://github.com/zqhxuyuan/2017/05/01/Spark-Metrics/"/>
    <id>http://github.com/zqhxuyuan/2017/05/01/Spark-Metrics/</id>
    <published>2017-04-30T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.384Z</updated>
    
    <content type="html"><![CDATA[<p>Spark Metrics<br><a id="more"></a></p>
<ul>
<li><a href="http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/" target="_blank" rel="noopener">http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/</a></li>
</ul>
<h2 id="命令行添加监控">命令行添加监控</h2><p>直接添加到命令行后</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--files=/yourPath/metrics.properties --conf spark.metrics.conf=metrics.properties</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The –files flag will cause /path/to/metrics.properties to be sent to every executor,<br>and spark.metrics.conf=metrics.properties will tell all executors to load that file<br>when initializing their respective MetricsSystems.</p>
</blockquote>
<p>或者用conf的形式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.metrics.conf.*.sink.graphite.class=org.apache.spark.metrics.sink.GraphiteSink \</span><br><span class="line">--conf spark.metrics.conf.*.sink.graphite.host=...</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Metrics">Spark Metrics</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink</span><br><span class="line">*.sink.console.period=10</span><br><span class="line">*.sink.console.unit=seconds</span><br><span class="line">*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink</span><br><span class="line">*.sink.csv.period=1</span><br><span class="line">*.sink.csv.unit=minutes</span><br><span class="line">*.sink.csv.directory=/tmp/</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">➜  spark-2.0.1-bin-hadoop2.7 bin/spark-shell</span><br><span class="line">Spark context Web UI available at http://10.57.2.5:4040</span><br><span class="line">Spark context available as &apos;sc&apos; (master = local[*], app id = local-1495078254084).</span><br><span class="line">Spark session available as &apos;spark&apos;.</span><br><span class="line">scala&gt; 17-5-18 11:31:05 ===============================================================</span><br><span class="line"></span><br><span class="line">-- Gauges ----------------------------------------------------------------------</span><br><span class="line">local-1495078254084.driver.BlockManager.disk.diskSpaceUsed_MB value = 0</span><br><span class="line">local-1495078254084.driver.BlockManager.memory.maxMem_MB value = 366</span><br><span class="line">local-1495078254084.driver.BlockManager.memory.memUsed_MB value = 0</span><br><span class="line">local-1495078254084.driver.BlockManager.memory.remainingMem_MB value = 366</span><br><span class="line">local-1495078254084.driver.DAGScheduler.job.activeJobs value = 0</span><br><span class="line">local-1495078254084.driver.DAGScheduler.job.allJobs value = 0</span><br><span class="line">local-1495078254084.driver.DAGScheduler.stage.failedStages value = 0</span><br><span class="line">local-1495078254084.driver.DAGScheduler.stage.runningStages value = 0</span><br><span class="line">local-1495078254084.driver.DAGScheduler.stage.waitingStages value = 0</span><br><span class="line"></span><br><span class="line">-- Histograms ------------------------------------------------------------------</span><br><span class="line">local-1495078254084.driver.CodeGenerator.compilationTime</span><br><span class="line">             count = 0</span><br><span class="line">               min = 0</span><br><span class="line">               max = 0</span><br><span class="line">              mean = 0.00</span><br><span class="line">            stddev = 0.00</span><br><span class="line">            median = 0.00</span><br><span class="line">              75% &lt;= 0.00</span><br><span class="line">              95% &lt;= 0.00</span><br><span class="line">              98% &lt;= 0.00</span><br><span class="line">              99% &lt;= 0.00</span><br><span class="line">            99.9% &lt;= 0.00</span><br><span class="line">local-1495078254084.driver.CodeGenerator.generatedClassSize</span><br><span class="line">             count = 0</span><br><span class="line">               min = 0</span><br><span class="line">               max = 0</span><br><span class="line">              mean = 0.00</span><br><span class="line">            stddev = 0.00</span><br><span class="line">            median = 0.00</span><br><span class="line">              75% &lt;= 0.00</span><br><span class="line">              95% &lt;= 0.00</span><br><span class="line">              98% &lt;= 0.00</span><br><span class="line">              99% &lt;= 0.00</span><br><span class="line">            99.9% &lt;= 0.00</span><br><span class="line">local-1495078254084.driver.CodeGenerator.generatedMethodSize</span><br><span class="line">             count = 0</span><br><span class="line">               min = 0</span><br><span class="line">               max = 0</span><br><span class="line">              mean = 0.00</span><br><span class="line">            stddev = 0.00</span><br><span class="line">            median = 0.00</span><br><span class="line">              75% &lt;= 0.00</span><br><span class="line">              95% &lt;= 0.00</span><br><span class="line">              98% &lt;= 0.00</span><br><span class="line">              99% &lt;= 0.00</span><br><span class="line">            99.9% &lt;= 0.00</span><br><span class="line">local-1495078254084.driver.CodeGenerator.sourceCodeSize</span><br><span class="line">             count = 0</span><br><span class="line">               min = 0</span><br><span class="line">               max = 0</span><br><span class="line">              mean = 0.00</span><br><span class="line">            stddev = 0.00</span><br><span class="line">            median = 0.00</span><br><span class="line">              75% &lt;= 0.00</span><br><span class="line">              95% &lt;= 0.00</span><br><span class="line">              98% &lt;= 0.00</span><br><span class="line">              99% &lt;= 0.00</span><br><span class="line">            99.9% &lt;= 0.00</span><br><span class="line"></span><br><span class="line">-- Timers ----------------------------------------------------------------------</span><br><span class="line">local-1495078254084.driver.DAGScheduler.messageProcessingTime</span><br><span class="line">             count = 0</span><br><span class="line">         mean rate = 0.00 calls/second</span><br><span class="line">     1-minute rate = 0.00 calls/second</span><br><span class="line">     5-minute rate = 0.00 calls/second</span><br><span class="line">    15-minute rate = 0.00 calls/second</span><br><span class="line">               min = 0.00 milliseconds</span><br><span class="line">               max = 0.00 milliseconds</span><br><span class="line">              mean = 0.00 milliseconds</span><br><span class="line">            stddev = 0.00 milliseconds</span><br><span class="line">            median = 0.00 milliseconds</span><br><span class="line">              75% &lt;= 0.00 milliseconds</span><br><span class="line">              95% &lt;= 0.00 milliseconds</span><br><span class="line">              98% &lt;= 0.00 milliseconds</span><br><span class="line">              99% &lt;= 0.00 milliseconds</span><br><span class="line">            99.9% &lt;= 0.00 milliseconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">17-5-18 11:31:15 ===============================================================</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(List(1,2,3,4,5)).count</span><br><span class="line">res1: Long = 5</span><br><span class="line"></span><br><span class="line">scala&gt; 17-5-18 11:33:15 ===============================================================</span><br><span class="line"></span><br><span class="line">-- Timers ----------------------------------------------------------------------</span><br><span class="line">local-1495078254084.driver.DAGScheduler.messageProcessingTime</span><br><span class="line">             count = 10</span><br><span class="line">         mean rate = 0.07 calls/second</span><br><span class="line">     1-minute rate = 0.16 calls/second</span><br><span class="line">     5-minute rate = 0.03 calls/second</span><br><span class="line">    15-minute rate = 0.01 calls/second</span><br><span class="line">               min = 0.03 milliseconds</span><br><span class="line">               max = 1207.28 milliseconds</span><br><span class="line">              mean = 125.02 milliseconds</span><br><span class="line">            stddev = 358.42 milliseconds</span><br><span class="line">            median = 0.32 milliseconds</span><br><span class="line">              75% &lt;= 16.58 milliseconds</span><br><span class="line">              95% &lt;= 1207.28 milliseconds</span><br><span class="line">              98% &lt;= 1207.28 milliseconds</span><br><span class="line">              99% &lt;= 1207.28 milliseconds</span><br><span class="line">            99.9% &lt;= 1207.28 milliseconds</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ ll /tmp/ -rth</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.DAGScheduler.stage.waitingStages.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.DAGScheduler.stage.runningStages.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.DAGScheduler.stage.failedStages.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   1.3K  5 18 11:36 local-1495078254084.driver.DAGScheduler.messageProcessingTime.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.DAGScheduler.job.allJobs.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.DAGScheduler.job.activeJobs.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   676B  5 18 11:36 local-1495078254084.driver.CodeGenerator.sourceCodeSize.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   676B  5 18 11:36 local-1495078254084.driver.CodeGenerator.generatedMethodSize.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   676B  5 18 11:36 local-1495078254084.driver.CodeGenerator.generatedClassSize.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   676B  5 18 11:36 local-1495078254084.driver.CodeGenerator.compilationTime.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   113B  5 18 11:36 local-1495078254084.driver.BlockManager.memory.remainingMem_MB.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.BlockManager.memory.memUsed_MB.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel   113B  5 18 11:36 local-1495078254084.driver.BlockManager.memory.maxMem_MB.csv</span><br><span class="line">-rw-r--r--   1 zhengqh  wheel    99B  5 18 11:36 local-1495078254084.driver.BlockManager.disk.diskSpaceUsed_MB.csv</span><br><span class="line"></span><br><span class="line">➜  /tmp cat local-1495078254084.driver.DAGScheduler.messageProcessingTime.csv</span><br><span class="line">t,count,max,mean,min,stddev,p50,p75,p95,p98,p99,p999,mean_rate,m1_rate,m5_rate,m15_rate,rate_unit,duration_unit</span><br><span class="line">1495078315,0,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,calls/second,milliseconds</span><br><span class="line">1495078375,0,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,0.000000,calls/second,milliseconds</span><br><span class="line">1495078435,10,1207.284400,125.017564,0.027442,358.422668,0.317114,16.580495,1207.284400,1207.284400,1207.284400,1207.284400,0.055257,0.082101,0.028931,0.010599,calls/second,milliseconds</span><br><span class="line">1495078495,10,1207.284400,125.017564,0.027442,358.422668,0.317114,16.580495,1207.284400,1207.284400,1207.284400,1207.284400,0.041499,0.030203,0.023686,0.009915,calls/second,milliseconds</span><br><span class="line">1495078555,10,1207.284400,125.017564,0.027442,358.422668,0.317114,16.580495,1207.284400,1207.284400,1207.284400,1207.284400,0.033225,0.011111,0.019393,0.009276,calls/second,milliseconds</span><br><span class="line">1495078577,10,1207.284400,125.017564,0.027442,358.422668,0.317114,16.580495,1207.284400,1207.284400,1207.284400,1207.284400,0.030895,0.007962,0.018142,0.009072,calls/second,milliseconds</span><br><span class="line">1495078577,10,1207.284400,125.017564,0.027442,358.422668,0.317114,16.580495,1207.284400,1207.284400,1207.284400,1207.284400,0.030890,0.007962,0.018142,0.009072,calls/second,milliseconds</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Cassandra_Metrics">Spark Cassandra Metrics</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">executor.source.cassandra-connector.class=org.apache.spark.metrics.CassandraConnectorSource</span><br><span class="line">driver.source.cassandra-connector.class=org.apache.spark.metrics.CassandraConnectorSource</span><br></pre></td></tr></table></figure>
<h2 id="Spark_Influx_Metrics">Spark Influx Metrics</h2><p><a href="https://github.com/palantir/spark-influx-sink">https://github.com/palantir/spark-influx-sink</a></p>
<p>spark.driver.extraClassPath=spark-influx-sink.jar:metrics-influxdb.jar<br>spark.executor.extraClassPath=spark-influx-sink.jar:metrics-influxdb.jar</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">*.sink.influx.class=org.apache.spark.metrics.sink.InfluxDbSink</span><br><span class="line">*.sink.influx.protocol=https</span><br><span class="line">*.sink.influx.host=localhost</span><br><span class="line">*.sink.influx.port=8086</span><br><span class="line">*.sink.influx.database=my_metrics</span><br><span class="line">*.sink.influx.auth=metric_client:PASSWORD</span><br><span class="line">*.sink.influx.tags=product:my_product,parent:my_service</span><br></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Spark Metrics&lt;br&gt;
    
    </summary>
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/categories/spark/"/>
    
    
      <category term="spark" scheme="http://github.com/zqhxuyuan/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>开源大数据ETL工具</title>
    <link href="http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/"/>
    <id>http://github.com/zqhxuyuan/2017/02/15/2017-02-17-BigData-ETL/</id>
    <published>2017-02-14T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.255Z</updated>
    
    <content type="html"><![CDATA[<p>BigData ETL Tools<br><a id="more"></a></p>
<h2 id="datatorrent(apex)">datatorrent(apex)</h2><p>执行<code>./datatorrent-rts-community-3.7.0.bin --help</code>打印帮助项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[qihuang.zheng@dp0653 install]$ sudo -u admin ./datatorrent-rts-community-3.7.0.bin \</span><br><span class="line">-B /usr/install/datatorrent-rts -g 9094 \</span><br><span class="line">-E DT_LOG_DIR=/home/admin/datatorrent \</span><br><span class="line">-E DT_RUN_DIR=/home/admin/run/datatorrent</span><br><span class="line"></span><br><span class="line">Verifying archive integrity... All good.</span><br><span class="line">Uncompressing DataTorrent Distribution  100%</span><br><span class="line"></span><br><span class="line">DataTorrent Platform 3.7.0 will be installed under /usr/install/datatorrent-rts/releases/3.7.0</span><br><span class="line"></span><br><span class="line">dtGateway can be managed with: /usr/install/datatorrent-rts/releases/3.7.0/bin/dtgateway [start|stop|status]</span><br><span class="line">DTGateway is running as pid 24571 and listening on 0.0.0.0:9094</span><br><span class="line"></span><br><span class="line">Please finish the remaining installation steps via DataTorrent Console at: http://dp0653:9094/</span><br></pre></td></tr></table></figure>
<p>创建apex项目，并打包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">name=salesapp</span><br><span class="line">version=3.5.0</span><br><span class="line"></span><br><span class="line">mvn -B archetype:generate \</span><br><span class="line">  -DarchetypeGroupId=org.apache.apex \</span><br><span class="line">  -DarchetypeArtifactId=apex-app-archetype \</span><br><span class="line">  -DarchetypeVersion=$version  \</span><br><span class="line">  -DgroupId=com.example \</span><br><span class="line">  -Dpackage=com.example.$name \</span><br><span class="line">  -DartifactId=$name \</span><br><span class="line">  -Dversion=1.0-SNAPSHOT</span><br></pre></td></tr></table></figure>
<p>上传到datatorrent平台</p>
<h2 id="StreamSets(https://github-com/streamsets/datacollector)">StreamSets(<a href="https://github.com/streamsets/datacollector">https://github.com/streamsets/datacollector</a>)</h2><h2 id="StreamFlow(https://github-com/lmco/streamflow)">StreamFlow(<a href="https://github.com/lmco/streamflow">https://github.com/lmco/streamflow</a>)</h2><h2 id="CDAP(https://github-com/caskdata/cdap)">CDAP(<a href="https://github.com/caskdata/cdap">https://github.com/caskdata/cdap</a>)</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;BigData ETL Tools&lt;br&gt;
    
    </summary>
    
      <category term="work" scheme="http://github.com/zqhxuyuan/categories/work/"/>
    
    
      <category term="bigdata" scheme="http://github.com/zqhxuyuan/tags/bigdata/"/>
    
  </entry>
  
  <entry>
    <title>开发者构建工具</title>
    <link href="http://github.com/zqhxuyuan/2017/01/01/Tools-Build/"/>
    <id>http://github.com/zqhxuyuan/2017/01/01/Tools-Build/</id>
    <published>2016-12-31T16:00:00.000Z</published>
    <updated>2019-02-14T13:42:29.388Z</updated>
    
    <content type="html"><![CDATA[<p>Maven,SBT构建工具<br><a id="more"></a></p>
<h2 id="Maven">Maven</h2><h3 id="assembly">assembly</h3><p>maven-assembly-plugin</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>maven-shade-plugin</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-shade-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">goal</span>&gt;</span></span><br><span class="line">                    shade</span><br><span class="line">                <span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">minimizeJar</span>&gt;</span>true<span class="tag">&lt;/<span class="name">minimizeJar</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadedArtifactAttached</span>&gt;</span>true<span class="tag">&lt;/<span class="name">shadedArtifactAttached</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">shadedClassifierName</span>&gt;</span>fat<span class="tag">&lt;/<span class="name">shadedClassifierName</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">relocations</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">relocation</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>com.google<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">shadedPattern</span>&gt;</span>shaded.guava<span class="tag">&lt;/<span class="name">shadedPattern</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">include</span>&gt;</span>com.google.**<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.common.base.Optional<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.common.base.Absent<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>com.google.common.base.Present<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">relocation</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">relocations</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filters</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">filter</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifact</span>&gt;</span>*:*<span class="tag">&lt;/<span class="name">artifact</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">excludes</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.SF<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.DSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">exclude</span>&gt;</span>META-INF/*.RSA<span class="tag">&lt;/<span class="name">exclude</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">excludes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filters</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ test ---</span><br><span class="line">[INFO] Building jar: /Users/zhengqh/Github/test/target/test-1.0-SNAPSHOT.jar</span><br><span class="line">[INFO] --- maven-assembly-plugin:2.2-beta-5:single (make-assembly) @ test ---</span><br><span class="line">...</span><br><span class="line">[INFO] Building jar: /Users/zhengqh/Github/test/target/test-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">[INFO] Minimizing jar test:test:jar:1.0-SNAPSHOT</span><br><span class="line"></span><br><span class="line">$ ll target</span><br><span class="line">-rw-r--r--  1 zhengqh  staff   8.1M  6 22 11:54 test-1.0-SNAPSHOT-fat.jar</span><br><span class="line">-rw-r--r--  1 zhengqh  staff    29M  6 22 11:54 test-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line">-rw-r--r--  1 zhengqh  staff   9.2K  6 22 11:54 test-1.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>
<p>maven-assembly-plugin生成test-1.0-SNAPSHOT-jar-with-dependencies.jar<br>maven-shade-plugin的shadedClassifierName为<code>fat</code>，结果：test-1.0-SNAPSHOT-fat.jar</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">➜  test jar -tvf target/test-1.0-SNAPSHOT-jar-with-dependencies.jar|grep shaded</span><br><span class="line">assembly并不会重命令，只有shade才可以</span><br><span class="line"></span><br><span class="line">➜  test jar -tvf target/test-1.0-SNAPSHOT-fat.jar|grep shaded</span><br><span class="line">     0 Thu Jun 22 11:54:42 CST 2017 shaded/</span><br><span class="line">     0 Thu Jun 22 11:54:42 CST 2017 shaded/guava/</span><br><span class="line">     0 Thu Jun 22 11:54:42 CST 2017 shaded/guava/common/</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<h3 id="install">install</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mvn install:install-file -Dfile=~/Downloads/ojdbc6-11.2.0.3.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0 -Dpackaging=jar</span><br><span class="line"></span><br><span class="line">mvn install:install-file -Dfile=pontus-api_2.11-0.0.1.jar -DgroupId=cn.fraudmetrix.pontus -DartifactId=pontus-api_2.11 -Dversion=0.0.1 -Dpackaging=jar</span><br><span class="line"></span><br><span class="line">[INFO] Scanning for projects...</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Building Maven Stub Project (No POM) 1</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO]</span><br><span class="line">[INFO] --- maven-install-plugin:2.4:install-file (default-cli) @ standalone-pom ---</span><br><span class="line">[INFO] Installing /Users/zhengqh/pontus-api_2.11-0.0.1.jar to /Users/zhengqh/.m2/repository/cn/fraudmetrix/pontus/pontus-api_2.11/0.0.1/pontus-api_2.11-0.0.1.jar</span><br><span class="line">[INFO] Installing /var/folders/xc/x0b8crk9667ddh1zhfs29_zr0000gn/T/mvninstall1940592568391629100.pom to /Users/zhengqh/.m2/repository/cn/fraudmetrix/pontus/pontus-api_2.11/0.0.1/pontus-api_2.11-0.0.1.pom</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] BUILD SUCCESS</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br><span class="line">[INFO] Total time: 1.088 s</span><br><span class="line">[INFO] Finished at: 2017-07-17T11:50:49+08:00</span><br><span class="line">[INFO] Final Memory: 6M/64M</span><br><span class="line">[INFO] ------------------------------------------------------------------------</span><br></pre></td></tr></table></figure>
<h3 id="deploy">deploy</h3><p>源码包上传</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn deploy</span><br></pre></td></tr></table></figure>
<p>本地包上传到nexus</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">mvn deploy:deploy-file -DgroupId=&lt;group-id&gt; \</span><br><span class="line">  -DartifactId=&lt;artifact-id&gt; \</span><br><span class="line">  -Dversion=&lt;version&gt; \</span><br><span class="line">  -Dpackaging=&lt;type-of-packaging&gt; \</span><br><span class="line">  -Dfile=&lt;path-to-file&gt; \</span><br><span class="line">  -DrepositoryId=&lt;id-to-map-on-server-section-of-settings.xml&gt; \</span><br><span class="line">  -Durl=&lt;url-of-the-repository-to-deploy&gt;</span><br><span class="line"></span><br><span class="line">mvn deploy:deploy-file -DgroupId=依赖项的GroupID \</span><br><span class="line">  -DartifactId=依赖项名称 \</span><br><span class="line">  -Dversion=依赖版本 \</span><br><span class="line">  -Dpackaging=jar \</span><br><span class="line">  -Dfile=三方库的文件路径 \</span><br><span class="line">  -DrepositoryId=fraudmetrixRepo \</span><br><span class="line">  -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/</span><br><span class="line"></span><br><span class="line">repositoryId对应~/.m2/setting.xml中的server配置</span><br><span class="line"></span><br><span class="line">&lt;server&gt;</span><br><span class="line">  &lt;id&gt;fraudmetrixRepo&lt;/id&gt;</span><br><span class="line">  &lt;username&gt;xxx&lt;/username&gt;</span><br><span class="line">  &lt;password&gt;xxx&lt;/password&gt;</span><br><span class="line">&lt;/server&gt;</span><br><span class="line"></span><br><span class="line">部署pontus-api.jar</span><br><span class="line">mvn deploy:deploy-file -DgroupId=cn.fraudmetrix.pontus -DartifactId=pontus-api_2.11 -Dversion=0.0.1 \</span><br><span class="line"> -Dpackaging=jar -Dfile=pontus-api_2.11-0.0.1.jar \</span><br><span class="line"> -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo  </span><br><span class="line"></span><br><span class="line">部署ojdbc.jar</span><br><span class="line">mvn deploy:deploy-file -Dfile=/Users/zhengqh/Downloads/install/ojdbc6-11.2.0.3.jar \</span><br><span class="line">  -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0 -Dpackaging=jar \</span><br><span class="line">  -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo</span><br></pre></td></tr></table></figure>
<p>以-数字开头或者-V开头生成准备文件：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> files = <span class="keyword">new</span> java.io.<span class="type">File</span>(<span class="string">"/Users/zhengqh/Downloads/V100R002C60U20CP003/common/lib"</span>).listFiles.map(_.getName).filter(_.startsWith(<span class="string">"h"</span>)).toList</span><br><span class="line"><span class="keyword">import</span> scala.util.matching.<span class="type">Regex</span></span><br><span class="line"><span class="keyword">val</span> numitemPattern = <span class="string">"(.*)(-[0-9|V].*)"</span>.r</span><br><span class="line">files.foreach(file =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> numitemPattern(art, version) = file</span><br><span class="line">  println(file + <span class="string">" "</span> + art + <span class="string">" "</span> + version.substring(<span class="number">1</span>).replace(<span class="string">".jar"</span>,<span class="string">""</span>))    </span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>导入到maven仓库：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat genMaven.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jar=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f3`</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$jar</span> <span class="variable">$art</span> <span class="variable">$ver</span>"</span></span><br><span class="line">  mvn deploy:deploy-file -Dfile=<span class="variable">$jar</span> \</span><br><span class="line">  -DgroupId=com.huawei.fusion -DartifactId=<span class="variable">$art</span> -Dversion=<span class="string">"<span class="variable">$ver</span>-FSV100R002C60U20CP003"</span> -Dpackaging=jar \</span><br><span class="line">  -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>不更改groupId,从MANIFEST中获取groupId</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cat genMaven.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jarName=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f3`</span><br><span class="line">  <span class="built_in">printf</span> <span class="string">"<span class="variable">$jarName</span> <span class="variable">$art</span> <span class="variable">$ver</span> "</span></span><br><span class="line">  jar xf <span class="variable">$jarName</span> META-INF/MANIFEST.MF</span><br><span class="line">  group=$(cat META-INF/MANIFEST.MF |grep Implementation-Vendor-Id |cut -d<span class="string">" "</span> -f2)</span><br><span class="line">  <span class="keyword">if</span> [ -n <span class="string">"<span class="variable">$group</span>"</span> ]; <span class="keyword">then</span></span><br><span class="line">   <span class="built_in">printf</span> <span class="string">"<span class="variable">$group</span>"</span></span><br><span class="line">   <span class="built_in">print</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">  rm -rf META-INF</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">cat genFS.txt | <span class="keyword">while</span> <span class="built_in">read</span> line</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  jarName=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f1`</span><br><span class="line">  art=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f2`</span><br><span class="line">  ver=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f3`</span><br><span class="line">  group=`<span class="built_in">echo</span> <span class="variable">$line</span> | cut -d<span class="string">" "</span> -f4`</span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"<span class="variable">$jarName</span> <span class="variable">$art</span> <span class="variable">$ver</span> <span class="variable">$group</span>"</span></span><br><span class="line">  mvn deploy:deploy-file -Dfile=<span class="variable">$jarName</span> \</span><br><span class="line">    -DgroupId=<span class="variable">$group</span> -DartifactId=<span class="variable">$art</span> -Dversion=<span class="string">"<span class="variable">$ver</span>-FSV100R002C60U20CP003"</span> -Dpackaging=jar \</span><br><span class="line">    -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo  </span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>找不到的jar包改版本后重新上传</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mvn deploy:deploy-file -Dfile=hadoop-yarn-server-tests-2.7.2.jar \</span><br><span class="line">  -DgroupId=org.apache.hadoop -DartifactId=hadoop-yarn-server-tests -Dversion=2.7.2-FSV100R002C60U20CP003 -Dpackaging=jar \</span><br><span class="line">  -Durl=http://maven.fraudmetrix.cn/nexus/content/repositories/releases/ -DrepositoryId=fraudmetrixRepo</span><br><span class="line">  </span><br><span class="line">find ~/.m2/ -name &quot;*.lastUpdated&quot; | xargs rm</span><br></pre></td></tr></table></figure>
<h2 id="SBT">SBT</h2><h2 id="Ref">Ref</h2><ul>
<li><a href="https://stackoverflow.com/questions/13620281/what-is-the-maven-shade-plugin-used-for-and-why-would-you-want-to-relocate-java" target="_blank" rel="noopener">https://stackoverflow.com/questions/13620281/what-is-the-maven-shade-plugin-used-for-and-why-would-you-want-to-relocate-java</a></li>
<li><a href="https://www.elastic.co/blog/to-shade-or-not-to-shade#sthash.CRl8HKfN.dpbs" target="_blank" rel="noopener">https://www.elastic.co/blog/to-shade-or-not-to-shade#sthash.CRl8HKfN.dpbs</a></li>
<li><a href="http://ju.outofmemory.cn/entry/67085" target="_blank" rel="noopener">http://ju.outofmemory.cn/entry/67085</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Maven,SBT构建工具&lt;br&gt;
    
    </summary>
    
      <category term="tool" scheme="http://github.com/zqhxuyuan/categories/tool/"/>
    
    
      <category term="work" scheme="http://github.com/zqhxuyuan/tags/work/"/>
    
  </entry>
  
</feed>
